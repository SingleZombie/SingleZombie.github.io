<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Designer, artist, philosopher, researcher.">
<meta property="og:type" content="website">
<meta property="og:title" content="周弈帆的博客">
<meta property="og:url" content="https://zhouyifan.net/page/2/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="Designer, artist, philosopher, researcher.">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Zhou Yifan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhouyifan.net/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="周弈帆的博客" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-switch_lang">

    <a href="https://zhouyifan.net/blog-en" rel="section"><i class="fa fa-language fa-fw"></i>English</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/07/27/20240717-ar-wo-vq/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/07/27/20240717-ar-wo-vq/" class="post-title-link" itemprop="url">解读何恺明新作：不用向量离散化的自回归图像生成（Autoregressive Image Generation without Vector Quantization）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-07-27 20:32:45" itemprop="dateCreated datePublished" datetime="2024-07-27T20:32:45+08:00">2024-07-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>自回归是一种根据之前已生成内容，不断递归预测下一项要生成的内容的生成模型。这种生成方式十分易懂，符合我们对生活的观察。比如我们希望模型生成一句话，第一个是「今」字，那么第二个字很可能就是「天」字。如果前三个字是「今天早」，那么第四个字就很可能是「上」。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（空）  -&gt; 今</span><br><span class="line">今      -&gt; 天</span><br><span class="line">今天    -&gt; 早</span><br><span class="line">今天早  -&gt; 上</span><br></pre></td></tr></table></figure><br>为这种自回归模型的而设计的 Transformer 网络在自然语言处理（NLP）中取得了极大的成功。然而，尽管许多人也尝试用它生成图像，自回归模型却一直没有成为最强大、最受欢迎的图像生成模型。</p>
<p>为了解决此问题，何恺明团队公布了论文 <em>Autoregressive Image Generation without Vector Quantization</em>。作者分析了目前最常见的自回归图像生成模型后，发现模型中的<strong>向量离散化 (Vector Quantization, VQ)</strong> 是拖累模型能力的罪魁祸首。作者用一些巧妙的方法绕过了 VQ，最终设计出了一种新式自回归模型。该模型在图像生成任务上表现出色，在 ImageNet 图像生成指标上不逊于最先进的图像扩散模型。在这篇博文中，我们就来学习一下这种新颖的无 VQ 自回归图像生成模型。</p>
<p>建议读者在阅读本文前熟悉 VQ-VAE、Transformer、DDPM 等经典工作，了解 NLP 和图像生成中连续值和离散值的概念。可以参考我之前写的文章：</p>
<p>轻松理解 VQ-VAE：首个提出 codebook 机制的生成模型</p>
<p>VQGAN 论文与源码解读：前Diffusion时代的高清图像生成模型</p>
<p>Transformer 论文精读</p>
<p>扩散模型(Diffusion Model)详解：直观理解、数学原理、PyTorch 实现</p>
<p>Stable Diffusion 解读（一）：回顾早期工作</p>
<h2 id="知识回顾"><a href="#知识回顾" class="headerlink" title="知识回顾"></a>知识回顾</h2><h3 id="连续值与离散值"><a href="#连续值与离散值" class="headerlink" title="连续值与离散值"></a>连续值与离散值</h3><p>在计算机科学中，我们既会用到连续值，也会用到离散值。比如颜色就是一个常见的连续值，我们用 0~1 之间的实数表示灰度从全黑到全白。而词元 (token) 需要用离散值表示，比如我们用 “0” 表示字母 “A”，”1” 表示 “B”, “2” 表示 “C”，并不代表 “B” 是 「’A’ 和 ‘C’ 的平均值」。离散值的数值只是用来区分不同概念的。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/1.jpg" alt></p>
<p>神经网络默认输入是连续变化的。因此，一个连续值可以直接输入进网络。而代表离散值的整数不能直接输入网络，需要先过一个嵌入层，再正常输入进网络。</p>
<h3 id="自回归与类别分布"><a href="#自回归与类别分布" class="headerlink" title="自回归与类别分布"></a>自回归与类别分布</h3><p>在自回归文本生成模型中，为了不断预测下一个词元，通常的做法是用一个神经网络建模下一个词元的类别分布（categorical distribution）。如下面的例子所示，所谓类别分布，就是下一步选择每一个词元的概率。有了概率分布后，我们就能用采样算法采样出下一个词元。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/2.jpg" alt></p>
<p>要训练这个预测模型也很简单。每次预测下一个词元的类别分布，其实就是一个分类任务。我们直接照着分类任务的做法，以数据集里现有句子为真值，用交叉熵损失函数就能训练这个预测模型了。</p>
<h3 id="自回归图像生成"><a href="#自回归图像生成" class="headerlink" title="自回归图像生成"></a>自回归图像生成</h3><p>由于 Transformer 在 NLP 中的成功，大家也想用 Transformer 做图像生成。在用自回归模型生成图像时，需要考虑图像和文本的两个区别：</p>
<ol>
<li>文本是一维的，天然有先后顺序以供自回归生成。而图像是二维的，没有先后顺序。</li>
<li>图像的颜色值是连续而非离散的。而只有离散值才能用类别分布表示。</li>
</ol>
<p>解决问题 1 的方法很简单：没有先后顺序，我们就人工定义一个先后顺序就好了，比如从左上到右下给图像编号。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/3.jpg" alt></p>
<p>而对于问题 2，一种最简单的方式是把连续的颜色值离散化。比如将原来 0 ~ 1 的灰度值转换为「0 号灰度」、「1 号灰度」、…… 「7号灰度」。神经网络像对待词元一样对待这些灰度值，不知道它们之间的大小关系，只知道生成图像的颜色只能由这 8 种「颜色词语」构成。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/4.jpg" alt></p>
<h3 id="向量离散化"><a href="#向量离散化" class="headerlink" title="向量离散化"></a>向量离散化</h3><p>把颜色值离散化后，我们的确可以用自回归做图像生成了。但是，由于图像的像素数比文章的词元数要多很多，这种逐像素生成方式会非常慢。为了加速自回归生成，VQ-VAE, VQGAN 等工作借由向量离散化自编码器（VQ 自编码器）实现了一个两阶段的图像生成方法：</p>
<ul>
<li>训练时，先训练一个包括编码器 (encoder) 和解码器 (decoder) 两个子模型的 VQ 自编码器，再训练一个生成压缩图像的自回归模型。</li>
<li>生成时，先用自回归模型生成出一个压缩图像，再用 VQ 自编码器将其复原成真实图像。</li>
</ul>
<p>相比普通的自编码器，VQ 自编码器有一项特点：它生成的压缩图像仅由离散值组成。这样，它就同时完成了两项任务，使得自回归模型能够高效地实现图像生成：1）将连续图像变成离散图像；2）减少要生成的像素数。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/5.jpg" alt></p>
<blockquote>
<p>如果你还是不太理解 VQ 的作用，请先回顾 VQ-VAE 工作，再来学习这篇工作。</p>
</blockquote>
<h2 id="抛弃-VQ，拥抱扩散模型"><a href="#抛弃-VQ，拥抱扩散模型" class="headerlink" title="抛弃 VQ，拥抱扩散模型"></a>抛弃 VQ，拥抱扩散模型</h2><p>我们来总结一下为什么要使用基于 VQ 的自回归图像生成：大家想用基于 Transformer 的自回归模型做图像生成。自回归模型在预测下一个词元/像素时，<strong>通常</strong>会用一个类别分布来建模下一项数据。由于类别分布只能描述离散数据，而图像又是连续数据，我们需要把连续像素值变成离散值。一种<strong>常用</strong>的将连续图像变成离散图像的方法是 VQ 自编码器，它既能减少图像尺寸以提高生成效率，又能将连续图像变成离散图像。</p>
<p>但相比普通的自编码器，如 VAE，VQ 自编码器有着一些缺点：</p>
<ul>
<li>VQ 自编码器很难训练</li>
<li>VQ 自编码器的重建效果没有 VAE 好。比如在 Stable Diffusion 中，开发者选择了用 VAE 而不是 VQ-VAE 作为自编码器</li>
</ul>
<p>出于抛弃 VQ 的想法，论文的作者发问道：「自回归图像生成真的需要和 VQ 绑定起来吗？」注意到，在我们刚刚阐述使用 VQ 自回归生成的动机时，用了几个「通常」、「常用」这样的非肯定词。这表明我们的这条推理链不是必然的。要取代 VQ，我们可以从两个方面入手：</p>
<ol>
<li>换一种更强力的把连续图像变成离散图像的方法</li>
<li>从更根本处入手，不用类别分布来建模下一项数据</li>
</ol>
<p>论文的作者选择了第二种做法：不就是建模一个像素值的分布吗？我们为什么要用死板的类别分布呢？既然扩散模型如此强大，能够拟合复杂的图像分布，那用它来拟合一个像素值的分布还不是轻轻松松？论文的核心思想也就呼之欲出了：<strong>用扩散模型而不是类别分布来建模自回归模型中下一个像素值的分布，从而抛弃自编码器里的 VQ 操作，提升模型能力。</strong></p>
<p>可能读者第一次看到这个想法时会有些疑惑：扩散模型不是用来生成一整张图像的吗？它怎么建模一个像素值的分布？它和自回归模型又有什么关系？我们来多花点时间深入理解这个想法。</p>
<p>在文本自回归生成中，输入是已生成文本，输出是下一个词元的类别分布。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/6.jpg" alt></p>
<p>而在图像自回归生成中，输入是已生成像素，输出是下一个像素的类别分布。现在，我们希望不用类别分布，而用另一种方式，<strong>根据之前的像素生成出下一个像素</strong>。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/7.jpg" alt></p>
<p>论文作者从扩散模型中获取了灵感。扩散模型是一种强力的生成模型，它可以不根据任何信息，或根据类别、文本等信息，隐式建模训练集的图像分布，从而生成符合训练集分布的图像。既然扩散模型能够建模复杂的图像分布，那它也可以根据之前像素的信息，建模下一个像素的分布。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/8.jpg" alt></p>
<p>那么，在这种新式自回归模型里，我们可以用约束于 Transformer 输出的上下文信息的扩散模型来建模下一个像素的分布，尽管现在我们并不知道每种颜色出现的概率。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/9.jpg" alt></p>
<p>这样做的好处是，以前我们只能用离散的有限类型的颜色（准确来说是图像词元）来表示图像，现在我们能够用连续值来表示图像。模型能够更加轻松地生成内容丰富的图像。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/10.jpg" alt></p>
<p>当然，抛弃了 VQ 后，自回归模型确实不需要 VQ 自编码器来把连续图像变成离散图像了。但是，我们依然需要用自编码器来压缩图像，减少要生成的像素数。本工作依然采取了 VQ-VAE、VQGAN 那种两阶段的生成方式，只不过把 VQ 自编码器换成了用 KL loss 约束的 VAE。</p>
<p>训练这种扩散模型的方法很简单。在每一步训练时，我们知道上下文像素是什么，也知道当前像素的真值是什么。那么，只要以上下文像素为约束，用当前像素的真值去训练一个带约束扩散模型就行了。作者把训练这种隐式描述下一个像素值分布的误差函数称为 Diffusion Loss。</p>
<p>具体来说，本工作使用了最基础的带约束 DDPM 扩散模型。它和标准 DDPM 的唯一区别在于误差函数多了一个约束信息 $z$，该信息是上下文像素过 Transformer 的输出。</p>
<script type="math/tex; mode=display">
L(z, x) = \mathbb{E}_{\epsilon}[||\epsilon-\epsilon_{\theta}(x_t|t, z)||^2]</script><p>$t$ 时刻的噪声图像 $x_t$也是由 DDPM 加噪公式得来的。</p>
<script type="math/tex; mode=display">
x_t = \sqrt{\bar{\alpha_t}} x + \sqrt{1 - \bar{\alpha_t}} \epsilon</script><p>Diffusion Loss 不仅可以用来训练表示分布的扩散模型，还可以训练前面提取上下文信息的 Transformer。由于约束信息 $z$ 来自 Transformer，可以把 Diffusion Loss 的梯度通过 $z$ 回传到 Transformer 的参数里。</p>
<p>扩散模型的采样公式也和 DDPM 的一样，这里不再赘述。特别地，以前的自回归模型在使用类别分布时，会用温度来控制采样的多样性。为了在扩散模型中也加入类似的温度参数，本工作参考了 <em>Diffusion models beat GANs on image synthesis</em> 论文的有关设计。</p>
<p>在具体模型超参数上，本工作的 DDPM 训练时有 1000 步，采样时有 100 步。乍看之下，DDPM 会为整个生成模型增加许多计算量，但由于只需要建模一个像素的分布，这套模型的 DDPM 可以用非常轻量级的结构。默认配置下，这套模型的 DDPM 的去噪模型是一个由 3 个残差块组成小型 MLP。每个残差块由 LayerNorm、线性层、SiLU、线性层组成。约束信息 $z$ 会和时刻 $t$ 的编码加在一起，用 DiT (<em>Scalable diffusion models with Transformers</em>) 里的 AdaLN 约束机制输入进 LayerNorm 层里。</p>
<h2 id="套用更先进的自回归模型"><a href="#套用更先进的自回归模型" class="headerlink" title="套用更先进的自回归模型"></a>套用更先进的自回归模型</h2><p>仅是去掉 VQ，把 Diffusion Loss 加进标准自回归模型，并不能得到一个很好的图像生成模型。于是，作者用更加先进的一些自回归模型（掩码生成模型 Masked Gernerative Models，如 <em>MaskGIT: Masked generative image Transformer</em>、<em>MAGE: Masked generative encoder to unify representation learning and image synthesis</em>）代替标准自回归模型，极大提升了模型的生成能力。</p>
<h3 id="双向注意力"><a href="#双向注意力" class="headerlink" title="双向注意力"></a>双向注意力</h3><p>在标准 Transformer 中（如下图 (a) causal 所示），每一个词元只能看到自己及之前词元的信息。这样做的好处是模型能够并行训练，串行推理。训练和推理的速度都会比较快。但是，由于每个词元看不到后面词元的信息，Transformer 提取整个句子（图像）特征的能力会下降。</p>
<p>而 MAE (<em>Masked autoencoders are scalable vision learners</em>) 论文提出了一种双向注意力机制，它可以让词元两两之间都传递信息。但是，这样模型就不能用同一个句子并行训练了，也失去了 KV cache 加速推理的手段。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/11.jpg" alt></p>
<blockquote>
<p>如果你不太了解 Transformer 为什么是并行训练，请仔细回顾 Transformer 论文中有关自回归机制的描述。</p>
</blockquote>
<h3 id="广义自回归模型"><a href="#广义自回归模型" class="headerlink" title="广义自回归模型"></a>广义自回归模型</h3><p>除了双向注意力外，作者还将一些掩码生成模型的设计融合进标准自回归模型。这种广义上的自回归模型效果更好，且能缓解双向注意力导致的推理速度慢的问题。</p>
<p>一般来说，用图像自回归模型时，我们都是按从左到右，从上到下的顺序生成词元，如下图 (a) 所示。但是，这种顺序不一定是最合理的。</p>
<p>按理来说，模型应该可以通过任何顺序生成词元，这样模型学到的生成方式更加多样。更合理的生成方式应该如下图 (b) 所示，不是从左到右，从上到下给词元编号，而是随机选择一个排列给图像编号。这样就能按照随机的顺序生成图像的词元了。</p>
<p>而在掩码自回归生成中，模型可以一次性生成任意一个集合的词元。因此，为了加速 (b) 模型，我们可以如下图 (c) 所示，在随机给词元编号后一次生成多个词元。(b) 可以看成是 (c) 一次只预测下一个词元的特例。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/12.jpg" alt></p>
<h2 id="Transformer-模型配置"><a href="#Transformer-模型配置" class="headerlink" title="Transformer 模型配置"></a>Transformer 模型配置</h2><p>本工作并没有给 Transformer 加入新设计，我们来确认一遍论文中介绍的 Transformer 配置。</p>
<p>本工作依然采取了两阶段的生成方法。第一个阶段的自编码器（又可以理解成 NLP 中的 tokenizer）来自 LDM 工作官方仓库的 VQ-16 和 KL-16 模型。前者是 VQ 自编码器（VQGAN），后者是一个加强版的 VAE。</p>
<p>本工作用的 Transformer 和 ViT 一样。得到图像词元后，词元会加上位置编码，且词元序列开头会附加一个 <code>[cls]</code> 词元，用以在类别约束生成任务里输入类别。</p>
<p>基于这个类别词元，本工作使用了一种特别的 Classifier-free guidance (CFG) 机制：模型用一个假类别词元来表示「类别不明」。训练时，10% 的正确类别词元被替换成了假类别词元。这样，在用扩散模型时，就可以根据标准 CFG 的做法，用正确类别和假类别实现 CFG。详情请参见论文附录 B。</p>
<p>在训练掩码自回归模型时，70%~100% 的词元是未知的。由于采样序列可能会很短，作者在输入序列前附加了 64 个 <code>[cls]</code> 词元。掩码自回归模型的其他主要设计都与 MAE 相同。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>本工作面向的是图像生成任务，主要评估 ImageNet 数据集上按类别生成的 FID 和 IS 指标。FID 越低越好，IS 越高越好。这篇工作的实验结果中有许多信息，让我们来仔细看一看这份结果。</p>
<h3 id="Diffusion-Loss-与广义自回归模型"><a href="#Diffusion-Loss-与广义自回归模型" class="headerlink" title="Diffusion Loss 与广义自回归模型"></a>Diffusion Loss 与广义自回归模型</h3><p>论文首先展示了 Diffusion Loss、广义自回归模型这两项主要设计的优越性，如下表所示。由于图像是按类别生成的，可以用 CFG 提升模型的生成效果。为了公平比较，模型使用的 VQ 自编码器和 KL 自编码器都来自 LDM 仓库。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/13.jpg" alt></p>
<p>表格的 4 大行展示了改进自回归模型的影响，每一大行里不同 loss 的对比体现了 Diffusioin Loss 的影响。</p>
<p>从第一大行可以看出，Diffusion Loss 似乎对标准自回归的改进不是很明显，且这一套方法的生成能力并不出色。只有把自回归模型逐渐改进后，Diffusion Loss 的效果才能逐渐体现出来。在后几行掩码自回归模型中，Diffusion Loss 的作用还是很大的。</p>
<p>而对比前三大行，我们可以发现自回归模型的架构极大地提升了生成效果，且似乎将 Transformer 由 causal 改成 bidirect 的提升更加显著。</p>
<p>第四大行相比第三大行，提升了每次预测的词元数，主要是为了加速。这两行的对比结果表明，做了这个加速操作后，模型生成能力并没有下降多少。后续实验都是基于第四行的配置。</p>
<h3 id="Diffusion-Loss-适配不同的自编码器"><a href="#Diffusion-Loss-适配不同的自编码器" class="headerlink" title="Diffusion Loss 适配不同的自编码器"></a>Diffusion Loss 适配不同的自编码器</h3><p>相比原来类别分布，用 Diffusion Loss 解除了自编码器必须输出离散图像的限制。因此，目前的模型能够适配多种自编码器，如下表所示。图中 rFID 指的是图像重建任务的 FID，越低越好。这里的 VQ-16 指的是将 VQGAN 的 VQ 层当作解码器的一部分，这样 VQGAN 的编码器输出也可以看成是连续图像，和 LDM 里的做法一样。最后一行的 KL-16 是作者重新重新在 ImageNet 上训练的 VAE，而前两行的 VQ-16 和 KL-16 是在 OpenImages 上训练的。由于后文的实验都基于 ImageNet，所以后文都会用第五行那个 VAE。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/14.jpg" alt></p>
<p>首先对比一下这里 VQ-16 w/o CFG 的 FID 和上表里最后一大行 CrossEnt 的 FID。这两组实验的自编码器相同，仅有误差函数不同。将误差函数从交叉熵换成了 Diffusion Loss 后，FID 从 8.79 变成了 7.82。这一项直接对比的实验证明了不考虑自编码器的改进时，Diffusion Loss 本身的优越性。</p>
<p>再对比前两行，KL 的自编码器无论是图像恢复指标还是最后的生成指标都优于 VQ 的自编码器。这印证了论文开头想要抛弃 VQ 自编码器的动机：VQ 自编码器逊于 KL 自编码器。</p>
<p>第三、第四行展示了方法也可以兼容下采样 8 倍的自编码器。本来测试用的 ImageNet 是 $256 \times 256$ 大小的，按照一开始下采样 16 倍的配置，能得到 $16 \times 16$ 的压缩图像，即输入 Transformer 的词元序列长度为 $16 \times 16$。现在改成了下采样 8 倍后，为了兼容之前 $16 \times 16$ 的序列长度，作者把 $2 \times 2$ 个像素打包成一个词元。论文里没讲是怎么打包的，我猜测是在通道上拼接。Consistency 是另一套自编码器，作者展示这个估计是为了说明这套方法兼容性很强。</p>
<h3 id="和-SOTA-图像生成模型对比"><a href="#和-SOTA-图像生成模型对比" class="headerlink" title="和 SOTA 图像生成模型对比"></a>和 SOTA 图像生成模型对比</h3><p>为了证明方法的优越性，论文还展示了本工作与其他 SOTA 工作在 ImageNet 图像生成任务上的定量对比结果。下表是 ImageNet $256 \times 256$ 的结果。为了方便对比，我还贴出了 DiT 论文里展示的表格（左表）。本文的模型在表里被称作 MAR。 </p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/15.jpg" alt></p>
<p>下表是 ImageNet $512 \times 512$ 的结果。左边那张表是 EDM2 展示的结果。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/16.jpg" alt></p>
<p>从表里可以看出，本工作在 ImageNet 图像生成任务上表现很不错，超越了绝大多数模型。</p>
<h3 id="图像生成速度对比"><a href="#图像生成速度对比" class="headerlink" title="图像生成速度对比"></a>图像生成速度对比</h3><p>下面是不同生成模型的速度对比结果。第一张图是本论文展示的和 DiT 的对比结果。DIT 采用的扩散模型采样步数是 (50 ,75, 150, 250)。由于本工作的性能瓶颈在自回归模型而不在扩散模型上，所以本工作展示的不同采样步数由自回归步数决定。图中的自回归步数是 (8, 16, 32, 64, 128)。中间的图是 LDM 的结果，同模型不同点表示的是采样步数为 (10, 20, 50, 100, 200) 的结果。右边的表是 EDM2 的采样速度等指标。左边两张图是 ImageNet $256 \times 256$ 上的，最右边的表是 ImageNet $512 \times 512$ 上的。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/17.jpg" alt></p>
<p>由于不同图表的采样速度指标不太一样，我们将指标统一成每秒生成的图像。从第一张图的对比可以看出，DiT 最快也是一秒 2.5 张图像左右，而 MAR 又快又好，默认（自回归步数 64）一秒生成 3 张图左右。同时，通过 MAR 和有 kv cache 加速的标准 AR 的对比，我们能发现 MAR 在默认自回归步数下还是比标准 AR 慢了不少。</p>
<p>我们再看中间 LDM 的速度。我们观察一下最常使用的 LDM-8。如果是令 DDIM 步数为 20 （第二快的结果）的话，LDM-8 的生成速度在一秒 16 张图像左右，还是比 MAR 快很多。DDIM 步数取 50 时也会比 MAR 快一些。</p>
<p>最后看右边较新的图像扩散模型 EDM2 的速度。由于这个是在 $512 \times 512$ 的图片上测试的，和前面的速度相比时大概要乘个 4。哪怕是最大的 XXL 模型，在有 guidance 时，生成速度也是 2 张图片每秒。换算到 $256 \times 256$ 上约 8 张图片每秒，还是比 MAR 快。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>自回归图像生成中的向量离散化和类别分布必须同时使用。为了去除表现较差的向量离散化操作，本工作的作者重新用扩散模型建模了自回归中下一个图像词元的分布，从而提升了模型的生成能力。由于标准自回归模型生成能力有限，为了进一步提升模型，作者又引入了最新的掩码自回归模型。最终的模型在 ImageNet 图像生成指标上取得了几乎最顶尖的结果。</p>
<p>以上是论文的叙述逻辑。但掩码自回归那一块应该是之前工作的研究成果，这篇文章实际上就是把新提出的 diffusion loss 用到了掩码自回归上，把本来在 ImageNet 上生成能力尚可的掩码自回归推到了最前列。</p>
<p>这篇文章在科研上的最大创新是打破了大家在图像自回归上的固有思维，认为必须用离散词元，必须用类别分布。但仔细一想，建模一个分布的方法其实许许多多。随便把另一种生成完整图像的模型用到生成一个像素上，就能取代之前的类别分布，得到更好的图像生成结果。这篇文章用简单的 DDPM 只是为了验证这个想法的可行性，用更复杂的模型或许能有更好的结果，但用 DDPM 做验证就足够了。之后肯定会有各种后续工作，研究如何用更好的模型来建模本框架中一个像素值的分布。</p>
<p>反过来想，这篇文章也在提醒我们，扩散模型并不只是可以用来生成图像，它的本质是建模一个分布。如果某个模型中间需要建模一个简单的分布的话，都可以尝试用 DDPM。</p>
<p>相比其科研创新，这篇文章在 ImageNet 图像生成指标的成就反而没有那么耀眼了。本工作在 ImageNet 的 FID 等指标上取得了几乎最优的结果，战胜了多数最强的扩散模型，有望将大家的科研眼光从扩散模型移到自回归上。但由于自回归本身步数较多，且每一步要在 Transformer 里做完整的注意力操作，这种方法的速度还是比扩散模型要慢一点。</p>
<p>目前 GitHub 上已有本工作的复现：<a target="_blank" rel="noopener" href="https://github.com/lucidrains/autoregressive-diffusion-pytorch">https://github.com/lucidrains/autoregressive-diffusion-pytorch</a> 。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/07/27/20240605-diffusers-training/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/07/27/20240605-diffusers-training/" class="post-title-link" itemprop="url">定制适合自己的 Diffusers 扩散模型训练脚本</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-07-27 20:31:57" itemprop="dateCreated datePublished" datetime="2024-07-27T20:31:57+08:00">2024-07-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%88%9B%E4%BD%9C/" itemprop="url" rel="index"><span itemprop="name">创作</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%88%9B%E4%BD%9C/%E7%BC%96%E7%A8%8B%E9%A1%B9%E7%9B%AE/" itemprop="url" rel="index"><span itemprop="name">编程项目</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Diffusers 库为社区用户提供了多种扩散模型任务的训练脚本。每个脚本都平铺直叙，没有多余的封装，把训练的绝大多数细节都写在了一个脚本里。这种设计既能让入门用户在不阅读源码的前提下直接用脚本训练，又方便高级用户直接修改脚本。</p>
<p>可是，这种设计就是最好的吗？关于训练脚本的最佳设计风格，社区用户们往往各执一词。有人更喜欢更贴近 PyTorch 官方示例的写法，而有人会喜欢用 PyTorch Lightning 等封装度高、重复代码少的库。而在我看来，选择哪种风格的训练脚本，确实是个人喜好问题。但是，在开始使用训练脚本之前，我们要从细节入手，理解训练脚本到底要做哪些事。学懂了之后，不管是用别人的训练库，还是定制适合自己的训练脚本，都是很轻松的。不管怎么说，Diffusers 的这种训练脚本是一份很好的学习素材。</p>
<p>当然，我在用 Diffusers 的训练脚本时，发现一旦涉及多类任务的训练，比如既要能训练 Stable Diffusion，又要能训练 VAE，那么这份脚本就会用起来比较困难，而写两份训练脚本又会有很大的冗余。Diffusers 的训练脚本依然有改进的空间。</p>
<p>在这篇文章中，我会主要面向想系统性学习扩散模型训练框架的读者，先详细介绍 Diffusers 官方训练脚本，再分享我重构训练脚本的过程，使得脚本能够更好地兼容多类模型的训练。文章的末尾，我会展示几个简单的扩散模型训练实例。</p>
<p>在阅读本文时，建议大家用电脑端，一边看源代码一边读文章。「官方训练脚本细读」一节细节较多，初次阅读时可以快速浏览，看完「训练脚本内容总结」中的流程图，再回头仔细看一遍。</p>
<h2 id="准备源代码"><a href="#准备源代码" class="headerlink" title="准备源代码"></a>准备源代码</h2><p>我们将以最简单的 DDPM 官方训练脚本 <code>examples/unconditional_image_generation/train_unconditional.py</code> 为例，学习训练脚本的通用写法。<code>examples</code> 文件夹在位于 Diffusers 官方 GitHub 仓库中，用 pip 安装的 Diffusers 可能没有这个文件夹，最好是手动 clone 官方仓库，再在本地查看这个文件夹。使用 Diffusers 训练时，可能还要安装其他库。官方在不同的训练教程里给了不同的安装指令，建议大家都安装上。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd examples/text_to_image</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install diffusers[training]</span><br></pre></td></tr></table></figure>
<p>我为本教程准备的脚本在仓库 <a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample">https://github.com/SingleZombie/DiffusersExample</a> 中。请 clone 这个仓库，再切换到 <code>TrainingScript</code> 目录下。<code>train_official.py</code> 是原官方训练脚本 <code>train_unconditional.py</code>，<code>train_0.py</code> 是第一次修改后的训练脚本<br>，<code>train_1.py</code> 是第二次修改后的训练脚本。</p>
<h2 id="官方训练脚本细读"><a href="#官方训练脚本细读" class="headerlink" title="官方训练脚本细读"></a>官方训练脚本细读</h2><p>先拉到文件的最底部，我们能在这找到程序的入口。在 <code>parse_args</code> 函数中，脚本会用 <code>argparse</code> 库解析命令行参数，并将所有参数保存在 <code>args</code> 里。<code>args</code> 会传进 <code>main</code> 函数里。稍后我们看到所有 <code>args.</code> 打头的变量调用，都表明该变量来自于命令行参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    args = parse_args()</span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure>
<p>接着，我们正式开始学习训练主函数。一开始，函数会配置 accelerate 库及日志记录器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">logging_dir = os.path.join(args.output_dir, args.logging_dir)</span><br><span class="line">accelerator_project_config = ProjectConfiguration(</span><br><span class="line">    project_dir=args.output_dir, logging_dir=logging_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># a big number for high resolution or big dataset</span></span><br><span class="line">kwargs = InitProcessGroupKwargs(timeout=timedelta(seconds=<span class="number">7200</span>))</span><br><span class="line">accelerator = Accelerator(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.logger == <span class="string">&quot;tensorboard&quot;</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_tensorboard_available():</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">elif</span> args.logger == <span class="string">&quot;wandb&quot;</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_wandb_available():</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">import</span> wandb</span><br></pre></td></tr></table></figure>
<p>在配置日志的中途，函数插入了一段修改模型存取逻辑的代码。为了让我们阅读代码的顺序与实际运行顺序一致，我们等待会用到了这段代码时再回头来读。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `accelerate` 0.16.0 will have better support for customized saving</span></span><br><span class="line"><span class="keyword">if</span> version.parse(accelerate.__version__) &gt;= version.parse(<span class="string">&quot;0.16.0&quot;</span>):</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_model_hook</span>(<span class="params">models, weights, output_dir</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_model_hook</span>(<span class="params">models, input_dir</span>):</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>跳过上面的代码，还是日志配置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make one log on every process with the configuration for debugging.</span></span><br><span class="line">logging.basicConfig(</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&quot;%(asctime)s - %(levelname)s - %(name)s - %(message)s&quot;</span>,</span><br><span class="line">    datefmt=<span class="string">&quot;%m/%d/%Y %H:%M:%S&quot;</span>,</span><br><span class="line">    level=logging.INFO,</span><br><span class="line">)</span><br><span class="line">logger.info(accelerator.state, main_process_only=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">if</span> accelerator.is_local_main_process:</span><br><span class="line">    datasets.utils.logging.set_verbosity_warning()</span><br><span class="line">    diffusers.utils.logging.set_verbosity_info()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    datasets.utils.logging.set_verbosity_error()</span><br><span class="line">    diffusers.utils.logging.set_verbosity_error()</span><br></pre></td></tr></table></figure>
<p>之后其他版本的训练脚本会有一段设置随机种子的代码，我们给这份脚本补上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If passed along, set the training seed now.</span></span><br><span class="line"><span class="keyword">if</span> args.seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    set_seed(args.seed)</span><br></pre></td></tr></table></figure>
<p>接着，函数会创建输出文件夹。如果我们想把模型推送到在线仓库上，函数还会创建一个仓库。</p>
<p>这段代码还出现了一行比较重要的判断语句：<code>if accelerator.is_main_process:</code>。在多卡训练时，只有主进程会执行这个条件语句块里的内容。该判断在并行编程中十分重要。很多时候，比如在输出、存取模型时，我们只需要让一个进程执行操作就行了。这个时候就要用到这行判断语句。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Handle the repository creation</span></span><br><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    <span class="keyword">if</span> args.output_dir <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        os.makedirs(args.output_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.push_to_hub:</span><br><span class="line">        repo_id = create_repo(...).repo_id</span><br></pre></td></tr></table></figure>
<p>准备完辅助工具后，函数开始准备模型。输入参数里的 <code>model_config_name_or_path</code> 表示预定义的模型配置文件。如果该配置文件不存在，则函数会用默认的配置创建一个 DDPM 的 U-Net 模型。在写我们自己的训练脚本时，我们需要在这个地方初始化我们需要的所有模型。比如训练 Stable Diffusion 时，除了 U-Net，需要在此处准备 VAE、CLIP 文本编码器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the model</span></span><br><span class="line"><span class="keyword">if</span> args.model_config_name_or_path <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    model = UNet2DModel(...)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    config = UNet2DModel.load_config(args.model_config_name_or_path)</span><br><span class="line">    model = UNet2DModel.from_config(config)</span><br></pre></td></tr></table></figure>
<p>这份脚本还帮我们写好了维护 EMA（指数移动平均）模型的功能。EMA 模型用于存储模型可学习的参数的局部平均值。有时 EMA 模型的效果会比原模型要好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create EMA for the model.</span></span><br><span class="line"><span class="keyword">if</span> args.use_ema:</span><br><span class="line">    ema_model = EMAModel(</span><br><span class="line">        model.parameters(),</span><br><span class="line">        model_cls=UNet2DModel,</span><br><span class="line">        model_config=model.config,</span><br><span class="line">        ...)</span><br></pre></td></tr></table></figure>
<p>此处函数还会根据 accelerate 配置自动设置模型的精度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">weight_dtype = torch.float32</span><br><span class="line"><span class="keyword">if</span> accelerator.mixed_precision == <span class="string">&quot;fp16&quot;</span>:</span><br><span class="line">    weight_dtype = torch.float16</span><br><span class="line">    args.mixed_precision = accelerator.mixed_precision</span><br><span class="line"><span class="keyword">elif</span> accelerator.mixed_precision == <span class="string">&quot;bf16&quot;</span>:</span><br><span class="line">    weight_dtype = torch.bfloat16</span><br><span class="line">    args.mixed_precision = accelerator.mixed_precision</span><br></pre></td></tr></table></figure>
<p>函数还会尝试启用 <code>xformers</code> 来提升 Attention 的效率。PyTorch 在 2.0 版本也加入了类似的 Attention 优化技术。如果你的显卡性能有限，且 PyTorch 版本小于 2.0，可以考虑使用 <code>xformers</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.enable_xformers_memory_efficient_attention:</span><br><span class="line">    <span class="keyword">if</span> is_xformers_available():</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>准备了 U-Net 后，函数会准备噪声调度器，即定义扩散模型的细节。</p>
<blockquote>
<p>注意，扩散模型不是一个神经网络，而是一套定义了加噪、去噪公式的模型。扩散模型中需要一个去噪模型来去噪，去噪模型一般是一个神经网络。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the scheduler</span></span><br><span class="line">accepts_prediction_type = <span class="string">&quot;prediction_type&quot;</span> <span class="keyword">in</span> <span class="built_in">set</span>(</span><br><span class="line">    inspect.signature(DDPMScheduler.__init__).parameters.keys())</span><br><span class="line"><span class="keyword">if</span> accepts_prediction_type:</span><br><span class="line">    noise_scheduler = DDPMScheduler(...)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    noise_scheduler = DDPMScheduler(...)</span><br></pre></td></tr></table></figure>
<p>准备完所有扩散模型组件后，函数开始准备其他和训练相关的模块。其他版本的训练脚本会在这个地方加一段缓存梯度和自动放缩学习率的代码，我们给这份脚本补上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.gradient_checkpointing:</span><br><span class="line">    unet.enable_gradient_checkpointing()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.scale_lr:</span><br><span class="line">    args.learning_rate = (</span><br><span class="line">        args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>函数先准备的训练模块是优化器。这里默认使用的优化器是 <code>AdamW</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(</span><br><span class="line">    model.parameters(),</span><br><span class="line">    lr=args.learning_rate,</span><br><span class="line">    betas=(args.adam_beta1, args.adam_beta2),</span><br><span class="line">    weight_decay=args.adam_weight_decay,</span><br><span class="line">    eps=args.adam_epsilon,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>函数随后会准备训练集。这个脚本用 HuggingFace 的 datasets 库来管理数据集。我们既可以读取在线数据集，也可以读取本地的图片文件夹数据集。自定义数据集的方法可以参考 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder">https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder</a> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.dataset_name <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    dataset = load_dataset(</span><br><span class="line">        args.dataset_name,</span><br><span class="line">        args.dataset_config_name,</span><br><span class="line">        cache_dir=args.cache_dir,</span><br><span class="line">        split=<span class="string">&quot;train&quot;</span>,</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    dataset = load_dataset(</span><br><span class="line">        <span class="string">&quot;imagefolder&quot;</span>, data_dir=args.train_data_dir, cache_dir=args.cache_dir, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">    <span class="comment"># See more about loading custom images at</span></span><br><span class="line">    <span class="comment"># https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder</span></span><br></pre></td></tr></table></figure>
<p>有了数据集后，函数会继续准备 PyTorch 的 DataLoader。在这一步中，除了定义 DataLoader 外，我们还要编写数据预处理的方法。下面这段代码的编写顺序和执行顺序不同，我们按执行顺序来整理一遍下面的代码：</p>
<ol>
<li>将预定义的预处理函数传给数据集对象 <code>dataset.set_transform(transform_images)</code>。在使用数据集里的数据时，才会调用这个函数预处理图像。</li>
<li>使用 PyTorch API 定义 DataLoader。<code>train_dataloader = ...</code></li>
<li>每次用 DataLoader 获取数据时，一个数据词典 <code>examples</code> 会被传入预处理函数 <code>transform_images</code>。<code>examples</code> 里既包含了图像数据，也包含了数据的各种标签。而对于无约束图像生成任务，我们只需要图像数据，因此可以直接通过词典的 <code>&quot;image&quot;</code> 键得到 PIL 格式的图像数据。用 <code>convert(&quot;RGB&quot;)</code> 把图像转成三通道后，该 PIL 图像会被传入预处理流水线。</li>
<li>图像预处理流水线 <code>augmentations</code> 是用 Torchvision 里的 <code>transform</code> API 定义的。默认的流水线包括短边缩放至指定分辨率、按分辨率裁剪、随机反转、归一化。</li>
<li>处理过的数据会被存到词典的 <code>&quot;input&quot;</code> 键里。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocessing the datasets and DataLoaders creation.</span></span><br><span class="line">augmentations = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        transforms.Resize(</span><br><span class="line">            args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),</span><br><span class="line">        transforms.CenterCrop(</span><br><span class="line">            args.resolution) <span class="keyword">if</span> args.center_crop <span class="keyword">else</span> transforms.RandomCrop(args.resolution),</span><br><span class="line">        transforms.RandomHorizontalFlip() <span class="keyword">if</span> args.random_flip <span class="keyword">else</span> transforms.Lambda(<span class="keyword">lambda</span> x: x),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>]),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform_images</span>(<span class="params">examples</span>):</span></span><br><span class="line">    images = [augmentations(image.convert(<span class="string">&quot;RGB&quot;</span>))</span><br><span class="line">                <span class="keyword">for</span> image <span class="keyword">in</span> examples[<span class="string">&quot;image&quot;</span>]]</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;input&quot;</span>: images&#125;</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">f&quot;Dataset size: <span class="subst">&#123;<span class="built_in">len</span>(dataset)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">dataset.set_transform(transform_images)</span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(</span><br><span class="line">    dataset, batch_size=args.train_batch_size, shuffle=<span class="literal">True</span>, num_workers=args.dataloader_num_workers</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在准备工作的最后，函数会准备学习率调度器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the learning rate scheduler</span></span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    args.lr_scheduler,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,</span><br><span class="line">    num_training_steps=(<span class="built_in">len</span>(train_dataloader) * args.num_epochs),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>准备完了所有模块，函数会调用 accelerate 库来把所有模块变成适合并行训练的模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(</span><br><span class="line">    model, optimizer, train_dataloader, lr_scheduler</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.use_ema:</span><br><span class="line">    ema_model.to(accelerator.device)</span><br></pre></td></tr></table></figure>
<p>之后函数还会用 accelerate 库配置训练日志。默认情况下日志名 <code>run</code> 由当前脚本名决定。如果不想让之前的日志被覆盖的话，可以让日志名 <code>run</code> 由当前的时间决定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    run = os.path.split(__file__)[-<span class="number">1</span>].split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">    accelerator.init_trackers(run)</span><br></pre></td></tr></table></figure>
<p>马上就要开始训练了。在此之前，函数会准备全局变量并记录日志。注意，这里函数会算一次总的 batch 数，它由输入 batch 数、进程数（显卡数）、梯度累计步数共同决定。梯度累计是一种用较少的显存实现大 batch 训练的技术。使用这项技术时，训练梯度不会每步优化，而是累计了若干步后再优化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">total_batch_size = args.train_batch_size * \</span><br><span class="line">    accelerator.num_processes * args.gradient_accumulation_steps</span><br><span class="line">num_update_steps_per_epoch = math.ceil(</span><br><span class="line">    <span class="built_in">len</span>(train_dataloader) / args.gradient_accumulation_steps)</span><br><span class="line">max_train_steps = args.num_epochs * num_update_steps_per_epoch</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">&quot;***** Running training *****&quot;</span>)</span><br><span class="line">logger.info(<span class="string">f&quot;  Num examples = <span class="subst">&#123;<span class="built_in">len</span>(dataset)&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(<span class="string">f&quot;  Num Epochs = <span class="subst">&#123;args.num_epochs&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(</span><br><span class="line">    <span class="string">f&quot;  Instantaneous batch size per device = <span class="subst">&#123;args.train_batch_size&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(</span><br><span class="line">    <span class="string">f&quot;  Total train batch size (w. parallel, distributed &amp; accumulation) = <span class="subst">&#123;total_batch_size&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(</span><br><span class="line">    <span class="string">f&quot;  Gradient Accumulation steps = <span class="subst">&#123;args.gradient_accumulation_steps&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(<span class="string">f&quot;  Total optimization steps = <span class="subst">&#123;max_train_steps&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">global_step = <span class="number">0</span></span><br><span class="line">first_epoch = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>在开始训练前，如果设置了 <code>args.resume_from_checkpoint</code>，则函数会读取之前训练过的权重。负责读取训练权重的函数是 <code>load_state</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.resume_from_checkpoint:</span><br><span class="line">    <span class="keyword">if</span> args.resume_from_checkpoint != <span class="string">&quot;latest&quot;</span>:</span><br><span class="line">        path = ..</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Get the most recent checkpoint</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> path <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        accelerator.load_state(os.path.join(args.output_dir, path))</span><br><span class="line">        accelerator.<span class="built_in">print</span>(<span class="string">f&quot;Resuming from checkpoint <span class="subst">&#123;path&#125;</span>&quot;</span>)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>在每个 epoch 中，函数会重置进度条。接着，函数会进入每一个 batch 的训练迭代。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train!</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(first_epoch, args.num_epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    progress_bar = tqdm(total=num_update_steps_per_epoch,</span><br><span class="line">                        disable=<span class="keyword">not</span> accelerator.is_local_main_process)</span><br><span class="line">    progress_bar.set_description(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader):</span><br></pre></td></tr></table></figure>
<p>如果是继续训练的话，训练开始之前会更新当前的步数 <code>step</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Skip steps until we reach the resumed step</span></span><br><span class="line"><span class="keyword">if</span> args.resume_from_checkpoint <span class="keyword">and</span> epoch == first_epoch <span class="keyword">and</span> step &lt; resume_step:</span><br><span class="line">    <span class="keyword">if</span> step % args.gradient_accumulation_steps == <span class="number">0</span>:</span><br><span class="line">        progress_bar.update(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">continue</span></span><br></pre></td></tr></table></figure>
<p>训练的一开始，函数会从数据的 <code>&quot;input&quot;</code> 键里取出图像数据。此处的键名是我们之前在数据预处理函数 <code>transform_images</code> 里写的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clean_images = batch[<span class="string">&quot;input&quot;</span>].to(weight_dtype)</span><br></pre></td></tr></table></figure>
<p>之后函数会设置扩散模型训练中的其他变量，包含随机噪声、时刻。由于本文的重点并不是介绍扩散模型的原理，这段代码我们就快速略过。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">noise = torch.randn(...)</span><br><span class="line">timesteps =...</span><br><span class="line">noisy_images = noise_scheduler.add_noise(</span><br><span class="line">    clean_images, noise, timesteps)</span><br></pre></td></tr></table></figure>
<p>接下来，函数会用去噪网络做前向传播。为了让模型能正确累计梯度，我们要用 <code>with accelerator.accumulate(model):</code> 把模型调用与反向传播的逻辑包起来。在这段代码中，我们会先得到模型的输出 <code>model_output</code>，再根据扩散模型得到损失函数 <code>loss</code>，最后用 accelerate 库的 API <code>accelerator</code> 代替原来 PyTorch API 来完成反向传播、梯度裁剪，并完成参数更新、学习率调度器更新、优化器更新。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> accelerator.accumulate(model):</span><br><span class="line">    <span class="comment"># Predict the noise residual</span></span><br><span class="line">    model_output = model(noisy_images, timesteps).sample</span><br><span class="line"></span><br><span class="line">    loss = ...</span><br><span class="line"></span><br><span class="line">    accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> accelerator.sync_gradients:</span><br><span class="line">        accelerator.clip_grad_norm_(model.parameters(), <span class="number">1.0</span>)</span><br><span class="line">    optimizer.step()</span><br><span class="line">    lr_scheduler.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>确保一步训练结束后，函数会更新和步数相关的变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.sync_gradients:</span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.step(model.parameters())</span><br><span class="line">    progress_bar.update(<span class="number">1</span>)</span><br><span class="line">    global_step += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>在这个地方，函数还会尝试保存模型。默认情况下，每 <code>args.checkpointing_steps</code> 步保存一次中间结果。确认要保存后，函数会算出当前的保存点名称，并根据最大保存点数 <code>checkpoints_total_limit</code> 决定是否要删除以前的保存点。做完准备后，函数会调用 <code>save_state</code> 保存当前训练时的所有中间变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">f accelerator.is_main_process:</span><br><span class="line">    <span class="keyword">if</span> global_step % args.checkpointing_steps == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> args.checkpoints_total_limit <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            checkpoints = os.listdir(args.output_dir)</span><br><span class="line">            checkpoints = [</span><br><span class="line">                d <span class="keyword">for</span> d <span class="keyword">in</span> checkpoints <span class="keyword">if</span> d.startswith(<span class="string">&quot;checkpoint&quot;</span>)]</span><br><span class="line">            checkpoints = <span class="built_in">sorted</span>(</span><br><span class="line">                checkpoints, key=<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x.split(<span class="string">&quot;-&quot;</span>)[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(checkpoints) &gt;= args.checkpoints_total_limit:</span><br><span class="line">                ...</span><br><span class="line"></span><br><span class="line">            save_path = os.path.join(</span><br><span class="line">            args.output_dir, <span class="string">f&quot;checkpoint-<span class="subst">&#123;global_step&#125;</span>&quot;</span>)</span><br><span class="line">            accelerator.save_state(save_path)</span><br><span class="line">            logger.info(<span class="string">f&quot;Saved state to <span class="subst">&#123;save_path&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在这个地方，主函数开头设置的存取模型回调函数终于派上用场了。在调用 <code>save_state</code> 时，会自动触发下面的回调函数来保存模型。如果不加下面的代码，所有模型默认会以 <code>.safetensor</code> 的形式存下来。而用了下面的代码后，模型能够被 <code>save_pretrained</code> 存进一个文件夹里，就像其他标准 Diffusers 模型一样。</p>
<blockquote>
<p>这里的输入参数 <code>models</code> 来自于之前的 <code>accelerator.prepare</code>，感兴趣可以去阅读文档或源码。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_model_hook</span>(<span class="params">models, weights, output_dir</span>):</span></span><br><span class="line">    <span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">        <span class="keyword">if</span> args.use_ema:</span><br><span class="line">            ema_model.save_pretrained(</span><br><span class="line">                os.path.join(output_dir, <span class="string">&quot;unet_ema&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, model <span class="keyword">in</span> <span class="built_in">enumerate</span>(models):</span><br><span class="line">            model.save_pretrained(os.path.join(output_dir, <span class="string">&quot;unet&quot;</span>))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># make sure to pop weight so that corresponding model is not saved again</span></span><br><span class="line">            weights.pop()</span><br></pre></td></tr></table></figure>
<p>与上面的这段代码对应，脚本还提供了读取文件的回调函数。它会在继续中断的训练后调用 <code>load_state</code> 时被调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_model_hook</span>(<span class="params">models, input_dir</span>):</span></span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        load_model = EMAModel.from_pretrained(</span><br><span class="line">            os.path.join(input_dir, <span class="string">&quot;unet_ema&quot;</span>), UNet2DModel)</span><br><span class="line">        ema_model.load_state_dict(load_model.state_dict())</span><br><span class="line">        ema_model.to(accelerator.device)</span><br><span class="line">        <span class="keyword">del</span> load_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(models)):</span><br><span class="line">        <span class="comment"># pop models so that they are not loaded again</span></span><br><span class="line">        model = models.pop()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># load diffusers style into model</span></span><br><span class="line">        load_model = UNet2DModel.from_pretrained(</span><br><span class="line">            input_dir, subfolder=<span class="string">&quot;unet&quot;</span>)</span><br><span class="line">        model.register_to_config(**load_model.config)</span><br><span class="line"></span><br><span class="line">        model.load_state_dict(load_model.state_dict())</span><br><span class="line">        <span class="keyword">del</span> load_model</span><br></pre></td></tr></table></figure>
<p>两个回调函数需要用下面的代码来设置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accelerator.register_save_state_pre_hook(save_model_hook)</span><br><span class="line">accelerator.register_load_state_pre_hook(load_model_hook)</span><br></pre></td></tr></table></figure>
<p>回到最新的代码处。训练迭代的末尾，脚本会记录当前步的日志。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">logs = &#123;<span class="string">&quot;loss&quot;</span>: loss.detach().item(), <span class="string">&quot;lr&quot;</span>: lr_scheduler.get_last_lr()[<span class="number">0</span>], <span class="string">&quot;step&quot;</span>: global_step&#125;</span><br><span class="line"><span class="keyword">if</span> args.use_ema:</span><br><span class="line">    logs[<span class="string">&quot;ema_decay&quot;</span>] = ema_model.cur_decay_value</span><br><span class="line">progress_bar.set_postfix(**logs)</span><br><span class="line">accelerator.log(logs, step=global_step)</span><br></pre></td></tr></table></figure>
<p>执行完了一个 epoch 后，脚本调用 accelerate API 保证所有进程均训练完毕。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">progress_bar.close()</span><br><span class="line">accelerator.wait_for_everyone()</span><br></pre></td></tr></table></figure>
<p>此处脚本可能会在主进程中验证模型或保存模型。如果当前是最后一个 epoch，或者达到了配置指定的验证/保存时刻，脚本就会执行验证/保存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    <span class="keyword">if</span> epoch % args.save_images_epochs == <span class="number">0</span> <span class="keyword">or</span> epoch == args.num_epochs - <span class="number">1</span>:</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % args.save_model_epochs == <span class="number">0</span> <span class="keyword">or</span> epoch == args.num_epochs - <span class="number">1</span>:</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>脚本默认的验证方法是随机生成图片，并用日志库保存图片。生成图片的方法是使用标准 Diffusers 采样流水线 <code>DDPMPipeline</code>。由于此时模型 <code>model</code> 可能被包裹成了一个用于多卡训练的 PyTorch 模块，需要用相关 API 把 <code>model</code> 解包成普通 PyTorch 模块 <code>unet</code>。如果使用了 EMA 模型，为了避免对 EMA 模型的干扰，此处需要先保存 EMA 模型参数，采样结束再还原参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> epoch % args.save_images_epochs == <span class="number">0</span> <span class="keyword">or</span> epoch == args.num_epochs - <span class="number">1</span>:</span><br><span class="line">    unet = accelerator.unwrap_model(model)</span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.store(unet.parameters())</span><br><span class="line">        ema_model.copy_to(unet.parameters())</span><br><span class="line"></span><br><span class="line">    pipeline = DDPMPipeline(</span><br><span class="line">        unet=unet,</span><br><span class="line">        scheduler=noise_scheduler,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    generator = torch.Generator(device=pipeline.device).manual_seed(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># run pipeline in inference (sample random noise and denoise)</span></span><br><span class="line">    images = pipeline(...).images</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.restore(unet.parameters())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># denormalize the images and save to tensorboard</span></span><br><span class="line">    images_processed = (images * <span class="number">255</span>).<span class="built_in">round</span>().astype(<span class="string">&quot;uint8&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.logger == <span class="string">&quot;tensorboard&quot;</span>:</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">elif</span> args.logger == <span class="string">&quot;wandb&quot;</span>:</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>在保存模型时，脚本同样会先用去噪模型 <code>model</code> 构建一个流水线，再调用流水线的保存方法 <code>save_pretrained</code> 将扩散模型的所有组件（去噪模型、噪声调度器）保存下来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> epoch % args.save_model_epochs == <span class="number">0</span> <span class="keyword">or</span> epoch == args.num_epochs - <span class="number">1</span>:</span><br><span class="line">    <span class="comment"># save the model</span></span><br><span class="line">    unet = accelerator.unwrap_model(model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.store(unet.parameters())</span><br><span class="line">        ema_model.copy_to(unet.parameters())</span><br><span class="line"></span><br><span class="line">    pipeline = DDPMPipeline(</span><br><span class="line">        unet=unet,</span><br><span class="line">        scheduler=noise_scheduler,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    pipeline.save_pretrained(args.output_dir)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.restore(unet.parameters())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.push_to_hub:</span><br><span class="line">        upload_folder(...)</span><br></pre></td></tr></table></figure>
<p>一个 epoch 训练的代码就到此结束了。所有 epoch 的训练结束后，脚本调用 API 结束训练。这个 API 会自动关闭所有的日志库。训练代码到这里也就结束了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerator.end_training()</span><br></pre></td></tr></table></figure>
<h2 id="训练脚本内容总结"><a href="#训练脚本内容总结" class="headerlink" title="训练脚本内容总结"></a>训练脚本内容总结</h2><p>大概熟悉了一遍这份训练脚本后，我们可以用下面的流程图概括训练脚本的执行顺序和主要内容。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/1.jpg" alt></p>
<h2 id="去掉命令行参数"><a href="#去掉命令行参数" class="headerlink" title="去掉命令行参数"></a>去掉命令行参数</h2><p>我不喜欢用命令行参数传训练参数，而喜欢把训练参数写进配置文件里，理由有：</p>
<ul>
<li>我一般会直接在命令行里手敲命令。如果命令行参数过多，我则会把要运行的命令及其参数保存在某文件里。这样还不如把参数写在另外的文件里。</li>
<li>将大量参数藏在一个词典 <code>args</code> 里，而不是把所有需用的参数在某处定义好，是一种很差的编程方式。各个参数将难以追踪。</li>
</ul>
<p>在正式重构脚本之前，我做的第一步是去掉脚本中原来的命令行参数，将所有参数先塞进一个数据类里面。脚本将只留一个命令行参数，表示参数配置文件的路径。具体做法如下：</p>
<p>先编写一个存命令行参数的数据类。这个类是一个 Python 的 <code>dataclass</code>。Python 中 <code>dataclass</code> 是一种专门用来放数据的类。定义数据类时，我们只需要定义类中所有数据的类型及默认值，不需要编写任何方法。初始化数据类时，我们只需要传一个词典或列表。一个示例如下（示例来源 <a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/understanding-python-dataclasses/）：">https://www.geeksforgeeks.org/understanding-python-dataclasses/）：</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"> </span><br><span class="line"><span class="comment"># A class for holding an employees content</span></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">employee</span>:</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Attributes Declaration</span></span><br><span class="line">    <span class="comment"># using Type Hints</span></span><br><span class="line">    name: <span class="built_in">str</span></span><br><span class="line">    emp_id: <span class="built_in">str</span></span><br><span class="line">    age: <span class="built_in">int</span></span><br><span class="line">    city: <span class="built_in">str</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">emp1 = employee(<span class="string">&quot;Satyam&quot;</span>, <span class="string">&quot;ksatyam858&quot;</span>, <span class="number">21</span>, <span class="string">&#x27;Patna&#x27;</span>)</span><br><span class="line">emp2 = employee(<span class="string">&quot;Anurag&quot;</span>, <span class="string">&quot;au23&quot;</span>, <span class="number">28</span>, <span class="string">&#x27;Delhi&#x27;</span>)</span><br><span class="line">emp3 = employee(&#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;Satyam&quot;</span>, </span><br><span class="line">   <span class="string">&quot;emp_id&quot;</span>: <span class="string">&quot;ksatyam858&quot;</span>, </span><br><span class="line">   <span class="string">&quot;age&quot;</span>: <span class="number">21</span>, </span><br><span class="line">   <span class="string">&quot;city&quot;</span>: <span class="string">&#x27;Patna&#x27;</span>&#125;)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;employee object are :&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(emp1)</span><br><span class="line"><span class="built_in">print</span>(emp2)</span><br><span class="line"><span class="built_in">print</span>(emp3)</span><br></pre></td></tr></table></figure></p>
<p>我们可以用 <code>dataclass</code> 编写一个存储所有命令行参数的数据类，该类开头内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseTrainingConfig</span>:</span></span><br><span class="line">    <span class="comment"># Dir</span></span><br><span class="line">    logging_dir: <span class="built_in">str</span></span><br><span class="line">    output_dir: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Logger and checkpoint</span></span><br><span class="line">    logger: <span class="built_in">str</span> = <span class="string">&#x27;tensorboard&#x27;</span></span><br><span class="line">    checkpointing_steps: <span class="built_in">int</span> = <span class="number">500</span></span><br><span class="line">    checkpoints_total_limit: <span class="built_in">int</span> = <span class="number">20</span></span><br><span class="line">    valid_epochs: <span class="built_in">int</span> = <span class="number">100</span></span><br><span class="line">    valid_batch_size: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line">    save_model_epochs: <span class="built_in">int</span> = <span class="number">100</span></span><br><span class="line">    resume_from_checkpoint: <span class="built_in">str</span> = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>之后在训练脚本里，我们可以把旧的命令行参数全删了，再加一个命令行参数 <code>cfg</code>，表示训练配置文件的路径。我们可以用 <code>omegaconf</code> 打开这个配置文件，得到一个词典 <code>data_dict</code>，再用这个词典构建配置文件 <code>cfg</code>。接下来，只需要把原来代码里所有 <code>args.</code> 改成 <code>cfg.</code> 就行了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> omegaconf <span class="keyword">import</span> OmegaConf</span><br><span class="line"><span class="keyword">from</span> training_cfg_0 <span class="keyword">import</span> BaseTrainingConfig</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&#x27;cfg&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">data_dict = OmegaConf.load(args.cfg)</span><br><span class="line">cfg = BaseTrainingConfig(**data_dict)</span><br></pre></td></tr></table></figure>
<p>第一次修改过的训练脚本为 <code>train_0.py</code>，配置文件类在 <code>training_cfg_0.py</code> 里，示例配置文件为 <code>cfg_0.json</code>，一个简单 DDPM 模型配置写在 <code>unet_cfg</code> 目录里。可以直接运行下面的命令测试此训练脚本。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_0.py cfg_0.json</span><br></pre></td></tr></table></figure>
<p>在配置文件里，我们只需要改少量的训练参数就行了。如果想知道还有哪些参数可以改，可以去查看 <code>training_cfg_0.py</code> 文件。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;logging_dir&quot;</span>: <span class="string">&quot;logs&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;output_dir&quot;</span>: <span class="string">&quot;models/ddpm_0&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;model_config&quot;</span>: <span class="string">&quot;unet_cfg&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;num_epochs&quot;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">&quot;train_batch_size&quot;</span>: <span class="number">64</span>,</span><br><span class="line">    <span class="attr">&quot;checkpointing_steps&quot;</span>: <span class="number">5000</span>,</span><br><span class="line">    <span class="attr">&quot;valid_epochs&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;valid_batch_size&quot;</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="attr">&quot;dataset_name&quot;</span>: <span class="string">&quot;ylecun/mnist&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;resolution&quot;</span>: <span class="number">32</span>,</span><br><span class="line">    <span class="attr">&quot;learning_rate&quot;</span>: <span class="number">1e-4</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>读者感兴趣的话也可以尝试这样改一遍代码。这样做会强迫自己读一遍训练脚本，让自己更熟悉这份代码。</p>
<h2 id="适配多种任务的训练脚本"><a href="#适配多种任务的训练脚本" class="headerlink" title="适配多种任务的训练脚本"></a>适配多种任务的训练脚本</h2><p>如果只是训练一种任务，Diffusers 的这种训练脚本还算好用。但如果我们想用完全相同的训练流程训练多种任务，这种脚本的弊端就暴露出来了：</p>
<ul>
<li>各任务的官方示例脚本本身就不完全统一。比如有的训练脚本支持设置随机种子，有的不支持。</li>
<li>一旦想修改训练过程，就得同时修改所有任务的脚本。这不符合编程中「代码复用」的思想。</li>
</ul>
<p>为此，我想重构一下官方训练脚本，将训练流程和每种任务的具体训练过程解耦开，让一份训练脚本能够被多种任务使用。于是，我又从头过了一遍训练脚本，将代码分成两类：所有任务都会用到的代码、仅 DDPM 训练会用到的代码。如下图所示，我用红字表示了训练脚本中应该由具体任务决定的部分。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/2.jpg" alt></p>
<p>根据这个划分规则，我将仅和 DDPM 相关的代码剥离出来，并用一个描述某具体任务的训练器接口类的方法调用代替原有代码。这样，每次换一个训练任务，只需要重新实现一个训练器类就行了。如下图所示，原流程图中所有红字的内容都可以由接口类的方法代替。对于不同任务，我们需要实现不同的训练器类。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/3.jpg" alt></p>
<p>具体在代码中，我写了一个接口类 <code>Trainer</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span>(<span class="params">metaclass=ABCMeta</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight_dtype, accelerator, logger, cfg</span>):</span></span><br><span class="line">        self.weight_dtype = weight_dtype</span><br><span class="line">        self.accelerator = accelerator</span><br><span class="line">        self.logger = logger</span><br><span class="line">        self.cfg = cfg</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_modules</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                     enable_xformer: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                     gradient_checkpointing: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_optimizers</span>(<span class="params">self, train_batch_size</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_lr_schedulers</span>(<span class="params">self, gradient_accumulation_steps, num_epochs</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_dataset</span>(<span class="params">self, dataset, train_dataloader</span>):</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        self.train_dataloader = train_dataloader</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_modules</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">models_to_train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, global_step, batch</span>) -&gt; <span class="built_in">dict</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">validate</span>(<span class="params">self, epoch, global_step</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_pipeline</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_model_hook</span>(<span class="params">self, models, weights, output_dir</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_model_hook</span>(<span class="params">self, models, input_dir</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>根据类型名和初始化参数可以创建具体的训练器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_trainer</span>(<span class="params"><span class="built_in">type</span>, weight_dtype, accelerator, logger, cfg_dict</span>) -&gt; Trainer:</span></span><br><span class="line">    <span class="keyword">from</span> ddpm_trainer <span class="keyword">import</span> DDPMTrainer</span><br><span class="line">    <span class="keyword">from</span> sd_lora_trainer <span class="keyword">import</span> LoraTrainer</span><br><span class="line"></span><br><span class="line">    __TYPE_CLS_DICT = &#123;</span><br><span class="line">        <span class="string">&#x27;ddpm&#x27;</span>: DDPMTrainer,</span><br><span class="line">        <span class="string">&#x27;lora&#x27;</span>: LoraTrainer</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> __TYPE_CLS_DICT[<span class="built_in">type</span>](weight_dtype, accelerator, logger, cfg_dict)</span><br></pre></td></tr></table></figure>
<p>原来训练脚本里的具体训练逻辑被接口类方法调用代替。比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># old</span></span><br><span class="line"><span class="keyword">if</span> cfg.model_config <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    model = UNet2DModel(...)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    config = UNet2DModel.load_config(cfg.model_config)</span><br><span class="line">    model = UNet2DModel.from_config(config)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create EMA for the model.</span></span><br><span class="line"><span class="keyword">if</span> cfg.use_ema:</span><br><span class="line">    ema_model = EMAModel(...)</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># new</span></span><br><span class="line">trainer.init_modules(enable_xformers, cfg.gradient_checkpointing)</span><br></pre></td></tr></table></figure>
<p>原来仅和 DDPM 训练相关的代码全被我搬到了 <code>DDPMTrainer</code> 类中。与之对应，除了代码需要搬走外，原配置文件里的数据也需要搬走。我在 <code>DDPMTrainer</code> 类里加了一个 <code>DDPMTrainingConfig</code> 数据类，用来存对应的配置数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DDPMTrainingConfig</span>:</span></span><br><span class="line">    <span class="comment"># Diffuion Models</span></span><br><span class="line">    model_config: <span class="built_in">str</span></span><br><span class="line">    ddpm_num_steps: <span class="built_in">int</span> = <span class="number">1000</span></span><br><span class="line">    ddpm_beta_schedule: <span class="built_in">str</span> = <span class="string">&#x27;linear&#x27;</span></span><br><span class="line">    prediction_type: <span class="built_in">str</span> = <span class="string">&#x27;epsilon&#x27;</span></span><br><span class="line">    ddpm_num_inference_steps: <span class="built_in">int</span> = <span class="number">100</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>因此，我们需要用稍微复杂一点的方式来创建配置文件。现在全局训练配置和任务配置放在两组配置里。配置文件最外层除 <code>&quot;base&quot;</code> 外的那个键表明了训练器的类型。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;base&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;logging_dir&quot;</span>: <span class="string">&quot;logs&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;output_dir&quot;</span>: <span class="string">&quot;models/ddpm_1&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;checkpointing_steps&quot;</span>: <span class="number">5000</span>,</span><br><span class="line">        <span class="attr">&quot;valid_epochs&quot;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="attr">&quot;dataset_name&quot;</span>: <span class="string">&quot;ylecun/mnist&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;resolution&quot;</span>: <span class="number">32</span>,</span><br><span class="line">        <span class="attr">&quot;train_batch_size&quot;</span>: <span class="number">64</span>,</span><br><span class="line">        <span class="attr">&quot;num_epochs&quot;</span>: <span class="number">10</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;ddpm&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;model_config&quot;</span>: <span class="string">&quot;unet_cfg&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;learning_rate&quot;</span>: <span class="number">1e-4</span>,</span><br><span class="line">        <span class="attr">&quot;valid_batch_size&quot;</span>: <span class="number">4</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">__TYPE_CLS_DICT = &#123;</span><br><span class="line">    <span class="string">&#x27;base&#x27;</span>: BaseTrainingConfig,</span><br><span class="line">    <span class="string">&#x27;ddpm&#x27;</span>: DDPMTrainingConfig,</span><br><span class="line">    <span class="string">&#x27;lora&#x27;</span>: LoraTrainingConfig</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_training_config</span>(<span class="params">config_path: <span class="built_in">str</span></span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, BaseTrainingConfig]:</span></span><br><span class="line">    data_dict = OmegaConf.load(config_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The config must have a &quot;base&quot; key</span></span><br><span class="line">    base_cfg_dict = data_dict.pop(<span class="string">&#x27;base&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The config must have one another model config</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(data_dict) == <span class="number">1</span></span><br><span class="line">    model_key = <span class="built_in">next</span>(<span class="built_in">iter</span>(data_dict))</span><br><span class="line">    model_cfg_dict = data_dict[model_key]</span><br><span class="line">    model_cfg_cls = __TYPE_CLS_DICT[model_key]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;base&#x27;</span>: BaseTrainingConfig(**base_cfg_dict),</span><br><span class="line">            model_key: model_cfg_cls(**model_cfg_dict)&#125;</span><br></pre></td></tr></table></figure>
<p>这样改完过后，训练脚本开头也需要稍作更改，其他地方保持不变。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> training_cfg_1 <span class="keyword">import</span> BaseTrainingConfig, load_training_config</span><br><span class="line"><span class="keyword">from</span> trainer <span class="keyword">import</span> Trainer, create_trainer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;cfg&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    cfgs = load_training_config(args.cfg)</span><br><span class="line">    cfg: BaseTrainingConfig = cfgs.pop(<span class="string">&#x27;base&#x27;</span>)</span><br><span class="line">    trainer_type = <span class="built_in">next</span>(<span class="built_in">iter</span>(cfgs))</span><br><span class="line">    trainer_cfg_dict = cfgs[trainer_type]</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    trainer: Trainer = create_trainer(</span><br><span class="line">        trainer_type, weight_dtype, accelerator, cfg.logger, trainer_cfg_dict)</span><br></pre></td></tr></table></figure>
<p>这次修改过的训练脚本为 <code>train_1.py</code>，配置文件类在 <code>training_cfg_1.py</code> 里，DDPM 训练器在 <code>TrainingScript/ddpm_trainer.py</code> 里,示例配置文件为 <code>cfg_1.json</code>。可以直接运行下面的命令测试此训练脚本。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_1.py cfg_1.json</span><br></pre></td></tr></table></figure>
<p>运行这一版或者上一版的训练脚本后，我们都能很快训练完一个 MNIST 上的 DDPM 模型。从训练可视化结果可以看出，代码重构大概是没有出错，模型能正确生成图片。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/4.jpg" alt></p>
<blockquote>
<p>对训练器类的程序设计思路感兴趣的话，欢迎阅读附录。</p>
</blockquote>
<h2 id="添加新的训练任务"><a href="#添加新的训练任务" class="headerlink" title="添加新的训练任务"></a>添加新的训练任务</h2><p>为了验证这套新代码的可拓展性，我仿照 Diffusers 官方 SD LoRA 训练脚本 <code>examples/text_to_image/train_text_to_image_lora.py</code>，快速实现了一个 SD LoRA 训练器类。这个类在 <code>sd_lora_trainer.py</code> 文件里。</p>
<p>我来简单介绍添加新训练任务的过程。要添加新训练任务，要修改三处：</p>
<ol>
<li>创建新文件，在文件里定义配置数据类及实现训练器类。</li>
<li>在 <code>trainer.py</code> 里导入新训练器类。</li>
<li>在 <code>training_cfg_1.py</code> 里导入新配置数据类。</li>
</ol>
<p>先来看较简单的第二处和第三处修改。导入新训练器类只需要加一行 import 和一条词典项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_trainer</span>(<span class="params"><span class="built_in">type</span>, weight_dtype, accelerator, logger, cfg_dict</span>) -&gt; Trainer:</span></span><br><span class="line">    <span class="keyword">from</span> ddpm_trainer <span class="keyword">import</span> DDPMTrainer</span><br><span class="line">    <span class="keyword">from</span> sd_lora_trainer <span class="keyword">import</span> LoraTrainer</span><br><span class="line"></span><br><span class="line">    __TYPE_CLS_DICT = &#123;</span><br><span class="line">        <span class="string">&#x27;ddpm&#x27;</span>: DDPMTrainer,</span><br><span class="line">        <span class="string">&#x27;lora&#x27;</span>: LoraTrainer</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> __TYPE_CLS_DICT[<span class="built_in">type</span>](weight_dtype, accelerator, logger, cfg_dict)</span><br></pre></td></tr></table></figure>
<p>导入新配置数据类也一样，一行 import 和一项词典项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sd_lora_trainer <span class="keyword">import</span> LoraTrainingConfig</span><br><span class="line"></span><br><span class="line">__TYPE_CLS_DICT = &#123;</span><br><span class="line">    <span class="string">&#x27;base&#x27;</span>: BaseTrainingConfig,</span><br><span class="line">    <span class="string">&#x27;ddpm&#x27;</span>: DDPMTrainingConfig,</span><br><span class="line">    <span class="string">&#x27;lora&#x27;</span>: LoraTrainingConfig</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而实现一个训练器类会比较繁琐。我是先把 DDPM 训练器类复制了过来，在此基础上进行修改。由于 SD LoRA 训练器有官方训练脚本作为参考，我还是和之前实现 DDPM 训练器一样，从官方训练脚本里抠出对应代码，将其填入训练器类方法里。比如在初始化模块时，我们不仅需要初始化 U-Net，还有 VAE 等模块。在初始化优化器时，应该只优化 LoRA 参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoraTrainer</span>(<span class="params">Trainer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_modules</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                     enable_xformer=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                     gradient_checkpointing=<span class="literal">False</span></span>):</span></span><br><span class="line">        cfg = self.cfg</span><br><span class="line">        <span class="comment"># Load scheduler, tokenizer and models.</span></span><br><span class="line">        self.noise_scheduler = DDPMScheduler...</span><br><span class="line">        self.tokenizer = CLIPTokenizer...</span><br><span class="line">        self.text_encoder = CLIPTextModel... </span><br><span class="line">        self.vae = AutoencoderKL...</span><br><span class="line">        self.unet = UNet2DConditionModel...</span><br><span class="line">        <span class="comment"># freeze parameters of models to save more memory</span></span><br><span class="line">        self.unet.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        self.vae.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        self.text_encoder.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.unet.parameters():</span><br><span class="line">            param.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        unet_lora_config = LoraConfig(...)</span><br><span class="line">        self.lora_layers = <span class="built_in">filter</span>(</span><br><span class="line">                <span class="keyword">lambda</span> p: p.requires_grad, self.unet.parameters())</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_optimizers</span>(<span class="params">self, train_batch_size</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.optimizer = torch.optim.AdamW(</span><br><span class="line">            self.lora_layers,</span><br><span class="line">            ...)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p>SD LoRA 训练器类在 <code>sd_lora_trainer.py</code> 文件里，对应配置文件为 <code>cfg_lora.json</code>。用下面的代码即可尝试 LoRA 训练。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_1.py cfg_lora.json</span><br></pre></td></tr></table></figure>
<p>可能是 MNIST 数据集的图片太小了，而 SD 又是为较大的图片设计的，又或是 LoRA 的拟合能力有限，生成的效果不是很好。但可以看出，SD LoRA 学到了 MNIST 的图片风格。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/5.jpg" alt></p>
<p>就我自己使用下来，添加一个新的训练任务还是非常轻松的。我可以只关心初始化模型、训练、验证等实现细节，而不用关心那些通用的训练代码。当然，这份通用训练脚本还不够强大，还不能处理更复杂的数据集。SD LoRA 其实需要一个带文本标注的数据集，但由于我只是想测试添加新训练器的难度，就没有去改数据集，只是默认用了空文本来训练 LoRA。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我自己在使用 Diffusers 训练脚本时，发现这种训练脚本难以适配多任务训练，于是重构了一份拓展性更强的训练脚本。在这篇文章中，我先是介绍了 Diffusers 训练脚本的通用框架，再分享了我改写脚本的过程。相信读者在读完本文后，不仅能够熟悉 Diffusers 训练脚本的具体原理，还能够动手修改它，或者基于我的这一版改进脚本，编写一份适合自己的训练脚本。</p>
<p>我重构的这套训练器也没有太多封装，在维持 Diffusers 那种平铺直叙风格的同时，将每种训练任务独有的代码、数据搬了出来，让开发者专注于编写新的逻辑。我没怎么用过别的训练框架，不太好直接对比。但至少相比于 PyTorch Lightning 那种模型和训练逻辑写在同一个类里的写法，我更认可 Diffusers 这种将模型结构和训练、采样分离的设计。这套框架的训练器也只有训练的逻辑，不会掺杂其他逻辑。</p>
<p>本文的代码链接为 <a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample/tree/main/TrainingScript">https://github.com/SingleZombie/DiffusersExample/tree/main/TrainingScript</a></p>
<p><strong>注意</strong>，这份代码是我随手写的，只测试了简单的训练命令。如果发现 bug，欢迎提 issue。这份代码仅供本文教学使用，功能有限，以后我会在其他地方更新这份代码。另外，以后我写其他训练教程时也会复用这套代码。</p>
<h2 id="附录：训练器程序设计思路"><a href="#附录：训练器程序设计思路" class="headerlink" title="附录：训练器程序设计思路"></a>附录：训练器程序设计思路</h2><p>在设计训练器接口类的接口时，其实我没有做多少主观设计，基本上都是按照一些设计原则，机械地将原来的训练脚本进行重构。我也不知道这些原则是怎么想出来的，只是根据我多年写代码的经验，我感觉按照这些规则做可以保证训练脚本和训练器之间耦合度更低，易于拓展。这些原则有：</p>
<ol>
<li>如果在另一项任务里这行代码会变动，则这项代码应写入训练器类。</li>
<li>如果某一数据的调用<strong>全部</strong>都被放入了训练器类里，那么这个数据应该是训练器类的成员变量。如果该数据来自配置文件，则将该数据的定义从全局配置移入训练器配置。</li>
<li>如果某数据既要在训练脚本中使用，又要在训练器类里使用，则在训练脚本中初始化该数据，并以<strong>初始化参数</strong>或者<strong>接口参数</strong>两种方式将数据传入训练器。传入方式由数据被确定的时刻决定。比如脚本一开始就初始化好的日志对象应该作为初始化参数，而一些中途计算的当前 batch 数等参数应该作为接口参数。</li>
<li>原则上，训练脚本不从数据类里获取数据。</li>
</ol>
<p>根据这些原则，在设计训练器接口类时，我并没有一开始就定下有哪些接口、接口的参数分别是什么，而是一边搬运代码，一边根据代码的实际内容动态地编写接口类。比如一开始，我的接口类构造函数并没有加入日志库类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span>(<span class="params">metaclass=ABCMeta</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight_dtype, accelerator, cfg</span>):</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>后来写训练器验证方法时，我发现这里必须要获取日志类的类型，不得已在构造函数里多加了一个参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validate</span>(<span class="params">self, epoch, global_step</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> self.logger == <span class="string">&quot;tensorboard&quot;</span>:</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight_dtype, accelerator, logger, cfg</span>):</span></span><br></pre></td></tr></table></figure></p>
<p>原则 3 和原则 4 本质上是将训练脚本也看成一个对象。所有数据要么属于训练脚本，要么属于训练类。原则 4 不从训练器里获取信息，某种程度上体现了面向对象中的封装性，不让训练器去改训练脚本里的数据。我尽可能地遵守了原则 4，但只有一处例外。在调用 <code>accelerate.prapare</code> 后，<code>train_dataloader</code> 在训练器里发生了更改。而 <code>train_dataloader</code> 其实是属于训练脚本的。没办法，这里只能去训练器里获取一次数据。我没来得及仔细研究，说不定 <code>accelerate.prapare</code> 可以多次调用，这样我就能让训练脚本自己维护 <code>train_dataloader</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer.prepare_modules()</span><br><span class="line">train_dataloader = trainer.train_dataloader</span><br></pre></td></tr></table></figure>
<p>这样看下来，这份代码框架在各种角度上都有很大的改进空间。以后我会来慢慢改进这份代码。就目前的设计，训练中整体逻辑、数据集、训练器三部分应该是相互独立的。数据集我还没有单独拿出来写。应该至少实现纯图像、带文本标注图像这两种数据集。</p>
<p>这次重构之后，我也有一些程序设计上的体会。重构代码比从头做程序设计要简单很多。重构只需要根据已有代码，设计出一套更合理的逻辑，像我这样按照某些原则，无脑地修改代码就行了。而程序设计需要考虑未知的情况，为未来可能加入的功能铺路。也正因为从头设计更难，有时会出现设计过度或者设计不足的情况。感觉更合理的开发方式是从头设计与重构交替进行。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/07/14/20240703-SD3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/07/14/20240703-SD3/" class="post-title-link" itemprop="url">Stable Diffusion 3 论文及源码概览</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-07-14 10:26:06" itemprop="dateCreated datePublished" datetime="2024-07-14T10:26:06+08:00">2024-07-14</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">代码阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>近期，最受开源社区欢迎的文生图模型 Stable Diffusion 的最新版本 Stable Diffusion 3 开放了源码和模型参数。开发者宣称，Stable Diffusion 3 使用了全新的模型结构和文本编码方法，能够生成更符合文本描述且高质量的图片。得知 Stable Diffusion 3 开源后，社区用户们纷纷上手测试，在网上分享了许多测试结果。而在本文中，我将面向之前已经熟悉 Stable Diffusion 的科研人员，快速讲解 Stable Diffusion 3 论文的主要内容及其在 Diffusers 中的源码。对于 Stable Diffusion 3 中的一些新技术，我并不会介绍其细节，而是会讲清其设计动机并指明进一步学习的参考文献。</p>
<h2 id="内容索引"><a href="#内容索引" class="headerlink" title="内容索引"></a>内容索引</h2><p>本文会从多个角度简单介绍 SD3，具体要介绍的方面如下所示。读者可以根据自己的需求，跳转到感兴趣的部分阅读。</p>
<h3 id="流匹配原理简介"><a href="#流匹配原理简介" class="headerlink" title="流匹配原理简介"></a>流匹配原理简介</h3><p>流匹配是一种定义图像生成目标的方法，它可以兼容当前扩散模型的训练目标。流匹配中一个有代表性的工作是整流 (rectified flow)，它也正是 SD3 用到的训练目标。我们会在本文中通过简单的可视化示例学习流匹配的思想。</p>
<h3 id="SD3-中的-DiT"><a href="#SD3-中的-DiT" class="headerlink" title="SD3 中的 DiT"></a>SD3 中的 DiT</h3><p>我们会从一个简单的类 ViT 架构开始，学习 SD3 中的去噪网络 DiT 模型是怎么一步一步搭起来的。读者不需要提前学过 DiT，只需要了解 Transformer 的结构，并大概知道视觉任务里的 Transformer 会做哪些通用的修改（如图块化），即可学懂 SD3 里的 DiT。</p>
<h3 id="SD3-模型与训练策略改进细节"><a href="#SD3-模型与训练策略改进细节" class="headerlink" title="SD3 模型与训练策略改进细节"></a>SD3 模型与训练策略改进细节</h3><p>除了将去噪网络从 U-Net 改成 DiT 外，SD3 还在模型结构与训练策略上做了很多小改进：</p>
<ul>
<li>改变训练时噪声采样方法</li>
<li>将一维位置编码改成二维位置编码</li>
<li>提升 VAE 隐空间通道数</li>
<li>对注意力 QK 做归一化以确保高分辨率下训练稳定</li>
</ul>
<p>本文会简单介绍这些改进。</p>
<h3 id="大型消融实验"><a href="#大型消融实验" class="headerlink" title="大型消融实验"></a>大型消融实验</h3><p>对于想训练大型文生图模型的开发者，SD3 论文提供了许多极有价值的大型消融实验结果。本文会简单分析论文中的两项实验结果：各训练目标在文生图任务中的表现、SD3 的参数扩增实验结果。</p>
<h3 id="SD3-Diffusers-源码解读"><a href="#SD3-Diffusers-源码解读" class="headerlink" title="SD3 Diffusers 源码解读"></a>SD3 Diffusers 源码解读</h3><p>本文会介绍如何配置 Diffusers 环境以用代码运行 SD3，并简单介绍相比于 SD，SD3 的采样代码和模型代码有哪些变动。</p>
<h2 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h2><h3 id="核心贡献"><a href="#核心贡献" class="headerlink" title="核心贡献"></a>核心贡献</h3><p>介绍 Stable Diffusion 3 (SD3) 的文章标题为 <em>Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</em>。与其说它是一篇技术报告，更不如说它是一篇论文，因为它确实是按照撰写学术论文的一般思路，将正文的叙述重点放到了方法的核心创新点上，而没有过多叙述工程细节。正如其标题所示，这篇文章的内容很简明，就是用<strong>整流 (rectified flow)</strong> 生成模型、<strong>Transformer</strong> 神经网络做了模型<strong>参数扩增</strong>实验，以实现高质量文生图大模型。</p>
<p>由于这是一篇实验主导而非思考主导的文章，论文的开头没有太多有价值的内容。从我们读者学习论文的角度，文章的核心贡献如下：</p>
<p>从方法设计上：</p>
<ul>
<li>首次在大型文生图模型上使用了整流模型。</li>
<li>用一种新颖的 Diffusion Transformer (DiT) 神经网络来更好地融合文本信息。</li>
<li>使用了各种小设计来提升模型的能力。如使用二维位置编码来实现任意分辨率的图像生成。</li>
</ul>
<p>从实验上：</p>
<ul>
<li>开展了一场大规模、系统性的实验，以验证哪种扩散模型/整流模型的学习目标最优。</li>
<li>开展了扩增模型参数的实验 (scaling study)，以证明提升参数量能提升模型的效果。</li>
</ul>
<h3 id="整流模型简介"><a href="#整流模型简介" class="headerlink" title="整流模型简介"></a>整流模型简介</h3><p>由于 SD3 最后用了整流模型来建模图像生成，所以文章是从一种称为流匹配 (Flow Matching) 的角度而非更常见的扩散模型的角度来介绍各种训练目标。鉴于 SD3 并没有对其他论文中提出的整流模型做太多更改，我们在阅读本文时可以主要关注整流的想法及其与扩散模型的关系，后续再从其他论文中学习整流的具体原理。在此，我们来大致认识一下流匹配与整流的想法。</p>
<p>所谓图像生成，其实就是让神经网络模型学习一个图像数据集所表示的分布，之后从分布里随机采样。比如我们想让模型生成人脸图像，就是要让模型学习一个人脸图像集的分布。为了直观理解，我们可以用二维点来表示一张图像的数据。比如在下图中我们希望学习红点表示的分布，即我们希望随机生成点，生成的点都落在红点处，而不是落在灰点处。</p>
<p><img src="/2024/07/14/20240703-SD3/1.jpg" alt></p>
<p>我们很难表示出一个适合采样的复杂分布。因此，我们会把学习一个分布的问题转换成学习一个简单好采样的分布到复杂分布的映射。一般这个简单分布都是标准正态分布。如下图所示，我们可以用简单的算法采样在原点附近的来自标准正态分布的蓝点，我们要想办法得到蓝点到红点的映射方法。</p>
<p><img src="/2024/07/14/20240703-SD3/2.jpg" alt></p>
<p>学习这种映射依然是很困难的。而近年来包括扩散模型在内的几类生成模型用一种巧妙的方法来学习这种映射：从纯噪声（标准正态分布里的数据）到真实数据的映射很难表示，但从真实数据到纯噪声的逆映射很容易表示。所以，我们先人工定义从图像数据集到噪声的变换路线（红线），再让模型学习逆路线（蓝线）。让噪声数据沿着逆路线走，就实现了图像生成。</p>
<p><img src="/2024/07/14/20240703-SD3/3.jpg" alt></p>
<p>我们又可以用一种巧妙的方法间接学习图像生成路线。知道了预定义的数据到噪声的路线后，我们其实就知道了数据在路线上每一位置的速度（红箭头）。那么，我们可以以每一位置的反向速度（蓝箭头）为真值，学习噪声到真实数据的速度场。这样的学习目标被称为流匹配。</p>
<p><img src="/2024/07/14/20240703-SD3/4.jpg" alt></p>
<p>对于不同的扩散模型及流匹配模型，其本质区别在于图像到噪声的路线的定义方式。在扩散模型中，图像到噪声的路线是由一个复杂的公式表示的。而整流模型将图像到噪声的路线定义为了直线。比如根据论文的介绍，整流中 $t$ 时刻数据 $z_t$ 由真实图像 $x_0$ 变换成纯噪声 $\epsilon$ 的位置为:</p>
<script type="math/tex; mode=display">
z_t = (1 - t) x_0 + t \epsilon</script><p>而较先进的扩散模型 EDM 提出的路线公式为（$b_t$ 是一个形式较为复杂的变量）：</p>
<script type="math/tex; mode=display">
z_t = x_0 + b_t \epsilon</script><p>由于整流最后学习出来的生成路线近乎是直线，这种模型在设计上就支持少步数生成。</p>
<blockquote>
<p>虽然整流模型是这样宣传的，但实际上 SD3 还是默认用了 28 步来生成图像。单看这篇文章，原整流论文里的很多设计并没有用上。对整流感兴趣的话，可以去阅读原论文 <em>Flow straight and fast: Learning to generate and transfer data with rectified flow</em></p>
<p>流匹配模型和扩散模型的另一个区别是，流匹配模型天然支持 image2image 任务。从纯噪声中生成图像只是流匹配模型的一个特例。</p>
</blockquote>
<h3 id="非均匀训练噪声采样"><a href="#非均匀训练噪声采样" class="headerlink" title="非均匀训练噪声采样"></a>非均匀训练噪声采样</h3><p>在学习这样一种生成模型时，会先随机采样一个时刻 $t \in [0, 1]$，根据公式获取此时刻对应位置在生成路线上的速度，再让神经网络学习这个速度。直观上看，刚开始和快到终点的路线很好学，而路线的中间处比较难学。因此，在采样时刻 $t$ 时，SD3 使用了一种非均匀采样分布。</p>
<p>如下图所示，SD3 主要考虑了两种公式: mode（左）和 logit-norm （右）。二者的共同点是中间多，两边少。mode 相比 logit-norm，在开始和结束时概率不会过分接近 0。</p>
<p><img src="/2024/07/14/20240703-SD3/5.jpg" alt></p>
<h3 id="网络整体架构"><a href="#网络整体架构" class="headerlink" title="网络整体架构"></a>网络整体架构</h3><p>以上内容都是和训练相关的理论基础，下面我们来看多数用户更加熟悉的文生图架构。</p>
<p>从整体架构上来看，和之前的 SD 一样，SD3 主要基于隐扩散模型（latent diffusion model, LDM）。这套方法是一个两阶段的生成方法：先用一个 LDM 生成隐空间低分辨率的图像，再用一个自编码器把图像解码回真实图像。</p>
<p>扩散模型 LDM 会使用一个神经网络模型来对噪声图像去噪。为了实现文生图，该去噪网络会以输入文本为额外约束。相比之前多数扩散模型，SD3 的主要改进是把去噪模型的结构从 U-Net 变为了 DiT。</p>
<blockquote>
<p>DiT 的论文为 <em>Scalable Diffusion Models with Transformers</em>。如果只是对 DiT 的结构感兴趣的话，可以去直接通过读 SD3 的源码来学习。读 DiT 论文时只需要着重学习 AdaLayerNormZero 模块。</p>
</blockquote>
<h3 id="提升自编码器通道数"><a href="#提升自编码器通道数" class="headerlink" title="提升自编码器通道数"></a>提升自编码器通道数</h3><p>在当时设计整套自编码器 + LDM 的生成架构时，SD 的开发者并没有仔细改进自编码器，用了一个能把图像下采样 8 倍，通道数变为 4 的隐空间图像。比如输入 $512 \times 512 \times 3$ 的图像会被自编码器编码成 $64 \times 64 \times 4$。而近期有些工作发现，这个自编码器不够好，提升隐空间的通道数能够提升自编码器的重建效果。因此，SD3 把隐空间图像的通道数从 $4$ 改为了 $16$。</p>
<h3 id="多模态-DiT-MM-DiT"><a href="#多模态-DiT-MM-DiT" class="headerlink" title="多模态 DiT (MM-DiT)"></a>多模态 DiT (MM-DiT)</h3><p>SD3 的去噪模型是一个 Diffusion Transformer (DiT)。如果去噪模型只有带噪图像这一种输入的话，DiT 则会是一个结构非常简单的模型，和标准 ViT 一样：图像过图块化层 (Patching) 并与位置编码相加，得到序列化的数据。这些数据会像标准 Transformer 一样，经过若干个子模块，再过反图块层得到模型输出。DiT 的每个子模块 DiT-Block 和标准 Transformer 块一样，由 LayerNorm, Self-Attention, 一对一线性层 (Pointwise Feedforward, FF) 等模块构成。</p>
<blockquote>
<p>图块化层会把 $2\times2$ 个像素打包成图块，反图块化层则会把图块还原回像素。</p>
</blockquote>
<p><img src="/2024/07/14/20240703-SD3/6.jpg" alt></p>
<p>然而，扩散模型中的去噪网络一定得支持带约束生成。这是因为扩散模型约束于去噪时刻 $t$。此外，作为文生图模型，SD3 还得支持文本约束。DiT 及本文的 MM-DiT 把模型设计的重点都放在了处理额外约束上。</p>
<p>我们先看一下模块是怎么处理较简单的时刻约束的。此处，如下图所示，SD3 的模块保留了 DiT 的设计，用自适应 LayerNorm (Adaptive LayerNorm, AdaLN) 来引入额外约束。具体来说，过了 LayerNorm 后，数据的均值、方差会根据时刻约束做调整。另外，过完 Attention 层或 FF 层后，数据也会乘上一个和约束相关的系数。</p>
<p><img src="/2024/07/14/20240703-SD3/7.jpg" alt></p>
<p>我们再来看文本约束的处理。文本约束以两种方式输入进模型：与时刻编码拼接、在注意力层中融合。具体数据关联细节可参见下图。如图所示，为了提高 SD3 的文本理解能力，描述文本 (“Caption”) 经由三种编码器编码，得到两组数据。一组较短的数据会经由 MLP 与文本编码加到一起；另一组数据会经过线性层，输入进 Transformer 的主模块中。</p>
<blockquote>
<p>将约束编码与时刻编码相加是一种很常见的做法。此前 U-Net 去噪网络中处理简单约束（如 ImageNet 类型约束）就是用这种方法。</p>
</blockquote>
<p><img src="/2024/07/14/20240703-SD3/8.jpg" alt></p>
<p>SD3 的 DiT 的子模块结构图如下所示。我们可以分几部分来看它。先看时刻编码 $y$ 的那些分支。和标准 DiT 子模块一样，$y$ 通过修改 LayerNorm 后数据的均值、方差及部分层后的数据大小来实现约束。再看输入的图像编码 $x$ 和文本编码 $c$。二者以相同的方式做了 DiT 里的 LayerNorm, FF 等操作。不过，相比此前多数基于 DiT 的模型，此模块用了一种特殊的融合注意力层。具体来说，在过注意力层之前，$x$ 和 $c$ 对应的 $Q, K, V$ 会分别拼接到一起，而不是像之前的模型一样，$Q$ 来自图像，$K, V$ 来自文本。过完注意力层，输出的数据会再次拆开，回到原本的独立分支里。由于 Transformer 同时处理了文本、图像的多模态信息，所以作者将模型取名为 MM-DiT (Multimodal DiT)。</p>
<p><img src="/2024/07/14/20240703-SD3/9.jpg" alt></p>
<blockquote>
<p>论文里讲:「这个结构可以等价于两个模态各有一个 Transformer，但是在注意力操作时做了拼接，使得两种表示既可以在独自的空间里工作也可以考虑到另一个表示。」然而，我不太喜欢这种尝试去凭空解读神经网络中间表示的表述。仅从数据来源来看，过了一个注意力层后，图像信息和文本信息就混在了一起。你很难说，也很难测量，之后的 $x$ 主要是图像信息，$c$ 主要是文本信息。只能说 $x, c$ 都蕴含了多模态的信息。之前 SD U-Net 里的 $x, c$ 可以认为是分别包含了图像信息和文本信息，因为之前的 $x$ 保留了二维图像结构，而 $c$ 仅由文本信息决定。</p>
</blockquote>
<h3 id="比例可变的位置编码"><a href="#比例可变的位置编码" class="headerlink" title="比例可变的位置编码"></a>比例可变的位置编码</h3><p>此前多数方法在使用类 ViT 架构时，都会把图像的图块从左上到右下编号，把二维图块拆成一维序列，再用这种一维位置编码来对待图块。</p>
<p><img src="/2024/07/14/20240703-SD3/10.jpg" alt></p>
<p>这样做有一个很大的坏处：生成的图像的分辨率是无法修改的。比如对于上图，假如采样时输入大小不是 $4 \times 4$，而是 $4 \times 5$，那么 $0$ 号图块的下面就是 $5$ 而不是 $4$ 了，模型训练时学习到的图块之间的位置关系全部乱套。</p>
<p>解决此问题的方法很简单，只需要将一维的编码改为二维编码。这样 Transformer 就不会搞混二维图块间的关系了。</p>
<p><img src="/2024/07/14/20240703-SD3/11.jpg" alt></p>
<p>SD3 的 MM-DiT 一开始是在 $256^2$ 固定分辨率上训练的。之后在高分辨率图像上训练时，开发者用了一些巧妙的位置编码设置技巧，让不同比例的高分辨率图像也能共享之前学到的这套位置编码。详细公式请参见原论文。</p>
<h3 id="训练数据预处理"><a href="#训练数据预处理" class="headerlink" title="训练数据预处理"></a>训练数据预处理</h3><p>看完了模块设计，我们再来看一下 SD3 在训练中的一些额外设计。在大规模训练前，开发者用三个方式过滤了数据：</p>
<ol>
<li>用了一个 NSFW 过滤器过滤图片，似乎主要是为了过滤色情内容。</li>
<li>用美学打分器过滤了美学分数太低的图片。</li>
<li>移除了看上去语义差不多的图片。</li>
</ol>
<p>虽然开发者们自信满满地向大家介绍了这些数据过滤技术，但根据社区用户们的反馈，可能正是因为色情过滤器过分严格，导致 SD3 经常会生成奇怪的人体。</p>
<p>由于在训练 LDM 时，自编码器和文本编码器是不变的，因此可以提前处理好所有训练数据的图像编码和文本编码。当然，这是一项非常基础的工程技巧，不应该写在正文里的。</p>
<h3 id="用-QK-归一化提升训练稳定度"><a href="#用-QK-归一化提升训练稳定度" class="headerlink" title="用 QK 归一化提升训练稳定度"></a>用 QK 归一化提升训练稳定度</h3><p>按照之前高分辨率文生图模型的训练方法，SD3 会先在 $256^2$ 的图片上训练，再在高分辨率图片上微调。然而，开发者发现，开始微调后，混合精度训练常常会训崩。根据之前工作的经验，这是由于注意力输入的熵会不受控制地增长。解决方法也很简单，只要在做注意力计算之前对 Q, K 做一次归一化就行，具体做计算的位置可以参考上文模块图中的 “RMSNorm”。不过，开发者也承认，这个技巧并不是一个长久之策，得具体问题具体分析。看来这种 DiT 模型在大规模训练时还是会碰到许多训练不稳定的问题，且这些问题没有一个通用解。 </p>
<h3 id="哪种扩散模型训练目标最适合文生图任务？"><a href="#哪种扩散模型训练目标最适合文生图任务？" class="headerlink" title="哪种扩散模型训练目标最适合文生图任务？"></a>哪种扩散模型训练目标最适合文生图任务？</h3><p>最后我们来看论文的实验结果部分。首先，为了寻找最好的扩散模型/流匹配模型，开发者开展了一场声势浩大的实验。实验涉及 61 种训练公式，其中的可变项有：</p>
<ul>
<li>对于普通扩散模型，考虑 $\epsilon$- 或 $\mathbf{v}$-prediction，考虑线性或 cosine 噪声调度。</li>
<li>对于整流，考虑不同的噪声调度。</li>
<li>对于 EDM，考虑不同的噪声调度，且尽可能与整流的调度机制相近以保证可比较。</li>
</ul>
<p>在训练时，除了训练目标公式可变外，优化算法、模型架构、数据集、采样器都不可变。所有模型在 ImageNet 和 CC12M 数据集上训练，在 COCO-2014 验证集上评估 FID 和 CLIP Score。根据评估结果，可以选出每个模型的最优停止训练的步数。基于每种目标下的最优模型，开发者对模型进行最后的排名。由于在最终评估时，仍有采样步数、是否使用 EMA 模型等可变采样配置，开发者在所有 24 种采样配置下评估了所有模型，并用一种算法来综合所有采样配置的结果，得到一个所有模型的最终排名。最终的排名结果如下面的表 1 所示。训练集上的一些指标如表 2 所示。</p>
<p><img src="/2024/07/14/20240703-SD3/12.jpg" alt></p>
<p>根据实验结果，我们可以得到一些直观的结论：整流领先于扩散模型。惊人的是，较新推出的 EDM 竟然没有战胜早期的 LDM (“eps/linear”)。</p>
<p>当然，我个人认为，应该谨慎看待这份实验结果。一般来说，大家做图像生成会用一个统一的指标，比如 ImageNet 上的 FID。这篇论文相当于是新提出了一种昂贵的评价方法。这种评价方法是否合理，是否能得到公认还犹未可知。另外，想说明一个生成模型的拟合能力不错，用 ImageNet 上的 FID 指标就足够有说服力了，大家不会对一个简单的生成模型有太多要求。然而，对于大型文生图模型，大家更关心的是模型的生成效果，而 FID 和 CLIP Score 并不能直接反映文生图模型的质量。因此，光凭这份实验结果，我们并不能说整流一定比之前的扩散模型要好。</p>
<p>会关注这份实验结果的应该都是公司里的文生图开发者。我建议体量小的公司直接参考这份实验结果，无脑使用整流来代替之前的训练目标。而如果有能力做同等级的实验的话，则不应该错过改良后的扩散模型，如最新的 EDM2，说不定以后还会有更好的文生图训练目标。</p>
<h3 id="参数扩增实验结果"><a href="#参数扩增实验结果" class="headerlink" title="参数扩增实验结果"></a>参数扩增实验结果</h3><p>现在多数生成模型都会做参数扩增实验，即验证模型表现随参数量增长而增长，确保模型在资源足够的情况下可以被训练成「大模型」。SD3 也做了类似的实验。开发者用参数 $d$ 来控制 MM-DiT 的大小，Transformer 块的个数为 $d$，且所有特征的通道数与 $d$ 成正比。开发者在 $256^2$ 的数据上训练了所有模型 500k 步，每 50k 步在 CoCo 数据集上统计验证误差。最终所有评估指标如下图所示。可以说，所有指标都表明，模型的表现的确随参数量增长而增长。更多结果请参见论文。</p>
<p><img src="/2024/07/14/20240703-SD3/13.jpg" alt></p>
<h2 id="Diffusers-源码阅读"><a href="#Diffusers-源码阅读" class="headerlink" title="Diffusers 源码阅读"></a>Diffusers 源码阅读</h2><h3 id="测试脚本"><a href="#测试脚本" class="headerlink" title="测试脚本"></a>测试脚本</h3><p>我们来阅读一下 SD3 在最流行的扩散模型框架 Diffusers 中的源码。在读源码前，我们先来跑通官方的示例脚本。</p>
<p>由于使用协议的限制，SD3 的环境搭起来稍微有点麻烦。首先，我们要确保 Diffuers 和 Transformers 都用的是最新版本。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade diffusers transformers</span><br></pre></td></tr></table></figure>
<p>之后，我们要注册 HuggingFace 账号，再在 SD3 的模型网站 <code>https://huggingface.co/stabilityai/stable-diffusion-3-medium</code> 里确认同意某些使用协议。之后，我们要设置 Access Token。具体操作如下所示，先点右上角的 “settings”，再点左边的 “Access Tokens”，创建一个新 token。将这个 token 复制保存在本地后，点击 token 右上角选项里的 “Edit Permission”，在权限里开启 “… public gated repos …”。</p>
<p><img src="/2024/07/14/20240703-SD3/14.jpg" alt></p>
<p>最后，我们用命令行登录 HuggingFace 并使用 SD3。先用下面的命令安装 HuggingFace 命令行版。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U &quot;huggingface_hub[cli]&quot;</span><br></pre></td></tr></table></figure>
<p>再输入 <code>huggingface-cli login</code>，命令行会提示输入 token 信息。把刚刚保存好的 token 粘贴进去，即可完成登录。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli login</span><br><span class="line"></span><br><span class="line">Enter your token (input will not be visible): 在这里粘贴 token</span><br></pre></td></tr></table></figure>
<p>做完准备后，我们就可以执行下面的测试脚本了。注意，该脚本会自动下载模型，我们需要保证当前环境能够访问 HuggingFace。执行完毕后，生成的 $1024 \times 1024$ 大小的图片会保存在 <code>tmp.png</code> 里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> StableDiffusion3Pipeline</span><br><span class="line"></span><br><span class="line">pipe = StableDiffusion3Pipeline.from_pretrained(</span><br><span class="line">    <span class="string">&quot;stabilityai/stable-diffusion-3-medium-diffusers&quot;</span>, torch_dtype=torch.float16)</span><br><span class="line">pipe = pipe.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line">image = pipe(</span><br><span class="line">    <span class="string">&quot;A cat holding a sign that says hello world&quot;</span>,</span><br><span class="line">    negative_prompt=<span class="string">&quot;&quot;</span>,</span><br><span class="line">    num_inference_steps=<span class="number">28</span>,</span><br><span class="line">    guidance_scale=<span class="number">7.0</span>,</span><br><span class="line">).images[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">image.save(<span class="string">&#x27;tmp.png&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>我得到的图片如下所示。看起来 SD3 理解文本的能力还是挺强的。</p>
<p><img src="/2024/07/14/20240703-SD3/15.jpg" alt></p>
<h3 id="模型组件"><a href="#模型组件" class="headerlink" title="模型组件"></a>模型组件</h3><p>接下来我们来快速浏览一下 SD3 流水线 <code>StableDiffusion3Pipeline</code> 的源码。在 IDE 里使用源码跳转功能可以在 <code>diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py</code> 里找到该类的源码。</p>
<p>通过流水线的 <code>__init__</code> 方法，我们能知道 SD3 的所有组件。组件包括自编码器 <code>vae</code>, MM-DiT <code>Transformer</code>, 流匹配噪声调度器 <code>scheduler</code>，以及三个文本编码器。每个编码器由一个 tokenizer 和一个 text encoder 组成.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    transformer: SD3Transformer2DModel,</span></span></span><br><span class="line"><span class="params"><span class="function">    scheduler: FlowMatchEulerDiscreteScheduler,</span></span></span><br><span class="line"><span class="params"><span class="function">    vae: AutoencoderKL,</span></span></span><br><span class="line"><span class="params"><span class="function">    text_encoder: CLIPTextModelWithProjection,</span></span></span><br><span class="line"><span class="params"><span class="function">    tokenizer: CLIPTokenizer,</span></span></span><br><span class="line"><span class="params"><span class="function">    text_encoder_2: CLIPTextModelWithProjection,</span></span></span><br><span class="line"><span class="params"><span class="function">    tokenizer_2: CLIPTokenizer,</span></span></span><br><span class="line"><span class="params"><span class="function">    text_encoder_3: T5EncoderModel,</span></span></span><br><span class="line"><span class="params"><span class="function">    tokenizer_3: T5TokenizerFast,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br></pre></td></tr></table></figure>
<p><code>vae</code> 的用法和之前 SD 的一模一样，编码时用 <code>vae.encode</code> 并乘 <code>vae.config.scaling_factor</code>，解码时除以 <code>vae.config.scaling_factor</code> 并用 <code>vae.decode</code>。</p>
<p>文本编码器的用法可以参见 <code>encode_prompt</code> 方法。文本会分别过各个编码器的 tokenizer 和 text encoder，得到三种文本编码，并按照论文中的描述拼接成两种约束信息。这部分代码十分繁杂，多数代码都是在处理数据形状，没有太多有价值的内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_prompt</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        prompt,</span></span></span><br><span class="line"><span class="params"><span class="function">        prompt_2,</span></span></span><br><span class="line"><span class="params"><span class="function">        prompt_3,</span></span></span><br><span class="line"><span class="params"><span class="function">        device,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_images_per_prompt,</span></span></span><br><span class="line"><span class="params"><span class="function">        do_classifier_free_guidance,</span></span></span><br><span class="line"><span class="params"><span class="function">        negative_prompt,</span></span></span><br><span class="line"><span class="params"><span class="function">        negative_prompt_2,</span></span></span><br><span class="line"><span class="params"><span class="function">        negative_prompt_3,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> prompt_embeds, negative_prompt_embeds,</span><br><span class="line">     pooled_prompt_embeds, negative_pooled_prompt_embeds</span><br></pre></td></tr></table></figure>
<h3 id="采样流水线"><a href="#采样流水线" class="headerlink" title="采样流水线"></a>采样流水线</h3><p>我们再来通过阅读流水线的 <code>__call__</code> 方法了解 SD3 采样的过程。由于 SD3 并没有修改 LDM 的这套生成框架，其采样流水线和 SD 几乎完全一致。SD3 和 SD 的 <code>__call__</code> 方法的主要区别是，生成文本编码时会生成两种编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(</span><br><span class="line">    prompt_embeds,</span><br><span class="line">    negative_prompt_embeds,</span><br><span class="line">    pooled_prompt_embeds,</span><br><span class="line">    negative_pooled_prompt_embeds,</span><br><span class="line">) = self.encode_prompt(...)</span><br></pre></td></tr></table></figure>
<p>在调用去噪网络时，那个较小的文本编码 <code>pooled_prompt_embeds</code> 会作为一个额外参数输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">noise_pred = self.transformer(</span><br><span class="line">    hidden_states=latent_model_input,</span><br><span class="line">    timestep=timestep,</span><br><span class="line">    encoder_hidden_states=prompt_embeds,</span><br><span class="line">    pooled_projections=pooled_prompt_embeds,</span><br><span class="line">    joint_attention_kwargs=self.joint_attention_kwargs,</span><br><span class="line">    return_dict=<span class="literal">False</span>,</span><br><span class="line">)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h3 id="MM-DiT-去噪模型"><a href="#MM-DiT-去噪模型" class="headerlink" title="MM-DiT 去噪模型"></a>MM-DiT 去噪模型</h3><p>相比之下，SD3 的去噪网络 MM-DiT 的改动较大。我们来看一下对应的 <code>SD3Transformer2DModel</code> 类，它位于文件 <code>diffusers\models\transformers\transformer_sd3.py</code>。</p>
<p>类的构造函数里有几个值得关注的模块：二维位置编码类 <code>PatchEmbed</code>、组合时刻编码和文本编码模块 <code>CombinedTimestepTextProjEmbeddings</code>、主模块类 <code>JointTransformerBlock</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    self.pos_embed = PatchEmbed(...)</span><br><span class="line">    self.time_text_embed = CombinedTimestepTextProjEmbeddings(...)</span><br><span class="line">    ...</span><br><span class="line">    self.transformer_blocks = nn.ModuleList(</span><br><span class="line">          [</span><br><span class="line">              JointTransformerBlock(..)</span><br><span class="line">              <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.config.num_layers)</span><br><span class="line">          ]</span><br><span class="line">      )</span><br></pre></td></tr></table></figure>
<p>类的前向传播函数 <code>forward</code> 里都是比较常规的操作。数据会依次经过前处理、若干个 Transformer 块、后处理。所有实现细节都封装在各个模块类里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">...</span>):</span></span><br><span class="line">    hidden_states = self.pos_embed(hidden_states)</span><br><span class="line">    temb = self.time_text_embed(timestep, pooled_projections)</span><br><span class="line">    encoder_hidden_states = self.context_embedder(encoder_hidden_states)</span><br><span class="line">    <span class="keyword">for</span> index_block, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.transformer_blocks):</span><br><span class="line">       encoder_hidden_states, hidden_states = block(...)</span><br><span class="line">    </span><br><span class="line">    encoder_hidden_states, hidden_states = block(</span><br><span class="line">    hidden_states=hidden_states, encoder_hidden_states=encoder_hidden_states, temb=temb</span><br><span class="line">)</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>接下来我们来看这几个较为重要的子模块。<code>PatchEmbed</code> 类的实现写在 <code>diffusers/models/embeddings.py</code> 里。这个类的实现写得非常清晰。<code>PatchEmbed</code> 类本身用于维护位置编码宽高、特征长度这些信息，计算位置编码的关键代码在 <code>get_2d_sincos_pos_embed</code> 中。<code>get_2d_sincos_pos_embed</code> 会生成 <code>(0, 0), (1, 0), ...</code> 这样的二维坐标网格，再调用 <code>get_2d_sincos_pos_embed_from_grid</code> 生成二维位置编码。<code>get_2d_sincos_pos_embed_from_grid</code> 会调用两次一维位置编码函数 <code>get_1d_sincos_pos_embed_from_grid</code>，也就是 Transformer 里那种标准位置编码生成函数，来分别生成两个方向的编码，最后拼接成二维位置编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PatchEmbed</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, latent</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        pos_embed = get_2d_sincos_pos_embed(...)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_2d_sincos_pos_embed</span>(<span class="params">...</span>):</span></span><br><span class="line">    grid_h = np.arange(...)</span><br><span class="line">    grid_w = np.arange(...)</span><br><span class="line">    grid = np.meshgrid(grid_w, grid_h)</span><br><span class="line">    ...</span><br><span class="line">    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_2d_sincos_pos_embed_from_grid</span>(<span class="params">...</span>):</span></span><br><span class="line">    <span class="comment"># use half of dimensions to encode grid_h</span></span><br><span class="line">    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // <span class="number">2</span>, grid[<span class="number">0</span>])  <span class="comment"># (H*W, D/2)</span></span><br><span class="line">    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // <span class="number">2</span>, grid[<span class="number">1</span>])  <span class="comment"># (H*W, D/2)</span></span><br><span class="line"></span><br><span class="line">    emb = np.concatenate([emb_h, emb_w], axis=<span class="number">1</span>)  <span class="comment"># (H*W, D)</span></span><br><span class="line">    <span class="keyword">return</span> emb</span><br></pre></td></tr></table></figure>
<p>组合时刻编码和文本编码模块 <code>CombinedTimestepTextProjEmbeddings</code> 的代码非常短。它实际上就是用通常的 <code>Timesteps</code> 类获取时刻编码，用一个 <code>text_embedder</code> 模块再次处理文本编码，最后把两个编码加起来。<br><code>text_embedder</code> 是一个线性层、激活函数、线性层构成的简单模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CombinedTimestepTextProjEmbeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embedding_dim, pooled_projection_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.time_proj = Timesteps(num_channels=<span class="number">256</span>, flip_sin_to_cos=<span class="literal">True</span>, downscale_freq_shift=<span class="number">0</span>)</span><br><span class="line">        self.timestep_embedder = TimestepEmbedding(in_channels=<span class="number">256</span>, time_embed_dim=embedding_dim)</span><br><span class="line">        self.text_embedder = PixArtAlphaTextProjection(pooled_projection_dim, embedding_dim, act_fn=<span class="string">&quot;silu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, timestep, pooled_projection</span>):</span></span><br><span class="line">        timesteps_proj = self.time_proj(timestep)</span><br><span class="line">        timesteps_emb = self.timestep_embedder(timesteps_proj.to(dtype=pooled_projection.dtype))  <span class="comment"># (N, D)</span></span><br><span class="line"></span><br><span class="line">        pooled_projections = self.text_embedder(pooled_projection)</span><br><span class="line"></span><br><span class="line">        conditioning = timesteps_emb + pooled_projections</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> conditioning</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PixArtAlphaTextProjection</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, caption</span>):</span></span><br><span class="line">        hidden_states = self.linear_1(caption)</span><br><span class="line">        hidden_states = self.act_1(hidden_states)</span><br><span class="line">        hidden_states = self.linear_2(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<p>MM-DiT 的主要模块 <code>JointTransformerBlock</code> 在 <code>diffusers/models/attention.py</code> 文件里。这个类的代码写得比较乱。它主要负责处理 LayerNorm 及数据的尺度变换操作，具体的注意力计算由注意力处理器 <code>JointAttnProcessor2_0</code> 负责。两处 LayerNorm 的实现方式竟然是不一样的。</p>
<p><img src="/2024/07/14/20240703-SD3/9.jpg" alt></p>
<p>我们先简单看一下构造函数里初始化了哪些模块。代码中，<code>norm1, ff, norm2</code> 等模块都是普通 Transformer 块中的模块。而加了 <code>_context</code> 的模块则表示处理文本分支 $c$ 的模块，如 <code>norm1_context, ff_context</code>。<code>context_pre_only</code> 表示做完了注意力计算后，还要不要给文本分支加上 LayerNorm 和 FeedForward。如前文所述，具体的注意力计算由 <code>JointAttnProcessor2_0</code> 负责。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JointTransformerBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, num_attention_heads, attention_head_dim, context_pre_only=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.context_pre_only = context_pre_only</span><br><span class="line">        context_norm_type = <span class="string">&quot;ada_norm_continous&quot;</span> <span class="keyword">if</span> context_pre_only <span class="keyword">else</span> <span class="string">&quot;ada_norm_zero&quot;</span></span><br><span class="line"></span><br><span class="line">        self.norm1 = AdaLayerNormZero(dim)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> context_norm_type == <span class="string">&quot;ada_norm_continous&quot;</span>:</span><br><span class="line">            self.norm1_context = AdaLayerNormContinuous(</span><br><span class="line">                dim, dim, elementwise_affine=<span class="literal">False</span>, eps=<span class="number">1e-6</span>, bias=<span class="literal">True</span>, norm_type=<span class="string">&quot;layer_norm&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">elif</span> context_norm_type == <span class="string">&quot;ada_norm_zero&quot;</span>:</span><br><span class="line">            self.norm1_context = AdaLayerNormZero(dim)</span><br><span class="line">        </span><br><span class="line">        processor = JointAttnProcessor2_0()</span><br><span class="line">        self.attn = Attention(</span><br><span class="line">            query_dim=dim,</span><br><span class="line">            cross_attention_dim=<span class="literal">None</span>,</span><br><span class="line">            added_kv_proj_dim=dim,</span><br><span class="line">            dim_head=attention_head_dim,</span><br><span class="line">            heads=num_attention_heads,</span><br><span class="line">            out_dim=dim,</span><br><span class="line">            context_pre_only=context_pre_only,</span><br><span class="line">            bias=<span class="literal">True</span>,</span><br><span class="line">            processor=processor,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.norm2 = nn.LayerNorm(dim, elementwise_affine=<span class="literal">False</span>, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.ff = FeedForward(dim=dim, dim_out=dim, activation_fn=<span class="string">&quot;gelu-approximate&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> context_pre_only:</span><br><span class="line">            self.norm2_context = nn.LayerNorm(dim, elementwise_affine=<span class="literal">False</span>, eps=<span class="number">1e-6</span>)</span><br><span class="line">            self.ff_context = FeedForward(dim=dim, dim_out=dim, activation_fn=<span class="string">&quot;gelu-approximate&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.norm2_context = <span class="literal">None</span></span><br><span class="line">            self.ff_context = <span class="literal">None</span>     </span><br></pre></td></tr></table></figure>
<p>我们再来看 <code>forward</code> 方法。在前向传播时，图像分支和文本分支会分别过 <code>norm1</code>，再一起过注意力操作，再分别过 <code>norm2</code> 和 <code>ff</code>。大概的代码如下所示，我把较复杂的 context 分支的代码略过了。</p>
<p>这份代码写得很不漂亮，按理说模块里两个 LayerNorm + 尺度变换 (即 Adaptive LayerNorm) 的操作是一样的，应该用同样的代码来处理。但是这个模块里 <code>norm1</code> 是 <code>AdaLayerNormZero</code> 类，<code>norm2</code> 是 <code>LayerNorm</code> 类。<code>norm1</code> 会自动做完 AdaLayerNorm 的运算，并把相关变量返回。而在 <code>norm2</code> 处，代码会先执行普通的 LayerNorm，再根据之前的变量手动调整数据的尺度。我们心里知道这份代码是在实现论文里那张结构图就好，没必要去仔细阅读。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self, hidden_states: torch.FloatTensor, encoder_hidden_states: torch.FloatTensor, temb: torch.FloatTensor</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">    norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.norm1(hidden_states, emb=temb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.context_pre_only:</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Attention.</span></span><br><span class="line">    attn_output, context_attn_output = self.attn(</span><br><span class="line">        hidden_states=norm_hidden_states, encoder_hidden_states=norm_encoder_hidden_states</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Process attention outputs for the `hidden_states`.</span></span><br><span class="line">    attn_output = gate_msa.unsqueeze(<span class="number">1</span>) * attn_output</span><br><span class="line">    hidden_states = hidden_states + attn_output</span><br><span class="line"></span><br><span class="line">    norm_hidden_states = self.norm2(hidden_states)</span><br><span class="line">    norm_hidden_states = norm_hidden_states * (<span class="number">1</span> + scale_mlp[:, <span class="literal">None</span>]) + shift_mlp[:, <span class="literal">None</span>]</span><br><span class="line">    ff_output = self.ff(norm_hidden_states)</span><br><span class="line">    ff_output = gate_mlp.unsqueeze(<span class="number">1</span>) * ff_output</span><br><span class="line"></span><br><span class="line">    hidden_states = hidden_states + ff_output</span><br><span class="line">    <span class="keyword">if</span> self.context_pre_only:</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> encoder_hidden_states, hidden_states</span><br></pre></td></tr></table></figure>
<p>融合注意力的实现方法很简单。和普通的注意力计算相比，这种注意力就是把另一条数据分支 <code>encoder_hidden_states</code> 也做了 QKV 的线性变换，并在做注意力运算前与原来的 QKV 拼接起来。做完注意力运算后，两个数据又会拆分回去。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JointAttnProcessor2_0</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Attention processor used typically in processing the SD3-like self-attention projections.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        attn: Attention,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states: torch.FloatTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states: torch.FloatTensor = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        *args,</span></span></span><br><span class="line"><span class="params"><span class="function">        **kwargs,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; torch.FloatTensor:</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">        <span class="comment"># `sample` projections.</span></span><br><span class="line">        query = attn.to_q(hidden_states)</span><br><span class="line">        key = attn.to_k(hidden_states)</span><br><span class="line">        value = attn.to_v(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># `context` projections.</span></span><br><span class="line">        encoder_hidden_states_query_proj = attn.add_q_proj(encoder_hidden_states)</span><br><span class="line">        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)</span><br><span class="line">        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># attention</span></span><br><span class="line">        query = torch.cat([query, encoder_hidden_states_query_proj], dim=<span class="number">1</span>)</span><br><span class="line">        key = torch.cat([key, encoder_hidden_states_key_proj], dim=<span class="number">1</span>)</span><br><span class="line">        value = torch.cat([value, encoder_hidden_states_value_proj], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split the attention outputs.</span></span><br><span class="line">        hidden_states, encoder_hidden_states = (</span><br><span class="line">            hidden_states[:, : residual.shape[<span class="number">1</span>]],</span><br><span class="line">            hidden_states[:, residual.shape[<span class="number">1</span>] :],</span><br><span class="line">        )</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我们学习了 SD3 论文及源码中的主要内容。相比于 SD，SD3 做了两项较大的改进：用整流代替原来的 DDPM 中的训练目标；将去噪模型从 U-Net 变成了能更好地处理多模态信息的 MM-DiT。SD3 还在模型结构和训练目标上做了许多小改进，如调整训练噪声采样分布、使用二维位置编码。SD3 论文展示了多项大型消融实验的结果，证明当前的 SD3 是以最优配置训练得到的。SD3 可以在 Diffusers 中使用。当然，由于 SD3 的使用协议较为严格，我们需要做一些配置，才能在代码中使用 SD3。SD3 的采样流水线基本没变，原来 SD 的多数编辑方法能够无缝迁移过来。而 SD3 的去噪模型变动较大，和 U-Net 相关的编辑方法则无法直接用过来。在学习源码时，主要值得学习的是新 MM-DiT 模型中每个 Transformer 层的实现细节。</p>
<p>尽管 SD3 并没有提出新的流匹配方法，但其实验结果表明流匹配模型可能更适合文生图任务。作为研究者，受此启发，我们或许需要关注一下整流等流匹配模型，知道它们的思想，分析它们与原扩散模型训练目标的异同，以拓宽自己的视野。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/06/24/20240622-CVPR2024/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/06/24/20240622-CVPR2024/" class="post-title-link" itemprop="url">顽抗生活中的厄运</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-06-24 19:18:57" itemprop="dateCreated datePublished" datetime="2024-06-24T19:18:57+08:00">2024-06-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/" itemprop="url" rel="index"><span itemprop="name">杂谈</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/%E8%AE%B0%E5%8F%99%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">记叙文</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>今年第一次参加 CVPR 的前后发生了很多糟心事。我被气得受不了了。</p>
<hr>
<p>美国签证不太好申，我周围所有因参加学术会议而申请签证的人都被审查了一个月。我决定参会与申请签证的时间较晚，过签的时间非常仅限。在会议开始前的倒数第二周，我收到了签证通过的通知，并得知我需要在一周后，也就是会议开始前的最后几个工作日领取带了美国签证的护照。能够恰好拿到签证，算得上是运气不错。可下一周发生的种种事情，却谈不上了幸运了。</p>
<p>会议开始前一周的周三早上，我赶早骑行前往学校参加会议前的最后一次周会。骑至半途，忽然天降暴雨。在新加坡，这种现象我已经见怪不怪了。我先穿上日常携带的雨衣，再将书包背到雨衣外，以免背上出太多汗。我又用手机向导师报告我可能会推迟参会，随后将手机和浸满了雨水的眼镜装进口袋。</p>
<p>这场雨就像老天给我开的一个玩笑。我一到目的地，雨就小了下来。上楼，掀开笔记本电脑，发现电脑被穿透了书包的一点雨水浸湿，开不了机。伸手一摸口袋，眼镜竟然也不见了。</p>
<p>这种雨中骑车的事我已经做过好多回了，但我从来没碰到过这么多意外。我有多次电脑进水的经历，可这是我第一次知道穿透书包的那一点水也能弄坏电脑。另外，本来我这条裤子的口袋是不会掉东西的，可这次恰好我要用手机发消息，而眼镜和手机放在了一起，顺势掉了出来。</p>
<p>无论多么令人懊恼，生活都得继续。我用上了我所有的规划能力来处理问题。我先和导师商议将周会延期。随后，我请同学陪我一起找眼镜。还好，眼镜在楼下就找到了，但是被我自己或者其他人踩了一脚，一边的镜框断了。我艰难地用半副眼镜处理完一天的杂事，于傍晚赶往市中心将电脑送修。晚上回家，我用强力胶将断裂的镜框部件粘在一起，眼镜勉强可以用了。第二天周四，我领完美国签证，同时得知电脑严重进水，一天修不好。我回家立刻拿出老电脑并配好工作环境。周五，我准备好一切出国开会所需资料，总算安顿好了一切。</p>
<p>周六，出发前一天，我去换美元。付款时要输入储蓄卡密码，我多次尝试也没能把密码输对，卡被冻结。这时我才想起来为什么我会不记得卡的密码：这张卡的密码是系统随机设置的，平时用卡根本不需要密码，所以我既忘了密码，也忘了密码不是我自己设的。这下去美国用不了这张卡了。还好，我还有国内的信用卡，完成了在新加坡后续的付款。</p>
<p>启程的飞机将于周日早上九点起飞。我起了个大早，提前前往机场。兴冲冲地首次坐上长途国际航班后，我立刻收到了飞机因故障无法起飞的通知，灰溜溜地下了飞机。航空公司给我安排的新航班是 36 小时之后，周一晚上九点。为表赔偿，航空公司给我们预订了新加坡五星级酒店的住宿，并附带了早中晚餐各 20 新币的代金券。走进五星级酒店最普通的房间，我原以为能享受一番，却又被正餐 30 新币起步（还不含税）的菜单气晕了过去。上午我在机场吃过了，中午我就随便点了个 20 新币的汤应付了过去。晚上我兼顾价格与饱腹度，点了一个 30 多新币的披萨。晚餐送至房间，我将代金券和 20 新币交给服务员后，服务员将代金券收走，现金还回，说道：“可以拿明天的早餐钱来抵这顿晚饭。”唉，我又亏了将近 10 新币，早知道点一个更好的披萨了。</p>
<p>周一，白天没有安排，我决定去解决新加坡储蓄卡的事。手机应用有挂失换卡的功能，却楞是没有申请解冻的功能。我只好顶着烈日，跨越条条街道，又穿梭于人山人海的购物中心，找到银行的支行。这个支行只有通过视频交流的虚拟柜台。为了确认身份，柜员问了我三个有关账户的验证问题。有一个问题的华文名词我听不懂，没答上来。因此，卡解冻失败，柜员建议我前往线下柜台。我真是纳闷，我有银行账户的一切访问权限，有身份证、护照，为什么要用这种方法来验证我的身份。但我今天时间多，不跟你们计较了。我又匆忙坐上满载的地铁，赶往下一处有线下柜台的支行。地图上说银行 4 点关门，我是三点半到的，可银行已经关门了。我突然感到一股违和感：今天是周一，为什么地铁那么多人？为什么购物中心那么多人？为什么银行提早下班？我一查，果然如我所料，原来今天是新加坡法定假日。今天，到美国开会的同学在当地开开心心地旅游，留在本地的人则能享受假期。就我一个人，飞机没坐上，假没休息到，事情没办成，从一对对情侣旁边擦身而过，在我本不该在的繁华市区里奔波。没办法，休息一下，晚上早点去坐飞机吧。</p>
<p>晚上，刚进机场，我的镜框因没有粘牢又断开了。还好我早有准备，拿出透明胶勉强固定了镜框。</p>
<p>过安检，我被选为 SSSS 级“幸运”用户，被安检警察拎到队伍前面，进行细致的搜身检查。也好，我也不会带什么危险物品，顺便插了个队。</p>
<blockquote>
<p>SSSS 是美国TSA （美国运输安全管理局）随机选取需要接受二次安检的乘客。安检人员看到SSSS的登机牌后会对乘客格外注意，在经过普通的X光扫描检查以后，行李会被拆开进行人工检查，人工彻底搜身，电子产品会接受爆炸物探测。</p>
</blockquote>
<p>上了飞机，我总算坐上了有着小屏幕的美联航高级国际航班。航班要航行 15 个小时，我计划在这段时间里写一篇文章。一看时间还早，我就开了两把小游戏。玩到一半，电脑啪地屏幕一黑，没电了。飞机上的插座怎么充不进电啊？过了好一会儿，机长发来广播：“很抱歉，有乘客反映插座失灵，我们将重启设备，尽快修好。（翻译自英文）”我满怀期待地等着电脑的开机，这一等就等到了飞机降落。临走前，贴心的乘务长向大家诚恳地致歉：“今天有一些旅客一直没有接上插座，我们真是深表歉意呢~（翻译自英文）”</p>
<p>第一趟飞机在旧金山降落，我还需要乘坐中转西雅图的飞机。由于下一站是国内航班，我需要在这里完成入境。入境的队伍很长，一眼望去，四五十分钟都不见得能排完队。帮我安排航班的工作人员想必之前是一位极限运动的教练，给我安排了一小时后结束登机、一个半小时后起飞的第二趟航班，让我在这里锻炼极限的时间管理。排队五十分钟后，我极速应付完安检人员的问题，总算顺利入境。眼下飞机只有十分钟就要停止登机了，在这种极限条件下，我的身体潜能与语言表达潜能被猛然激活。我背着沉甸甸的书包——我唯一的行李，仔细地看遍每一个路标，在机场里沿着最短路线狂奔，流畅地办理登机、安检。我这一个多小时一直没来得及上厕所。跑至空荡荡的登机口前，我遗憾地望了一眼对面的厕所后，急忙向柜台后的空乘人员出示机票。“啊！你很幸运啊！快点上飞机。”在我的极限运营下，我成功在起飞前登机，就是肚子下面不太舒服。到了原定的起飞时间，我正双目紧闭静待起飞，忽然听见了机长包含温情的广播：“我们得知有部分旅客还在转机，飞机将在约十五分钟后起飞。”不早说啊！我这么赶是为了什么啊！快把厕所门打开！</p>
<p>飞机起飞，我如愿以偿地解了个手，满足地回座位睡了一觉。可能是剧烈运动导致新陈代谢加快，醒来后，我又是一阵内急。可又好巧不巧，飞机刚开始降落，我还要等半个多小时才能下飞机上厕所。在这段时间里，我仿佛领悟了长生不老之道，一分钟可以当十分钟来过。我思考了我为什么这么难受，为什么上这趟飞机，为什么会存在这个世界上。恍惚的精神慢慢回归，我规划出了飞机降落后最快的下机方式。总算到站了，舱门一打开，我就背起书包，右手紧扣腹部，面露难色，名正言顺地在下机的队伍里插队。一路小跑到厕所前，这下没人看，不用演戏了。可我的手却怎么都不肯从肚子上松开——哦，原来我真的憋得肚子不舒服了。好在厕所只有一步之遥了。我特意找了个高级包厢，坐下来不紧不慢地上起了厕所。这是我第一次憋得这么难受，也是第一次觉得上厕所是人生中最幸福的事。</p>
<p>当地时间凌晨两点，我总算来到了西雅图机场出口。没有飞机要赶，没有厕所要上，充足休息了近 20 个小时的我，即将迎来光明的前程。我没有办理美国的流量，因为我知道在需要用网络的地方都有免费无线网络。按照网上的攻略，我前往停车场的某处，准备用 Uber 打车。一到停车场，凌冽的寒气扑面而来。这冷空调开得有点大啊。不对！这是室外的正常气温。我来美国之前是看了天气预报的，每天的温度都是 10 到 20 多度。我过了太久的夏天，对 10 度的天气没有概念，也没有料到我大晚上还会待在室外。但没事啊，打上车就好了。我在机场就配置好了 Uber，只差呼叫司机这一步了。欸，怎么付款前还要验证手机号？这里信号很差，接收国内手机的短信要三分钟左右。当然，这个延迟是我在多次发送短信后才意识到的。由于我多次输入了上一次短信的验证码，Uber 最后忍无可忍，把我的账号禁用了。寒风中，我身着短袖短裤，在空荡的候车处死盯没有回应的手机，打着牙颤，默默无语。冷静了一会儿，我想我有信用卡有钱，为什么非得用软件打车不可。于是，我走到了普通打车处，稍等片刻，上车，最终总算抵达了酒店。</p>
<p>第一次出国开会，第一次前往美国旅行。对于多数人来说快乐的事情，到我这为什么就成了渡劫呢？这些事或许算不上什么人生大事，但攒在一起总是会令人烦恼。诚然，有些事是我考虑不周，但大多数事是我完全无法控制的。不过这些都没关系。在麻将等概率游戏中搏杀已久的我知道，过去是无法改变的，结果通常是无法控制的，我们能做的只有在此刻寻找问题的最优解。哪怕是碰到了这么多事，我还是拿出了我玩解谜游戏的所有实力，尽力去处理好每一件事。<del>如果还是对生活的不幸感到愤愤不平，不如带着幸灾乐祸的心理想一想我这次事件里的美联航。他们的航班延误，影响了那么多乘客，最后给每个人都赔了一晚的五星级酒店，还不是把问题解决了。</del></p>
<hr>
<p>以上是我头脑冷静时写的东西。参会过程省略以示愤怒。</p>
<hr>
<p>本来我准备了很多要写的东西，回程的时候又被气到了，累死了，不写了。这“一天”里，我提前3小时到机场，坐2小时飞机，等5小时飞机，又坐15小时飞机。累计睡眠5~6乘1个小时。到了新加坡我还想省钱，没打车，在机场走了半小时，又坐了1小时地铁。下地铁后走回家，莫名其妙绕了路，在太阳底下穿长裤走了1个多小时。好不容易回到家，洗完澡想刷个牙，牙膏还没了。真作假时假亦真，没睡醒时醒亦困。看上去我这几天睡了很多次觉，但我根本没有睡醒。算上在美国的时间，我已经颠沛流离了好几天，各种硬抗时差，已经不知道睡够六小时是怎样的感觉了。别人比我早一两天来，晚一两天走，玩得开开心心。我除了在会场当3天“好学生”外，剩下近一周时间全在参与人生的极限挑战。怪不得学术大佬都不愿意出来开会，从明年开始我也是学术大佬了。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/06/05/20240407-SVD-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/06/05/20240407-SVD-1/" class="post-title-link" itemprop="url">Stable Video Diffusion 源码解读 (Diffusers 版)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-06-05 15:52:35" itemprop="dateCreated datePublished" datetime="2024-06-05T15:52:35+08:00">2024-06-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">代码阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在上篇文章中，我们浏览了 Stable Video Diffusion (SVD) 的论文，并特别学习了没有在论文中提及的模型结构、噪声调度器这两个模块。在这篇文章中，让我们来看看 SVD 在 Diffusers 中的源码实现。我们会先学习 SVD 的模型结构，再学习 SVD 的采样流水线。在本文的多数章节中，我都会将 SVD 的结构与 Stable Diffusion (SD) 的做对比，帮助之前熟悉 SD 的读者快速理解 SVD 的性质。强烈建议读者在阅读本文前先熟悉 SD 及其在 Diffusers 中的实现。</p>
<p><a href="https://zhouyifan.net/2024/01/23/20230713-SD3/">Stable Diffusion Diffusers 实现源码解读</a></p>
<h2 id="简单采样实验"><a href="#简单采样实验" class="headerlink" title="简单采样实验"></a>简单采样实验</h2><p>目前开源的 SVD 仅有图生视频模型，即给定视频首帧，模型生成视频的后续内容。在首次开源时，SVD 有 1.0 和 1.0-xt 两个版本。二者模型结构配置相同，主要区别在于训练数据上。SVD 1.0 主要用于生成 14 帧 576x1024 的视频，而 1.0-xt 版本由 1.0 模型微调而来，主要用于生成 25 帧 576x1024 的视频。后来，开发团队又开源了 SVD 1.1-xt，该模型在固定帧率的视频数据上微调，输出视频更加连贯。为了做实验方便，在这篇文章中，我们将使用最基础的 SVD 1.0 模型。</p>
<p>参考 Diffusers 官方文档: <a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/main/en/using-diffusers/svd">https://huggingface.co/docs/diffusers/main/en/using-diffusers/svd</a> ，我们来创建一个关于 SVD 的 “Hello World” 项目。如果你的电脑可以访问 HuggingFace 原站的话，直接运行下面的脚本就行了；如果不能访问原网站，可以尝试取消代码里的那行注释，访问 HuggingFace 镜像站；如果还是不行，则需要手动下载 “stabilityai/stable-video-diffusion-img2vid” 仓库，并将仓库路径改成本地下载的仓库路径。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os.environ[&#x27;HF_ENDPOINT&#x27;] = &#x27;https://hf-mirror.com&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> StableVideoDiffusionPipeline</span><br><span class="line"><span class="keyword">from</span> diffusers.utils <span class="keyword">import</span> load_image, export_to_video</span><br><span class="line"></span><br><span class="line">pipe = StableVideoDiffusionPipeline.from_pretrained(</span><br><span class="line">    <span class="string">&quot;stabilityai/stable-video-diffusion-img2vid&quot;</span>, torch_dtype=torch.float16, variant=<span class="string">&quot;fp16&quot;</span></span><br><span class="line">)</span><br><span class="line">pipe.enable_model_cpu_offload()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the conditioning image</span></span><br><span class="line">image = load_image(<span class="string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png&quot;</span>)</span><br><span class="line">image = image.resize((<span class="number">1024</span>, <span class="number">576</span>))</span><br><span class="line"></span><br><span class="line">generator = torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">frames = pipe(image, decode_chunk_size=<span class="number">8</span>, generator=generator).frames[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">export_to_video(frames, <span class="string">&quot;generated.mp4&quot;</span>, fps=<span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<p>成功运行后，我们能得到这样的一个火箭升空视频。它的第一帧会和我们的输入图片一模一样。</p>
<p><img src="/2024/06/05/20240407-SVD-1/rocket.gif" alt></p>
<h2 id="SVD-概览"><a href="#SVD-概览" class="headerlink" title="SVD 概览"></a>SVD 概览</h2><p>由于 SVD 并没有在论文里对其图生视频模型做详细的介绍，我们没有官方资料可以参考，只能靠阅读源码来了解 SVD 的实现细节。为了让大家在读代码时不会晕头转向，我会在读代码前简单概述一下 SVD 的模型结构和采样方法。</p>
<p>SVD 和 SD 一样，是一个隐扩散模型（Latent Diffusion Model, LDM）。图像（视频帧）的生成由两个阶段组成：先由扩散模型生成压缩图像，再由 VAE 解码成真实图像。</p>
<p>扩散模型在生成图像时，会用一个去噪 U-Net $\epsilon_\theta$ 反复对纯噪声图像 $z_T$ 去噪，直至得到一幅有意义的图片 $z$。为了让模型输出我们想要的图像，我们会用一些额外的信息来约束模型，或者说将约束信息也输入进 U-Net。对于文生图 SD 来说，额外约束是文本。对于图生视频 SVD 来说，额外约束是图像。LDM 提出了两种输入约束信息的方式：与输入噪声图像拼接、作为交叉注意力模块的 K, V。SD 仅使用了交叉注意力的方式，而 SVD 同时使用了两种方式。</p>
<p><img src="/2024/06/05/20240407-SVD-1/0-1.jpg" alt></p>
<p>上面这两种添加约束信息的方法适用于信息量比较大的约束。实际上，还有一种更简单的输入实数约束信息的方法。除了噪声输入外，去噪模型还必须输入当前的去噪时刻 $t$。自最早的 DDPM 以来，时刻 $t$ 都是先被转换成位置编码，再输入进 U-Net 的所有残差块中。仿照这种输入机制，如果有其他的约束信息和 $t$ 一样可以用一个实数表示，则不必像前面那样将这种约束信息与输入拼接或输入交叉注意力层，只需要把约束也转换成位置编码，再与 $t$ 的编码加在一起。</p>
<p>SVD 给模型还添加了三种额外约束：噪声增强程度、帧率、运动程度。这三种约束都是用和时刻编码相加的形式实现的。</p>
<p><img src="/2024/06/05/20240407-SVD-1/0-2.jpg" alt></p>
<p>即使现在不完全理解这三种额外约束的意义也不要紧。稍后我们会在学习 U-Net 结构时看到这种额外约束是怎么添加进 U-Net 的，在学习采样流水线时了解这三种约束的意义。</p>
<p>总结一下，除了添加了少数模块外，SVD 和 SD 的整体架构一样，都是以去噪 U-Net 为核心的 LDM。除了原本扩散模型要求的噪声、去噪时刻这两种输入外，SVD 还加入了 4 种约束信息：约束图像（视频首帧）、噪声增强程度、帧率、运动程度。约束图像是最主要的约束信息，它会与噪声输入拼接，且输入进 U-Net 的交叉注意力层中。后三种额外约束会以和处理去噪时刻类似的方式输入进 U-Net 中。</p>
<h2 id="去噪模型结构"><a href="#去噪模型结构" class="headerlink" title="去噪模型结构"></a>去噪模型结构</h2><p>接下来，我们来学习 SVD 的去噪模型的结构。在 Diffusers 中，一个扩散模型的参数、配置全部放在一个模型文件夹里，该文件夹的各个子文件夹存储了模型的各个模块，如自编码器、去噪模型、调度器等。我们可以在 <code>https://huggingface.co/stabilityai/stable-video-diffusion-img2vid/tree/main</code> 找到 SVD 的模型文件夹，或者访问我们本地下载好的模型文件夹。</p>
<p>SVD 的去噪 U-Net 放在模型文件夹的 <code>unet</code> 子文件夹里。通过阅读子文件夹里的 <code>config.json</code>，我们就能知道模型类的名字是什么，并知道初始化模型的参数有哪些。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_class_name&quot;</span>: <span class="string">&quot;UNetSpatioTemporalConditionModel&quot;</span>,</span><br><span class="line">  ...</span><br><span class="line">  <span class="attr">&quot;down_block_types&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;DownBlockSpatioTemporal&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  ...</span><br><span class="line">  <span class="attr">&quot;up_block_types&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;UpBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlockSpatioTemporal&quot;</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>通过在本地 Diffusers 库文件夹里搜索类名 <code>UNetSpatioTemporalConditionModel</code>，或者利用 IDE 的 Python 智能提示功能，在前文的示例脚本里跳转到 <code>StableVideoDiffusionPipeline</code> 所在文件，再跳转到 <code>UNetSpatioTemporalConditionModel</code> 所在文件，我们就能知道 SVD 的去噪 U-Net 类定义在 <code>diffusers/models/unet_spatio_temporal_condition.py</code> 里。我们可以对照位于 <code>diffusers/models/unet_2d_condition.py</code> 的 SD 的 2D U-Net 类 <code>UNet2DConditionModel</code> 来看一下 SVD 的 U-Net 有何不同。</p>
<p>先来看 <code>__init__</code> 构造函数。SVD U-Net 几乎就是一个写死了许多参数的特化版 2D U-Net，其构造函数也基本上是 SD 2D U-Net 的构造函数的子集。比如 2D U-Net 允许用 <code>act_fn</code> 来指定模型的激活函数，默认为 <code>&quot;silu&quot;</code>，而 SVD U-Net 直接把所有模块的激活函数写死成 <code>&quot;silu&quot;</code>。经过简化后，SVD U-Net 的构造函数可读性高了很多。我们从参数开始读起，逐一了解构造函数每一个参数的意义：</p>
<ul>
<li><code>sample_size=None</code>：隐空间图片边长。供其他代码调用，与 U-Net 无关。</li>
<li><code>in_channels=8</code>：输入通道数。</li>
<li><code>out_channels=4</code>: 输出通道数。</li>
<li><code>down_block_types</code>：每一大层下采样模块的类名。</li>
<li><code>up_block_types</code>：每一大层上采样模块的类名。</li>
<li><code>block_out_channels = (320, 640, 1280, 1280)</code>：每一大层的通道数。</li>
<li><code>addition_time_embed_dim=256</code>: 每个额外约束的通道数。</li>
<li><code>projection_class_embeddings_input_dim=768</code>: 所有额外约束的通道数。</li>
<li><code>layers_per_block=2</code>: 每一大层有几个结构相同的模块。</li>
<li><code>cross_attention_dim=1024</code>: 交叉注意力层的通道数。</li>
<li><code>transformer_layers_per_block=1</code>: 每一大层的每一个模块里有几个 Transformer 层。</li>
<li><code>num_attention_heads=(5, 10, 10, 20)</code>: 各大层多头注意力层的头数。</li>
<li><code>num_frames=25</code>: 训练时的帧数。供其他代码调用，与 U-Net 无关。</li>
</ul>
<p>SVD U-Net 的参数基本和 SD 的一致，不同之处有：1）稍后我们会在采样流水线里看到，SVD 把图像约束拼接到了噪声图像上，所以整个噪声输入的通道数是原来的两倍，从 4 变为 8；2）多了一个给采样代码用的 <code>num_frames</code> 参数，它其实没有被 U-Net 用到。</p>
<p>我们再来大致过一下构造函数的实现细节。SVD U-Net 的整体结构和 2D U-Net 的几乎一致。数据先经过下采样模块，再经过中间模块，最后过上采样模块。下采样模块和上采样模块之间有短路连接。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, down_block_type <span class="keyword">in</span> <span class="built_in">enumerate</span>(down_block_types):</span><br><span class="line">    ...</span><br><span class="line">    down_block = get_down_block(...)</span><br><span class="line">    self.down_blocks.append(down_block)</span><br><span class="line"></span><br><span class="line">self.mid_block = UNetMidBlockSpatioTemporal(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, up_block_type <span class="keyword">in</span> <span class="built_in">enumerate</span>(up_block_types):</span><br><span class="line">    ...</span><br><span class="line">    up_block = get_up_block(...)</span><br><span class="line">    self.up_blocks.append(up_block)</span><br><span class="line"></span><br><span class="line">self.conv_norm_out = nn.GroupNorm(...)</span><br><span class="line">self.conv_act = nn.SiLU()</span><br><span class="line">self.conv_out = nn.Conv2d(...)</span><br></pre></td></tr></table></figure>
<p>扩散模型还需要处理去噪时刻约束 $t$。U-Net 会先用正弦编码（Transformer 里的位置编码）<code>time_proj</code> 来将时刻转为向量，再用一系列线性层 <code>time_embedding</code> 预处理这个编码。该编码后续会输入进 U-Net 主体的每一个模块中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.time_proj = Timesteps(block_out_channels[<span class="number">0</span>], <span class="literal">True</span>, downscale_freq_shift=<span class="number">0</span>)</span><br><span class="line">timestep_input_dim = block_out_channels[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)</span><br></pre></td></tr></table></figure>
<p>除了多数扩散模型都有的 U-Net 模块外，SVD 还加入了额外约束模块。如前文所述，对于能用一个实数表示的约束，可以使用和处理时刻类似的方式，先让其过位置编码层，再过线性层，最后把得到的输出编码和时刻编码加起来。所以，和这种额外约束相关的模块在代码里叫做 <code>add_time</code>。在 2D U-Net 里，额外约束是可选的。SD 没有用到额外约束。而 SVD 把额外约束设为了必选模块。稍后我们会在采样流水线里看到，SVD 将视频的帧率、运动程度、噪声增强强度作为了生成时的额外约束。这些约束都是用这种与时刻编码相加的形式实现的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.add_time_proj = Timesteps(addition_time_embed_dim, <span class="literal">True</span>, downscale_freq_shift=<span class="number">0</span>)</span><br><span class="line">self.add_embedding = TimestepEmbedding(projection_class_embeddings_input_dim, time_embed_dim)</span><br></pre></td></tr></table></figure></p>
<p>构造函数的代码就看完了。在构造函数中，我们认识了 SVD U-Net 的各个模块，但对其工作原理或许还存在着些许疑惑。我们来模型的前向传播函数 <code>forward</code> 里看一下各个模块是怎么处理输入的。</p>
<p>看代码前，我们先回顾一下概念，整理一下 U-Net 的数据处理流程。下面是我之前给 SD U-Net 画的示意图。该图对 SVD 同样适用。和 SD 相比，SVD 的输入 <code>x</code> 不仅包括噪声图像（准确说是多个表示视频帧的图像），还包括作为约束的首帧图像； <code>c</code> 换成了首帧图像的 CLIP 编码；<code>t</code> 不仅包括时刻，还包括一些额外约束。</p>
<p><img src="/2024/06/05/20240407-SVD-1/1-1.jpg" alt></p>
<p>和上图所示的一样，SVD U-Net 的 <code>forward</code> 方法的输入包含图像 <code>sample</code>，时刻 <code>timestep</code>，交叉注意力层约束（图像编码） <code>encoder_hidden_states</code> , 额外约束 <code>added_time_ids</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    sample: torch.FloatTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    timestep: <span class="type">Union</span>[torch.Tensor, <span class="built_in">float</span>, <span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    encoder_hidden_states: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    added_time_ids: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    return_dict: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br></pre></td></tr></table></figure>
<p>方法首先会处理去噪时刻和额外参数，我们来看一下这两个输入是怎么拼到一起的。</p>
<p>做完一系列和形状相关的处理后，输入时刻 <code>timestep</code> 变成了 <code>timesteps</code>。随后，该变量会先过正弦编码（位置编码）层 <code>time_proj</code>，再过一些线性层 <code>time_embedding</code>，得到最后输入 U-Net 主体的时刻嵌入 <code>emb</code>。这两个模块的命名非常容易混淆，千万别弄反了。类似地，额外约束也是先过正弦编码层 <code>add_time_proj</code>，再过一些线性层 <code>add_embedding</code>，最后其输出 <code>aug_emb</code> 会加到 <code>emb</code> 上。当然，为了确保结果可以相加，<code>time_embedding</code> 和 <code>add_time_proj</code> 的输出通道数是相同的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># preprocessing</span></span><br><span class="line"><span class="comment"># timesteps = timestep</span></span><br><span class="line"></span><br><span class="line">t_emb = self.time_proj(timesteps)</span><br><span class="line">t_emb = t_emb.to(dtype=sample.dtype)</span><br><span class="line"></span><br><span class="line">emb = self.time_embedding(t_emb)</span><br><span class="line"></span><br><span class="line">time_embeds = self.add_time_proj(added_time_ids.flatten())</span><br><span class="line">time_embeds = time_embeds.reshape((batch_size, -<span class="number">1</span>))</span><br><span class="line">time_embeds = time_embeds.to(emb.dtype)</span><br><span class="line">aug_emb = self.add_embedding(time_embeds)</span><br><span class="line">emb = emb + aug_emb</span><br></pre></td></tr></table></figure>
<p>这里有关额外约束的处理写得很差，逻辑也很难读懂。在构造函数里，额外约束的正弦编码层 <code>add_time_proj</code> 的输出通道数 <code>addition_time_embed_dim</code> 是 256, 线性模块 <code>add_embedding</code> 的输入通道数 <code>projection_class_embeddings_input_dim</code> 是 768。两个通道数不一样的模块是怎么接起来的？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">  ...</span></span></span><br><span class="line"><span class="params"><span class="function">  addition_time_embed_dim: <span class="built_in">int</span> = <span class="number">256</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">  projection_class_embeddings_input_dim: <span class="built_in">int</span> = <span class="number">768</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">  ...</span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">self</span>.<span class="title">add_time_proj</span> = <span class="title">Timesteps</span>(<span class="params">addition_time_embed_dim, <span class="literal">True</span>, downscale_freq_shift=<span class="number">0</span></span>)</span></span><br><span class="line"><span class="function"><span class="title">self</span>.<span class="title">add_embedding</span> = <span class="title">TimestepEmbedding</span>(<span class="params">projection_class_embeddings_input_dim, time_embed_dim</span>)</span></span><br></pre></td></tr></table></figure>
<p>原来，在下面这份模块前向传播代码中，<code>added_time_ids</code> 的形状是 <code>[batch_size, 3]</code>。其中的 <code>3</code> 表示有三个额外约束。做了 <code>flatten()</code> 再过 <code>add_time_proj</code> 后，可以得到形状为 <code>[3 * batch_size, 256]</code> 的正弦编码 <code>time_embeds</code>。之所以三个约束可以用同一个模块来处理，是因为正弦编码没有学习参数，对所有输入都会产生同样的输出。得到 <code>time_embeds</code> 后，再根据从输入噪声图像里得到的 <code>batch_size</code>，用 <code>reshape</code> 把 <code>time_embeds</code> 的形状变成 <code>[batch_size, 768]</code>。这样，<code>time_embeds</code> 就可以输入进 <code>add_embedding</code> 里了。 <code>add_embedding</code> 是有可学习参数的，三个约束必须分别处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">time_embeds = self.add_time_proj(added_time_ids.flatten())</span><br><span class="line">time_embeds = time_embeds.reshape((batch_size, -<span class="number">1</span>))</span><br><span class="line">time_embeds = time_embeds.to(emb.dtype)</span><br><span class="line">aug_emb = self.add_embedding(time_embeds)</span><br></pre></td></tr></table></figure>
<p>这些代码不应该这样写的。当前的写法不仅可读性差，还不利于维护。比较好的写法是在构造函数里把输入参数从<code>projection_class_embeddings_input_dim</code> 改为 <code>num_add_time</code>，表示额外约束的数量。之后，把 <code>add_embedding</code> 的输入通道数改成 <code>num_add_time * addition_time_embed_dim</code>。这样，使用者不必手动设置合理的 <code>add_embedding</code> 的输入通道数（比如保证 768 必须是 256 的 3 倍），只设置有几个额外约束就行了。这样改了之后，为了提升可读性，还可以像下面那样把 <code>reshape</code> 里的那个 <code>-1</code> 写清楚来。Diffusers 采用这种比较混乱的写法，估计是因为这段代码是从 2D U-Net 里摘抄出来的。而原 2D U-Net 需要兼容更复杂的情况，所以 <code>add_time_proj</code> 和 <code>add_embedding</code> 的通道数需要分别指定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">time_embeds = time_embeds.reshape((batch_size, -<span class="number">1</span>))</span><br><span class="line">-&gt;</span><br><span class="line">time_embeds = time_embeds.reshape((batch_size, self.num_add_time * self.addition_time_embed_dim))</span><br></pre></td></tr></table></figure>
<p>预处理完时刻和额外约束后，方法还会修改所有输入的形状，使得它们第一维的长度都是 <code>batch_size</code> 乘视频帧数。正如我们在上一篇文章中学到的，为了兼容图像模型里的模块，我们要先把视频长度那一维和 batch 那一维合并，等到了和时序相关的模块再对视频长度那一维单独处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Flatten the batch and frames dimensions</span></span><br><span class="line"><span class="comment"># sample: [batch, frames, channels, height, width] -&gt; [batch * frames, channels, height, width]</span></span><br><span class="line">sample = sample.flatten(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># Repeat the embeddings num_video_frames times</span></span><br><span class="line"><span class="comment"># emb: [batch, channels] -&gt; [batch * frames, channels]</span></span><br><span class="line">emb = emb.repeat_interleave(num_frames, dim=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># encoder_hidden_states: [batch, 1, channels] -&gt; [batch * frames, 1, channels]</span></span><br><span class="line">encoder_hidden_states = encoder_hidden_states.repeat_interleave(num_frames, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>后面的代码就和 2D U-Net 的几乎一样了。数据依次经过下采样块、中间块、上采样块。下采样块的中间结果还会保存在栈 <code>down_block_res_samples</code> 里，作为上采样模块的额外输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sample = self.conv_in(sample)</span><br><span class="line"></span><br><span class="line">image_only_indicator = torch.zeros(batch_size, num_frames, dtype=sample.dtype, device=sample.device)</span><br><span class="line"></span><br><span class="line">down_block_res_samples = (sample,)</span><br><span class="line"><span class="keyword">for</span> downsample_block <span class="keyword">in</span> self.down_blocks:</span><br><span class="line">    sample, res_samples = downsample_block(...)</span><br><span class="line">    down_block_res_samples += res_samples</span><br><span class="line"></span><br><span class="line">sample = self.mid_block(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, upsample_block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.up_blocks):</span><br><span class="line">    res_samples = down_block_res_samples[-<span class="built_in">len</span>(upsample_block.resnets) :]</span><br><span class="line">    down_block_res_samples = down_block_res_samples[: -<span class="built_in">len</span>(upsample_block.resnets)]</span><br><span class="line">    sample = upsample_block(...)</span><br><span class="line"></span><br><span class="line">sample = self.conv_norm_out(sample)</span><br><span class="line">sample = self.conv_act(sample)</span><br><span class="line">sample = self.conv_out(sample)</span><br></pre></td></tr></table></figure>
<p>光看 U-Net 类，我们还看不出 SVD 的 3D U-Net 和 2D U-Net 的区别。接下来，我们来看一看 U-Net 中某一个具体的模块是怎么实现的。由于 U-Net 下采样块、中间块、上采样块的结构是类似的，我们只挑某一大层的下采样模块类 <code>CrossAttnDownBlockSpatioTemporal</code> 来学习。</p>
<p>在 <code>CrossAttnDownBlockSpatioTemporal</code> 类中，我们可以看到 SVD U-Net 的每一个子模块都可以拆成残差卷积块和 Transformer 块。数据在经过子模块时，会先过残差块，再过 Transformer 块。我们来继续深究时序残差块类 <code>SpatioTemporalResBlock</code> 和时序 Transformer 块 <code>TransformerSpatioTemporalModel</code> 的实现细节。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># __init__</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">    in_channels = in_channels <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> out_channels</span><br><span class="line">    resnets.append(</span><br><span class="line">        SpatioTemporalResBlock(...)</span><br><span class="line">    )</span><br><span class="line">    attentions.append(</span><br><span class="line">        TransformerSpatioTemporalModel(...)</span><br><span class="line">    )</span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line">blocks = <span class="built_in">list</span>(<span class="built_in">zip</span>(self.resnets, self.attentions))</span><br><span class="line"><span class="keyword">for</span> resnet, attn <span class="keyword">in</span> blocks:</span><br><span class="line">    hidden_states = resnet(hidden_states, ...)</span><br><span class="line">    hidden_states = attn(hidden_states, ...)</span><br></pre></td></tr></table></figure>
<p>在开始看代码之前，我们再回顾一下论文里有关 3D U-Net 块的介绍。SVD 的 U-Net 是从 Video LDM 的 U-Net 改过来的。下面的模块结构图源自 Video LDM 论文，我将其改成了能描述 SVD U-Net 块的图。图中红框里的模块表示在原 SD 2D U-Net 块的基础上新加入的模块。可以看出，SVD 实际上就是在原来的 2D 残差块后面加了一个 3D 卷积层，原空间注意力块后面加了一个时序注意力层。旧模块输出和新模块输出之间用一个比例 $\alpha$ 来线性混合。中间数据形状变换的细节我们已经在上篇文章里学过了，这篇文章里我们主要关心这些模块在代码里大概是怎么定义的。</p>
<p><img src="/2024/06/05/20240407-SVD-1/1-2.jpg" alt></p>
<p>3D 残差块类 <code>SpatioTemporalResBlock</code> 在 <code>diffusers/models/resnet.py</code> 文件中。它有三个子模块，分别对应上文示意图中的 2D 残差块、时序残差块（3D 卷积）、混合模块。在运算时，旧模块的输出会缓存到<br> <code>hidden_states_mix</code> 中，新模块的输出为 <code>hidden_states</code>，二者最终会送入混合模块 <code>time_mixer</code> 做一个线性混合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpatioTemporalResBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.spatial_res_block = ResnetBlock2D(...)</span><br><span class="line">        self.temporal_res_block = TemporalResnetBlock(...)</span><br><span class="line">        self.time_mixer = AlphaBlender(...)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        hidden_states = self.spatial_res_block(hidden_states, temb)</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        hidden_states_mix = hidden_states</span><br><span class="line">        </span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        hidden_states = self.temporal_res_block(hidden_states, temb)</span><br><span class="line">        hidden_states = self.time_mixer(</span><br><span class="line">            x_spatial=hidden_states_mix,</span><br><span class="line">            x_temporal=hidden_states,</span><br><span class="line">        )</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p><code>ResnetBlock2D</code> 是 SD 2D U-Net 的残差模块，我们在这篇文章里就不去学习它了。 时序残差块 <code>TemporalResnetBlock</code> 和 2D 残差块的结构几乎完全一致，唯一的区别在于 2D 卷积被换成了 3D 卷积。从代码中我们可以知道，这个模块是一个标准的残差块，数据会依次过两个卷积层，并在最后输出前与输入相加。扩散模型中的时刻约束 <code>temb</code> 会在数据过完第一个卷积层后，加到数据上。值得注意的是，虽然类里面的卷积层名字叫 3D 卷积，但实际上它的卷积核形状为 <code>(3, 1, 1)</code>，这说明这个卷积层实际上只是一个时序维度上窗口大小为 3 的 1D 卷积层。</p>
<p><img src="/2024/06/05/20240407-SVD-1/1-3.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemporalResnetBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        kernel_size = (<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        padding = [k // <span class="number">2</span> <span class="keyword">for</span> k <span class="keyword">in</span> kernel_size]</span><br><span class="line"></span><br><span class="line">        self.norm1 = torch.nn.GroupNorm(...)</span><br><span class="line">        self.conv1 = nn.Conv3d(...)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> temb_channels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.time_emb_proj = nn.Linear(temb_channels, out_channels)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.time_emb_proj = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.norm2 = torch.nn.GroupNorm(...)</span><br><span class="line"></span><br><span class="line">        self.dropout = torch.nn.Dropout(<span class="number">0.0</span>)</span><br><span class="line">        self.conv2 = nn.Conv3d(...)</span><br><span class="line"></span><br><span class="line">        self.nonlinearity = get_activation(<span class="string">&quot;silu&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.use_in_shortcut = self.in_channels != out_channels</span><br><span class="line"></span><br><span class="line">        self.conv_shortcut = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> self.use_in_shortcut:</span><br><span class="line">            self.conv_shortcut = nn.Conv3d(...)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_tensor, temb</span>):</span></span><br><span class="line">        hidden_states = input_tensor</span><br><span class="line"></span><br><span class="line">        hidden_states = self.norm1(hidden_states)</span><br><span class="line">        hidden_states = self.nonlinearity(hidden_states)</span><br><span class="line">        hidden_states = self.conv1(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.time_emb_proj <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            temb = self.nonlinearity(temb)</span><br><span class="line">            temb = self.time_emb_proj(temb)[:, :, :, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">            temb = temb.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">            hidden_states = hidden_states + temb</span><br><span class="line"></span><br><span class="line">        hidden_states = self.norm2(hidden_states)</span><br><span class="line">        hidden_states = self.nonlinearity(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.conv2(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.conv_shortcut <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_tensor = self.conv_shortcut(input_tensor)</span><br><span class="line"></span><br><span class="line">        output_tensor = input_tensor + hidden_states</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_tensor</span><br></pre></td></tr></table></figure>
<p>混合模块 <code>AlphaBlender</code> 其实就只是定义了一个可学习的混合比例 <code>mix_factor</code>，之后用这个比例来混合空间层输出和时序层输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlphaBlender</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        alpha: <span class="built_in">float</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.register_parameter(<span class="string">&quot;mix_factor&quot;</span>, torch.nn.Parameter(torch.Tensor([alpha])))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        x_spatial,</span></span></span><br><span class="line"><span class="params"><span class="function">        x_temporal,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="comment"># Get mix_factor</span></span><br><span class="line">        alpha = self.get_alpha(...)</span><br><span class="line">        alpha = alpha.to(x_spatial.dtype)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.switch_spatial_to_temporal_mix:</span><br><span class="line">            alpha = <span class="number">1.0</span> - alpha</span><br><span class="line"></span><br><span class="line">        x = alpha * x_spatial + (<span class="number">1.0</span> - alpha) * x_temporal</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>看完了3D 残差块 <code>SpatioTemporalResBlock</code> 的内容，我们接着来看 3D 注意力块 <code>TransformerSpatioTemporalModel</code> 的内容。<code>TransformerSpatioTemporalModel</code> 也主要由 2D Transformer 块 <code>BasicTransformerBlock</code>、时序 Transformer 块 <code>TemporalBasicTransformerBlock</code> 、混合模块组成 <code>AlphaBlender</code>。它们的连接方式和上面的残差块类似。时序 Transformer 块和普通 2D Transformer 块一样，都是有自注意力、交叉注意力、全连接层的标准 Transformer 模块，它们的区别只在于时序 Transformer 块对输入做形状变换的方式不同，会让数据在时序维度上做信息交互。这里我们就不去进一步深究它们的实现细节了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerSpatioTemporalModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_attention_heads: <span class="built_in">int</span> = <span class="number">16</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_head_dim: <span class="built_in">int</span> = <span class="number">88</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        in_channels: <span class="built_in">int</span> = <span class="number">320</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        out_channels: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_layers: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        cross_attention_dim: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        self.transformer_blocks = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                BasicTransformerBlock(...)</span><br><span class="line">                <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.temporal_transformer_blocks = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                TemporalBasicTransformerBlock(...)</span><br><span class="line">                <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.time_pos_embed = TimestepEmbedding(in_channels, time_embed_dim, out_dim=in_channels)</span><br><span class="line">        self.time_proj = Timesteps(in_channels, <span class="literal">True</span>, <span class="number">0</span>)</span><br><span class="line">        self.time_mixer = AlphaBlender(alpha=<span class="number">0.5</span>, ...)</span><br></pre></td></tr></table></figure>
<p>这个时序 Transformer 模块类有一个地方值得注意。我们知道，Transformer 模型本身是不知道输入数据的顺序的。无论是注意力层还是全连接层，它们都与顺序无关。为了让模型知道数据的先后顺序，比如在 NLP 里我们希望模型知道一句话里每个单词的前后顺序，我们会给输入数据加上位置编码。而有些时候我们觉得模型不用知道数据的先后顺序。比如在 SD 的 2D 图像 Transformer 块里，我们把每个像素当成一个 token，每个像素在 Transformer 块的运算方式是相同的，与其所在位置无关。而在处理视频时序的 Transformer 块中，知道视频每一帧的先后顺序看起来还是很重要的。所以，和 SD 的 2D Transformer 块不同，SVD 的时序 Transformer 块根据视频的帧号设置了位置编码，用和 NLP 里处理文本类似的方式处理视频。SVD 的时序 Transformer 类在构造函数里定义了生成位置编码的模块 <code>TimestepEmbedding</code>, <code>Timesteps</code>。在前向传播时，<code>forward</code> 方法会用 <code>torch.arange(num_frames)</code> 根据总帧数生成帧号列表，并经过两个模块得到最终的位置编码嵌入 <code>emb</code>。嵌入 <code>emb</code> 会在数据过时序 Transformer 块前与输入 <code>hidden_states_mix</code> 相加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerSpatioTemporalModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.time_pos_embed = TimestepEmbedding(in_channels, time_embed_dim, out_dim=in_channels)</span><br><span class="line">        self.time_proj = Timesteps(in_channels, <span class="literal">True</span>, <span class="number">0</span>)</span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        num_frames_emb = torch.arange(num_frames, device=hidden_states.device)</span><br><span class="line">        num_frames_emb = num_frames_emb.repeat(batch_size, <span class="number">1</span>)</span><br><span class="line">        num_frames_emb = num_frames_emb.reshape(-<span class="number">1</span>)</span><br><span class="line">        t_emb = self.time_proj(num_frames_emb)</span><br><span class="line">        emb = self.time_pos_embed(t_emb)</span><br><span class="line">        emb = emb[:, <span class="literal">None</span>, :]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> block, temporal_block <span class="keyword">in</span> <span class="built_in">zip</span>(self.transformer_blocks, self.temporal_transformer_blocks):</span><br><span class="line">            hidden_states = block(</span><br><span class="line">                ...</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            hidden_states_mix = hidden_states</span><br><span class="line">            hidden_states_mix = hidden_states_mix + emb</span><br><span class="line"></span><br><span class="line">            hidden_states_mix = temporal_block(...)</span><br><span class="line">            hidden_states = self.time_mixer(...)</span><br><span class="line"></span><br><span class="line">        hidden_states = self.proj_out(hidden_states)</span><br><span class="line">        hidden_states = hidden_states.reshape(batch_frames, height, width, inner_dim).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line"></span><br><span class="line">        output = hidden_states + residual</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>到这里，我们就读完了 SVD U-Net 的主要代码。相比 SD U-Net，SVD U-Net 主要做了以下修改：</p>
<ul>
<li>由于输入多了一张约束图像，输入通道数变为原来的两倍。</li>
<li>多加了三个和视频相关的额外约束。它们是通过和扩散模型的时刻嵌入相加输入进模型的。它们的命名通常与 <code>add_time</code> 相关。</li>
<li>仿照 Video LDM 的结构设计，SVD 也在 2D 残差块后面加入了由 3D 卷积组成的时序残差块，在空间 Transformer 块后面加入了对时序维度做注意力的时序 Transformer 块。新旧模块的输出会以一个可学习的比例线性混合。</li>
</ul>
<h2 id="VAE-结构"><a href="#VAE-结构" class="headerlink" title="VAE 结构"></a>VAE 结构</h2><p>SVD 不仅微调了 SD 的 U-Net，还微调了 VAE 的解码器，让输出视频在时序上更加连贯。由于更新 VAE 和更新 U-Net 的方法几乎一致，我们就来快速看一下 SVD 的时序 VAE 的结构，而跳过每个模块的更新细节。</p>
<p>通过阅读 VAE 的配置文件，我们可以知道时序 VAE 的类名为  <code>AutoencoderKLTemporalDecoder</code>，它位于文件 <code>diffusers/models/autoencoders/autoencoder_kl_temporal_decoder.py</code> 中。从它的构造函数里我们可以知道，时序 VAE 的编码器类是 <code>Encoder</code>，和 SD 的一样，只是解码器类从 <code>Decoder</code> 变成了 <code>TemporalDecoder</code>。我们来看一下这个新解码器类的代码做了哪些改动。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AutoencoderKLTemporalDecoder</span>(<span class="params">ModelMixin, ConfigMixin</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        in_channels: <span class="built_in">int</span> = <span class="number">3</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        out_channels: <span class="built_in">int</span> = <span class="number">3</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.encoder = Encoder(...)</span><br><span class="line"></span><br><span class="line">        self.decoder = TemporalDecoder(...)</span><br><span class="line"></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>在 SD 中，VAE 和 U-Net 的组成模块是几乎一致的，二者的结构主要有三个区别：1）由于 VAE 的解码器和编码器是独立的，它们之间没有残差连接。而 U-Net 是一个整体，它的编码器（下采样块）和解码器（上采样块）之间有残差连接，以减少数据在下采样中的信息损失; 2）由于 VAE 中图像的尺寸较大，仅在 VAE 最深层图像尺寸为 <code>64x64</code> 时才有自注意力层。具体来说，这个自注意力层加到了 VAE 解码器的一开头，代码中相关模块称为 <code>mid_block</code>；3）VAE 仅有空间自注意力，而 SD U-Net 用了完整的 Transformer 块（包含自注意力层、交叉注意力层、全连接层）。由于 SD VAE 和 U-Net 结构上的相似性，SVD 的开发者直接把对 U-Net 的更新也搬到了 VAE 上来。</p>
<p>SVD VAE 解码器仅做了两项更新：1）将所有模块里的 2D 残差块都被换成了我们在上文中见过的 3D 残差块；2）在最终输出前加了一个 3D 卷积（时序维度上的 1D 卷积）。VAE 的自注意力层的结构并没有更新。更新 2D 残差块的方法和 U-Net 的是一致的。比如在新的上采样块类 <code>UpBlockTemporalDecoder</code> 中，我们就可以看到之前在新 U-Net 里看过的 3D 残差块类 <code>SpatioTemporalResBlock</code> 的身影。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> ..unet_3d_blocks <span class="keyword">import</span> MidBlockTemporalDecoder, UpBlockTemporalDecoder</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpBlockTemporalDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            ...</span><br><span class="line">            resnets.append(SpatioTemporalResBlock(...))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemporalDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers_per_block = layers_per_block</span><br><span class="line"></span><br><span class="line">        self.conv_in = nn.Conv2d(...)</span><br><span class="line">        self.mid_block = MidBlockTemporalDecoder(...)</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(block_out_channels)):</span><br><span class="line">            ...</span><br><span class="line">            up_block = UpBlockTemporalDecoder(...)</span><br><span class="line">            self.up_blocks.append(up_block)</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        conv_out_kernel_size = (<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.time_conv_out = torch.nn.Conv3d(...)</span><br></pre></td></tr></table></figure>
<h2 id="采样流水线"><a href="#采样流水线" class="headerlink" title="采样流水线"></a>采样流水线</h2><p>看完了 U-Net 和 VAE 的代码后，我们来看整套 SVD 的采样代码。和其他方法一样，在 Diffusers 中，一套采样方法会用一个流水线类 (<code>xxxPipeline</code>)来表示。SVD 对应的流水线类叫做 <code>StableVideoDiffusionPipeline</code>。我们可以利用 IDE 的代码跳转功能，在本文开头的示例采样脚本中跳转至 <code>StableVideoDiffusionPipeline</code> 所在源文件 <code>diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py</code>。</p>
<p>如示例脚本所示，使用流水线类时，可以将类实例 <code>pipe</code> 当成一个函数来用。这种用法实际上会调用实例的 <code>__call__</code> 方法。所以，在阅读流水线类的代码时，我们可以先忽略其他部分，直接看 <code>__call__</code> 方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pipe = StableVideoDiffusionPipeline.from_pretrained(...)</span><br><span class="line">frames = pipe(image, decode_chunk_size=<span class="number">8</span>, generator=generator).frames[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p><code>__call__</code> 的参数定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    image: <span class="type">Union</span>[PIL.Image.Image, <span class="type">List</span>[PIL.Image.Image], torch.FloatTensor],</span></span></span><br><span class="line"><span class="params"><span class="function">    height: <span class="built_in">int</span> = <span class="number">576</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    width: <span class="built_in">int</span> = <span class="number">1024</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_frames: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_inference_steps: <span class="built_in">int</span> = <span class="number">25</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    min_guidance_scale: <span class="built_in">float</span> = <span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    max_guidance_scale: <span class="built_in">float</span> = <span class="number">3.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    fps: <span class="built_in">int</span> = <span class="number">7</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    motion_bucket_id: <span class="built_in">int</span> = <span class="number">127</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    noise_aug_strength: <span class="built_in">float</span> = <span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    decode_chunk_size: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_videos_per_prompt: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    generator: <span class="type">Optional</span>[<span class="type">Union</span>[torch.Generator, <span class="type">List</span>[torch.Generator]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    latents: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    output_type: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="string">&quot;pil&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    callback_on_step_end: <span class="type">Optional</span>[<span class="type">Callable</span>[[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="type">Dict</span>], <span class="literal">None</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    callback_on_step_end_tensor_inputs: <span class="type">List</span>[<span class="built_in">str</span>] = [<span class="string">&quot;latents&quot;</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    return_dict: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br></pre></td></tr></table></figure>
<p><code>__call__</code> 的参数就是我们在使用 SVD 采样时能修改的参数，我们需要把其中的主要参数弄懂。各参数的解释如下：</p>
<ul>
<li><code>image</code>：SVD 会根据哪张图片生成视频。</li>
<li><code>height, width</code>: 生成视频的尺寸。如果输入图片与这个尺寸对不上，会将输入图片的尺寸调整为该尺寸。</li>
<li><code>num_frames</code>: 生成视频的帧数。SVD 1.0 版默认 14 帧，1.0-xt 版默认 25 帧。</li>
<li><code>min_guidance_scale</code>, <code>max_guidance_scale</code>: 使用 Classifiser-free Guidance (CFG) 的强度范围。SVD 用了一种特殊的设置 CFG 强度的机制，稍后我们会在采样代码里见到。</li>
<li><code>fps</code>：输出视频期望的帧率。SVD 的额外约束。实际上这个帧率肯定是不准的，只不过提高这个值可以让视频更平滑。</li>
<li><code>motion_bucket_id</code>: SVD 的额外约束。官方没有解释该值的原理，只说明了提高该值能让输出视频的运动更多。</li>
<li><code>noise_aug_strength</code>: 对输入图片添加的噪声强度。值越低输出视频越像原图。</li>
<li><code>decode_chunk_size</code>: 一次放几张图片进时序 VAE 做解码，用于在内存占用和效果之间取得一个平衡。按理说一次处理所有图片得到的视频连续性最好，但那样也会消耗过多的内存。</li>
<li><code>num_videos_per_prompt</code>: 对于每张输入图片 （prompt），输出几段视频。</li>
<li><code>generator</code>: PyTorch 的随机数生成器。如果想要手动控制生成中的随机种子，就手动设置这个变量。</li>
<li><code>latents</code>: 强制指定的扩散模型的初始高斯噪声。</li>
<li><code>output_type</code>: 输出图片格式，是 NumPy、PIL，还是 PyTorch。</li>
<li><code>callback_on_step_end</code>，<code>callback_on_step_end_tensor_inputs</code> 用于在不修改原流水线代码的情况下向采样过程中添加额外的处理逻辑。学习代码的时候可以忽略。</li>
<li><code>return_dict</code>: 流水线是返回一个词典，还是像普通 Python 函数一样返回用元组表示的多个返回值。</li>
</ul>
<p>大致搞清楚了输入参数的意义后，我们来看流水线的执行代码。一开始的代码都是在预处理输入，可以直接跳过。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0. Default height and width to unet</span></span><br><span class="line">height = height <span class="keyword">or</span> self.unet.config.sample_size * self.vae_scale_factor</span><br><span class="line">width = width <span class="keyword">or</span> self.unet.config.sample_size * self.vae_scale_factor</span><br><span class="line"></span><br><span class="line">num_frames = num_frames <span class="keyword">if</span> num_frames <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.unet.config.num_frames</span><br><span class="line">decode_chunk_size = decode_chunk_size <span class="keyword">if</span> decode_chunk_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> num_frames</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Check inputs. Raise error if not correct</span></span><br><span class="line">self.check_inputs(image, height, width)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Define call parameters</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(image, PIL.Image.Image):</span><br><span class="line">    batch_size = <span class="number">1</span></span><br><span class="line"><span class="keyword">elif</span> <span class="built_in">isinstance</span>(image, <span class="built_in">list</span>):</span><br><span class="line">    batch_size = <span class="built_in">len</span>(image)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    batch_size = image.shape[<span class="number">0</span>]</span><br><span class="line">device = self._execution_device</span><br><span class="line">self._guidance_scale = max_guidance_scale</span><br></pre></td></tr></table></figure>
<p>之后，代码开始预处理交叉注意力层的约束信息。在 SD 里，约束信息是文本，所以这一步会用 CLIP 文本编码器得到约束文本的嵌入。而 SVD 是一个图生视频模型，所以这一步会用 CLIP 图像编码器得到约束图像的嵌入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Encode input image</span></span><br><span class="line">image_embeddings = self._encode_image(image, device, num_videos_per_prompt, self.do_classifier_free_guidance)</span><br></pre></td></tr></table></figure>
<p>代码还把额外约束帧率 <code>fps</code> 减了个一，因为训练的时候模型实际上输入的额外约束是 <code>fps - 1</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fps = fps - <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>接着，代码开始处理与噪声拼接的约束图像。回顾一下，SVD 的约束图像以两种形式输入进模型：一种是过 CLIP 图像编码器，以交叉注意力 K，V 的形式输入，其预处理如上部分的代码所示；另一种形式是与原去噪 U-Net 的噪声输入拼接，其预处理如当前这部分代码所示。</p>
<p>在预处理要拼接的图像时，代码会先调用预处理器 <code>image_processor.preprocess</code>，把其他格式的图像转成 PyTorch 的 <code>Tensor</code> 类型。之后，代码会随机生成一点高斯噪声，并把噪声根据噪声增强强度 <code>noise_aug_strength</code> 加到这张约束图像上。这种做法来自于之前有约束图像的扩散模型 <em>Cascaded diffusion models</em>。<code>noise_aug_strength</code> 稍后会作为额外约束输入进 U-Net 里，与去噪时刻的编码相加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image = self.image_processor.preprocess(image, height=height, width=width).to(device)</span><br><span class="line">noise = randn_tensor(image.shape, generator=generator, device=device, dtype=image.dtype)</span><br><span class="line">image = image + noise_aug_strength * noise</span><br></pre></td></tr></table></figure>
<p>加了这个噪声后，图像会过 VAE 的编码器，得到 <code>image_latents</code>。<code>image_latents</code> 会通过 <code>repeat</code> 操作复制成多份，并于稍后拼接到每一帧带噪图像上。注意，一般图像在过 VAE 的编码器后，要乘一个系数 <code>vae.config.scaling_factor</code>; 在过 VAE 的解码器前，要除以这个系数。然而，只有在这个地方，<code>image_latents</code> 没有乘系数。我个人觉得这是开发者的一个失误。当然，做不做这个操作对于模型来说区别不大，因为模型能很快学会这种系数上的差异。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4. Encode input image using VAE</span></span><br><span class="line">image_latents = self._encode_vae_image(</span><br><span class="line">    image,</span><br><span class="line">    device=device,</span><br><span class="line">    num_videos_per_prompt=num_videos_per_prompt,</span><br><span class="line">    do_classifier_free_guidance=self.do_classifier_free_guidance,</span><br><span class="line">)</span><br><span class="line">image_latents = image_latents.to(image_embeddings.dtype)</span><br><span class="line">image_latents = image_latents.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, num_frames, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>下一步，代码会把三个额外约束拼接在一起，得到 <code>added_time_ids</code>。它会接入到 U-Net 中，与时刻编码加到一起。在训练时，帧率 <code>fps</code> 和 运动程度 <code>motion_bucket_id</code> 完全来自于数据集标注，而 <code>noise_aug_strength</code> 是可以随机设置的。在采样时，这三个参数都可以手动设置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5. Get Added Time IDs</span></span><br><span class="line">added_time_ids = self._get_add_time_ids(</span><br><span class="line">    fps,</span><br><span class="line">    motion_bucket_id,</span><br><span class="line">    noise_aug_strength,</span><br><span class="line">    image_embeddings.dtype,</span><br><span class="line">    batch_size,</span><br><span class="line">    num_videos_per_prompt,</span><br><span class="line">    self.do_classifier_free_guidance,</span><br><span class="line">)</span><br><span class="line">added_time_ids = added_time_ids.to(device)</span><br></pre></td></tr></table></figure>
<p>再下一步，代码会将采样的总步数 <code>num_inference_steps</code> 告知采样调度器 <code>scheduler</code>。这一步是 Diffusers API 的要求。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 6. Prepare timesteps</span></span><br><span class="line">timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, <span class="literal">None</span>, sigmas)</span><br></pre></td></tr></table></figure>
<p>然后，代码会随机生成初始高斯噪声。不同的随机噪声即对应不同的输出视频。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 7. Prepare latent variables</span></span><br><span class="line">num_channels_latents = self.unet.config.in_channels</span><br><span class="line">latents = self.prepare_latents(</span><br><span class="line">    batch_size * num_videos_per_prompt,</span><br><span class="line">    num_frames,</span><br><span class="line">    num_channels_latents,</span><br><span class="line">    height,</span><br><span class="line">    width,</span><br><span class="line">    image_embeddings.dtype,</span><br><span class="line">    device,</span><br><span class="line">    generator,</span><br><span class="line">    latents,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>开始采样前，SVD 对约束图像的强度做了一种很特殊的设定。在看代码之前，我们先回顾一下约束强度的意义。现在的扩散模型普遍使用了 CFG (Classifier-free Guidance) 技术，它允许我们在采样时灵活地调整模型和约束信息的相符程度。这个强度默认取 1.0。我们可以通过增大强度来提升模型的生成效果，比如在 SD 中，这个强度一般取 7.5，这代表模型会更加贴近输入文本。</p>
<p>而 SVD 中，约束信息为图像。开发者对视频的不同帧采用了不同的约束强度：首帧为 <code>min_guidance_scale</code>, 末帧为 <code>max_guidance_scale</code>。强度从首帧到末帧线性增加。默认情况下，约束强度的范围是 [1, 3]。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 8. Prepare guidance scale</span></span><br><span class="line">guidance_scale = torch.linspace(min_guidance_scale, max_guidance_scale, num_frames).unsqueeze(<span class="number">0</span>)</span><br><span class="line">guidance_scale = guidance_scale.to(device, latents.dtype)</span><br><span class="line">guidance_scale = guidance_scale.repeat(batch_size * num_videos_per_prompt, <span class="number">1</span>)</span><br><span class="line">guidance_scale = _append_dims(guidance_scale, latents.ndim)</span><br><span class="line"></span><br><span class="line">self._guidance_scale = guidance_scale</span><br></pre></td></tr></table></figure>
<p>最后，就来到了扩散模型的去噪循环了。根据之前采样调度器返回的采样时刻列表 <code>timesteps</code>，代码从中取出去噪时刻，对纯噪声输入迭代去噪。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">num_warmup_steps = <span class="built_in">len</span>(timesteps) - num_inference_steps * self.scheduler.order</span><br><span class="line">self._num_timesteps = <span class="built_in">len</span>(timesteps)</span><br><span class="line"><span class="keyword">with</span> self.progress_bar(total=num_inference_steps) <span class="keyword">as</span> progress_bar:</span><br><span class="line">    <span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(timesteps):</span><br></pre></td></tr></table></figure>
<p>去噪迭代的一开始，代码会根据是否要执行 CFG 来决定是否要把输入额外复制一份。这是因为做 CFG 时，我们需要把同一个输入过两次去噪模型，一次带约束，一次不带约束。为了简化这个流程，我们可以直接把输入复制一遍，这样只要过一次去噪模型就能得到两个输出了。下一行的 <code>scale_model_input</code> 是 Diffusers 的 API 要求，可以忽略。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># expand the latents if we are doing classifier free guidance</span></span><br><span class="line">latent_model_input = torch.cat([latents] * <span class="number">2</span>) <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">else</span> latents</span><br><span class="line">latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)</span><br></pre></td></tr></table></figure>
<p>接着，加了噪声、过了 VAE 解码器、没有乘系数的约束图像 <code>image_latents</code> 会与普通的噪声拼接到一起，作为模型的直接输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Concatenate image_latents over channels dimension</span></span><br><span class="line">latent_model_input = torch.cat([latent_model_input, image_latents], dim=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>准备好了所有输入后，代码调用 U-Net 对输入噪声图像去噪。输入包括直接输入 <code>latent_model_input</code>，去噪时刻 <code>t</code>，约束图像的 CLIP 嵌入 <code>image_embeddings</code>，三个额外约束的拼接 <code>added_time_ids</code>。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># predict the noise residual</span></span><br><span class="line">noise_pred = self.unet(</span><br><span class="line">    latent_model_input,</span><br><span class="line">    t,</span><br><span class="line">    encoder_hidden_states=image_embeddings,</span><br><span class="line">    added_time_ids=added_time_ids,</span><br><span class="line">    return_dict=<span class="literal">False</span>,</span><br><span class="line">)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>去噪结束后，代码根据公式做 CFG。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># perform guidance</span></span><br><span class="line"><span class="keyword">if</span> self.do_classifier_free_guidance:</span><br><span class="line">    noise_pred_uncond, noise_pred_cond = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_cond - noise_pred_uncond)</span><br></pre></td></tr></table></figure>
<p>有了去噪的输出 <code>noise_pred</code> 还不够，我们还需要用一些比较复杂的公式计算才能得到下一时刻的噪声图像。这一切都被 Diffusers 封装进调度器里了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">latents = self.scheduler.step(noise_pred, t, latents).prev_sample</span><br></pre></td></tr></table></figure>
<p>以上就是一步去噪迭代的主要内容。代码会反复执行去噪迭代。这后面除了下面这行会调用 VAE 解码器将隐空间的视频解码回真实视频外，没有其他重要代码了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frames = self.decode_latents(latents, num_frames, decode_chunk_size)</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我们学习了图生视频模型 SVD 的模型结构和采样代码。整体上看，SVD 相较 SD 在模型上的修改不多，只是在原来的 2D 模块后面加了一些在时序维度上交互信息的卷积块和 Transformer 块。在学习时，我们应该着重关注 SVD 的采样流水线。SVD 使用拼接和交叉注意力两种方式添加了图像约束，并以与时刻编码相加的方式额外输入了三种约束信息。由于视频不同帧对于首帧的依赖情况不同，SVD 还使用了一种随帧号线性增长的 CFG 强度设置方式。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/06/05/20240405-SVD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/06/05/20240405-SVD/" class="post-title-link" itemprop="url">Stable Video Diffusion 结构浅析与论文速览</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-06-05 15:52:11" itemprop="dateCreated datePublished" datetime="2024-06-05T15:52:11+08:00">2024-06-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">扩散模型</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>近期，各个科技公司纷纷展示了自己在视频生成模型上的最新成果。虽然不少模型的演示效果都非常惊艳，但其中可供学术界研究的开源模型却少之又少。Stable Video Diffusion (SVD) 算得上是目前开源视频生成模型中的佼佼者，有认真一学的价值。在这篇文章中，我将面向之前已经熟悉 Stable Diffusion (SD) 的读者，简要解读 SVD 的论文。由于 SVD 的部分结构复用了之前的工作，并没有在论文正文中做详细介绍，所以我还会补充介绍一下 SVD 的模型结构、调度器。后续我还会在其他文章中详细介绍 SVD 的代码实现及使用方法。</p>
<p><img src="/2024/06/05/20240405-SVD/1.jpg" alt></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Stable Video Diffusion 是 Stability 公司于 2023 年 11 月 21 日公布并开源的一套用扩散模型实现的视频生成模型。由于该模型是从 Stability 公司此前发布的著名文生图模型 Stable Diffusion 2.1 微调而成的，因而得名 Stable Video Diffusion。SVD 的技术报告论文与模型同日发布，它对 SVD 的训练过程做了一个详细的分享。由于该论文过分偏向实践，这里我们仅对它的开头及中间模型设计的几处关键部分做解读。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>最近，有许多视频生成模型都是在图像生成模型 SD 的基础上，添加和视频时序相关的模块，并在小规模高质量视频数据集上微调新模型。而 SVD 作者认为，该领域在训练方法及精制数据集的策略上并未达成统一。这篇文章的主要贡献，也正是提出了一套训练方法与精制数据集的方法。具体而言，SVD 的训练由三个阶段组成：文生图预训练、视频预训练、高质量视频微调。同时，SVD 提出了一种系统性的数据精制流程，包含数据的标注与过滤这两部分的策略。论文会分享诸多的实验成果，包括验证精心构建的数据集对生成高质量视频的必要性、探究视频预训练与微调这两步的重要性、展示基础模型如何为图生视频等下游任务提供强大的运动表示、演示模型如何提供多视角三维先验并可以作为微调多视角扩散模型的基础模型在一轮神经网络推理中同时生成多视角的图片。</p>
<blockquote>
<p>「构建」一个数据集在论文中通常用动词 curate 及名词 curation 指代。curate 原指展出画作时，选择、组织和呈现艺术品的过程。而现代将这个词用在数据集上时，则转变为表示精心选择、组织和管理数据的过程。中文中并没有完全对应的翻译，我暂时将这个词翻译为「精制」，以区别于随便收集一些数据来构成一个数据集。</p>
</blockquote>
<p>总结一下，SVD 并没有强调在模型设计或者采样算法上的创新，而主要宣传了该工作在数据集精制及训练策略上的创新。对于大部分普通研究人员来说，由于没有训练大视频模型的需求，该文章的很多内容都价值不大。我们就只是来大致过一遍这篇文章的主要内容。</p>
<h2 id="SVD-模型架构回顾"><a href="#SVD-模型架构回顾" class="headerlink" title="SVD 模型架构回顾"></a>SVD 模型架构回顾</h2><h3 id="Video-LDM-与-SVD"><a href="#Video-LDM-与-SVD" class="headerlink" title="Video-LDM 与 SVD"></a>Video-LDM 与 SVD</h3><p>在阅读正文之前，我们先来回顾一下此前视频生成模型的开发历程，并重点探究 SVD 的模型架构——Video LDM 的具体组成。绝大多数工作在训练一个基于扩散模型的视频生成模型时，都是在预训练的 SD 上加入时序模块，如 3D 卷积，并通过微调把一个图像生成模型转换成视频生成模型。由于 SD 是一种 LDM (Latent Diffusion Model)，所以这些视频模型都可以归类为 Video-LDM。所谓 LDM，就是一种先生成压缩图像，再用解码模型把压缩图像还原成真实图像的模型。而对于视频，Video-LDM 则会先生成边长压缩过的视频，再把压缩视频还原。</p>
<p><img src="/2024/06/05/20240405-SVD/2.jpg" alt></p>
<p>虽然 Video-LDM 严格上来说是一个视频扩散模型的种类，但大家一般会用Video LDM （没有横杠） 来指代 <em>Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</em> 这篇工作。这篇论文已在 CVPR 2023 上发布，两个主要作者正是前一年在 CVPR 上发表 SD 论文的主要作者，也是现在这篇 SVD 论文的主要作者。从署名上来看，似乎两个作者在毕业后就加入了 Stability 公司，并将 Video LDM 拓展成了 SVD。论文中也讲到，SVD 完全复用了 Video LDM 的结构。为了了解 SVD 的模型结构，我们再来回顾一下 Video LDM 的结构。</p>
<p>在 SD 的基础上，Video LDM 做对模型结构了两项改动：在扩散模型的去噪模型 U-Net 中加入时序层、在对图像压缩和解压的 VAE 的解码器中加入时序层。</p>
<h3 id="添加时序层"><a href="#添加时序层" class="headerlink" title="添加时序层"></a>添加时序层</h3><p>Video LDM 在 U-Net 中加入时序层的方法与多数同期方法相同，是在每个原来处理图像的空间层后面加上处理视频的时序层。Video LDM 加入的时序层包括 3D 卷积层与时序注意力层。这些新模块本身不难理解，但我们需要着重关注这些新模块是怎么与原模型兼容的。</p>
<p>要兼容各个模块，其实就是要兼容数据的形状。本来，图像生成模型的 U-Net 的输入形状为 <code>B C H W</code>，分别表示图像数、通道数、高、宽。而视频数据的形状是 <code>B T C H W</code>，即视频数、视频长度、通道数、高、宽。要让视频数据复用之前的图像模型的结构，只要把数据前两维合并，变成 <code>(B T) C H W</code> 即可。这种做法就是把 B 组长度为 T 的视频看成了 $B \cdot T$ 张图片。</p>
<p>对于之前已有的空间层，只要把数据形状变成 <code>(B T) C H W</code> 就没问题了。而 SVD 又新加入了两种时序层：3D 卷积和时序注意力。我们来看一下数据是怎么经过这些新的时序层的。</p>
<p>2D 卷积会对 <code>B C H W</code> 的数据的后两个高、宽维度做卷积。类似地，3D 卷积会对数据最后三个时间、高、宽维度做卷积。所以，过 3D 卷积前，要把形状从 <code>(B T) C H W</code> 变成 <code>B C T H W</code>，做完卷积再还原。</p>
<p>接下来我们来看新的时序注意力。这个地方稍微有点难理解，我们从最简单的注意力开始一点一点学习。最早的 NLP 中的注意力层的输入形状为 <code>B L C</code>，表示数据数、token 长度、token 通道数。<code>L</code> 这一维最为重要，它表示了 <code>L</code>个 token 之间互相交换信息。如果把其拓展成图像空间注意力，则 token 表示图像的每一个像素。在这种注意力层中，<code>L</code> 是 <code>(H W)</code>，<code>B C H W</code> 的数据会被转换成 <code>B (H W) C</code> 输入进注意力层。这表示同一组图像中，每个像素两两之间交换信息。而让视频数据过空间注意力层时，只需要把 <code>B</code> 换成 <code>(B T)</code> 即可，即把数据形状从 <code>(B T) C H W</code> 变为 <code>(B T) (H W) C</code>。这表示同一组、同一帧的图像的每个像素之间，两两交换信息。</p>
<p>在 SVD 新加入的时序注意力层中，token 依旧指代是某一组、某一帧上的一个像素。然而，这次我们不是让同一张图像的像素互相交换信息，而是让不同时刻的像素互相交换信息。因此，这次 token 长度 <code>L</code> 是 <code>T</code>，它表示要像素在时间维度上交换信息。这样，在视频数据过时序层里的自注意力层时，要把数据形状从 <code>(B T) C H W</code> 变成 <code>(B H W) T C</code>。这表示每一组、图像每一处的像素独立处理，它们仅与同一位置不同时间的像素进行信息交换。</p>
<blockquote>
<p>此处如果你没有理解注意力层的形状变换也不要紧，它只是一个实现细节，不影响后面的阅读。如果感兴趣的话，可以回顾一下 Transformer 论文的代码，看一下注意力运算为什么是对 <code>B L C</code> 的数据做操作的。</p>
</blockquote>
<p><img src="/2024/06/05/20240405-SVD/3.jpg" alt></p>
<h3 id="微调-VAE-解码器"><a href="#微调-VAE-解码器" class="headerlink" title="微调 VAE 解码器"></a>微调 VAE 解码器</h3><p>Video LDM 的另一项改动是修改了图像压缩模型 VAE 的解码器。具体来说，方法先在 VAE 的解码器中加入类似的时序层，并在 VAE 配套的 GAN 的判别器里也加入了时序层，随后开始微调。在微调时，编码器不变，仅训练解码器和判别器。</p>
<blockquote>
<p>如果你没听过这套 VAE + GAN 的架构的话，请回顾 Stable Diffusion 论文及与其紧密相关的 VQGAN 论文。</p>
</blockquote>
<p><img src="/2024/06/05/20240405-SVD/4.jpg" alt></p>
<p>以上就是 Video LDM 的模型结构。SVD 对其没有做任何更改，所以也没有在论文里对模型结构做详细介绍。稍有不同的是，Video LDM 仅微调了新加入的模块，而 SVD 在加入新模块后对模型的所有参数都进行了重新训练。</p>
<h2 id="SVD-训练细节"><a href="#SVD-训练细节" class="headerlink" title="SVD 训练细节"></a>SVD 训练细节</h2><p>SVD 分四节介绍了模型训练过程。第一节介绍了数据精制的过程，后三节分别介绍了训练的三个阶段：文生图预训练、视频预训练、高质量视频微调。</p>
<p>获取了一个大规模视频数据集后，SVD 的数据精制主要由预处理和标注这两步组成。由于视频生成模型主要关注生成同一个场景的视频，而不考虑转场的问题，每段训练视频也应该尽量只包含一个场景。为此，预处理主要是在用一些自动化视频剪切工具把收集到的视频进一步切成连续的片段。经切片后，视频片段数变为原来的4倍。标注主要是给视频加上文字描述，以训练一个文生视频的模型。SVD 在添加文字描述时用到了多个标注模型，并使用大语言模型来润色描述。经预处理和标注后，得到的数据集被称作 LVD (Large Video Dataset)。</p>
<p>SVD 数据精制的细节中，比较值得注意的是有关视频帧数的处理。由于开发团队发现视频数据的播放速度快慢不一，于是他们使用光流预测模型来大致估计每段视频的播放速度（以帧率 FPS 表示），并将视频的帧率也作为标注。这样，在训练时，视频的帧率也可以作为一种约束信息。这样的好处是，在我们在生成视频时，可以用该约束来指定视频的播放速度。</p>
<p>之后我们来看 SVD 模型训练的三个阶段。对于第一个文生图预训练阶段，论文没有对模型结构做过多修改，因为他们在这一步使用了之前训练好的 SD 2.1。不过，SVD 在这一步做了一个非常重要的改进：SVD 的噪声调度器从原版的 DDPM 改成了 EDM，采样方法也改成了 EDM 的。</p>
<p>EDM 的论文全称为 <em>Elucidating the Design Space of Diffusion-Based Generative Models</em> 。这篇论文用一种概括性较强的数学模型统一表示了此前各种各样的扩散模型结构，并提出了改进版模型的训练及采样策略。简单来说，EDM 把扩散模型不同时刻的噪声强度表示成 $\sigma_t$，它表示在 $t$ 时刻时，对来自数据集的图像加了标准差为 $\sigma_t$ 的高斯噪声 $\mathcal{N}(\mathbf{0}, \sigma_t^2\mathbf{I})$。一开始，对于没加噪声的图像，$\sigma_0=0$。对于最后一个时刻 $T$ 的图像，$\sigma_T$ 要足够大，使得原图像的内容被完全破坏。</p>
<blockquote>
<p>这里时刻 $0$ 与时刻 $T$ 的定义与 DDPM 论文相同，与 EDM 论文相反。</p>
</blockquote>
<p>有了这样一种统一的表示后，EDM 对扩散模型的训练和采样都做了不少改进。这里我们仅关注其中最重要的一条改进：将离散噪声改进成连续噪声。原来 DDPM 的去噪模型会输入时刻 $t$ 这个参数。EDM 论文指出，$t$ 实际上表示了噪声强度 $\sigma_t$，应该把 $\sigma_t$ 输入进模型。与其用离散的 $t$ 训练一个只认识离散噪声强度的去噪模型，不如训练一个认识连续噪声强度 $\sigma$ 的模型。这样，在采样 $n$ 步时，我们不再是选择离散去噪时刻<code>[timestep[n], timestep[n - 1], ..., 0]</code>，而是可以选择连续噪声强度<code>[sigma[n], sigma[n - 1], ..., 0]</code> 。这样采样更灵活，效果也更好。在第一个训练阶段中，SVD 照搬了 EDM 的这种训练方法，改进了原来的 DDPM。SVD 的默认采样策略也使用了 EDM 的 。我们会在之后的代码实践文章中详细学习这种新采样方法。</p>
<p>对于第二个视频预训练阶段，或许是因为视频模型和图像模型的训练过程毫无区别，论文的介绍重点依然放在了这一阶段的数据处理上，而没有强调训练方法上的创新。简单来看，这一阶段的目标是得到一个过滤后的高质量数据集 LVD-F。为了找到这样一种合适的过滤方案，开发团队先用排列组合生成了大量的过滤方案：对每类指标（文本视频匹配度、美学分数、帧率等）都设置 12.5%, 25% 或 50% 的过滤条件，然后不同指标的条件之间排列组合。之后，开发团队抽取原数据集的一个子集 LVD-10M，用各个方案得到过滤后的视频子集 LVD-10-F。最后，用这样得到的子数据集分别训练模型，比较模型输出的好坏，以决定在完整数据集上使用的最优过滤方案。</p>
<p>在第三个阶段，参考以往多阶段训练图像模型的经验，SVD 也在另一个小而精的视频数据集上进行微调。此数据集的获取方法并没有在论文中给出，大概率是人工手动收集并标注。</p>
<h2 id="SVD-应用"><a href="#SVD-应用" class="headerlink" title="SVD 应用"></a>SVD 应用</h2><p>经上述训练后，开发团队得到了一个低分辨率的基础文生视频模型。在实验部分，SVD 论文除了给出视频生成模型在各大公开数据集上的指标外，还分享了几个基于基础模型的应用。</p>
<h3 id="高分辨率文生视频"><a href="#高分辨率文生视频" class="headerlink" title="高分辨率文生视频"></a>高分辨率文生视频</h3><p>基础文生视频最直接的应用就是高分辨率文生视频。实现的方法很简单，只要准备一个高分辨率的视频数据集，在此数据集上微调原基础模型即可。SVD 高分辨率文生视频模型能生成 576 x 1024 的视频。</p>
<p><img src="/2024/06/05/20240405-SVD/5.jpg" alt></p>
<h3 id="高分辨率图生视频"><a href="#高分辨率图生视频" class="headerlink" title="高分辨率图生视频"></a>高分辨率图生视频</h3><p>除了文生视频外，也可以用基础模型来微调出一个图生视频模型。为了把约束从文本换成图像，开发团队将 U-Net 交叉注意力层的约束从文本嵌入变成了约束图像的图像嵌入，并将约束图像与原 U-Net 的噪声输入在通道维度上拼接在一起。特别地，参考以往 <em>Cascaded diffusion models</em> 论文的经验，约束图像在与噪声输入拼接前，会加上一些噪声。除此之外，由于约束机制的变动，像文生图模型一样将约束强度（CFG scale）设成 7.5 会让 SVD 图生视频模型产生瑕疵。因此，SVD 图生视频模型每一帧的约束强度不同，从第一帧到最后一帧以 1.0 到 3.0 线性增长。</p>
<p><img src="/2024/06/05/20240405-SVD/6.jpg" alt></p>
<p>参考之前 <em>AnimateDiff</em> 工作，SVD 也成功训练了相机运动 LoRA，使得图生视频模型只会生成平移、缩放等某一种特定相机运动的视频。</p>
<p><img src="/2024/06/05/20240405-SVD/7.jpg" alt></p>
<h3 id="视频插帧"><a href="#视频插帧" class="headerlink" title="视频插帧"></a>视频插帧</h3><p>Video LDM 曾提出了一种把基础视频模型变成视频插帧模型方法。该方法以视频片段的首末帧为额外约束，在此新约束下把视频生成模型微调成了预测中间帧的视频预测模型。SVD 以同样方式实现了这一应用。</p>
<h3 id="多视角生成"><a href="#多视角生成" class="headerlink" title="多视角生成"></a>多视角生成</h3><p>多视角生成是计算机视觉中另一类重要的任务：给定 3D 物体某个视角的图片，需要算法生成物体另外视角的图片，从而还原 3D 物体的原貌。而视频生成模型从数据中学到了物体的平滑变换规律，恰好能帮助到多视角生成任务。SVD 论文用不少篇幅介绍了如何在 3D 数据集上生成视频并微调基础模型，从而得到一个能生成环绕物体旋转的视频的模型。</p>
<p><img src="/2024/06/05/20240405-SVD/8.jpg" alt></p>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>Stable Video Diffusion 是在文生图模型 Stable Diffusion 2.1 的基础上添加了和 Video LDM 相同的视频模块微调而成的一套视频生成模型。SVD 的论文主要介绍了其精制数据集的细节，并展示了几个微调基础模型能实现的应用。通过微调基础低分辨率文生视频模型，SVD 可以用于高分辨率文生视频、高分辨率图生视频、视频插帧、多视角生成。</p>
<p>对于没有资源与需求训练大视频模型的多数科研人员而言，没有深究这篇文章细节的必要。并且，由于 SVD <strong>只开源了图生视频模型</strong> （3D模型后来是在 SV3D 论文中正式公布的），这篇文章比较有用的只有和图生视频相关的部分。为了彻底搞懂 SVD 的原理，读这篇论文是不够的，我们还需要通过回顾 Video LDM 论文来了解模型结构，学习 EDM 论文来了解训练及采样机制。</p>
<p>这篇文章主要是面向熟悉 Stable Diffusion 的读者的。如果你缺少某些背景知识，欢迎读我之前介绍 Stable Diffusion 的文章。我没有在本文过多介绍 SVD 的实现细节，欢迎阅读我之后发表的 SVD 代码实践文章。</p>
<p>Stable Diffusion 解读（一）：回顾早期工作</p>
<p>Stable Diffusion 解读（二）：论文精读</p>
<p>Stable Diffusion 解读（三）：原版实现及Diffusers实现源码解读</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/04/04/20240221-TorchEval-FID/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/04/04/20240221-TorchEval-FID/" class="post-title-link" itemprop="url">FID 指标简介与修正 TorchEval FID 计算接口经历分享</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-04-04 14:08:44" itemprop="dateCreated datePublished" datetime="2024-04-04T14:08:44+08:00">2024-04-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/%E9%A1%B9%E7%9B%AE/" itemprop="url" rel="index"><span itemprop="name">项目</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>FID 是一种衡量图像生成模型质量的指标。对于这种常见的指标，一般都能找到好用的 PyTorch 计算接口。然而，当我用 PyTorch 的官方库 TorchEval 来算 FID 指标时，却发现它的结果和多数非官方库无法对齐。我花了不少时间，总算把 TorchEval 的 FID 计算接口修好了。在这篇文章中，我将分享有关 FID 计算的知识以及我调试 TorchEval 的经历，并总结用 pytorch-fid, torch-fidelity, TorchEval 算 FID 的方法。文章最后，我还会分享一个偶然发现的用于反映模型训练时的当前 FID 的方法。</p>
<h2 id="FID-指标简介"><a href="#FID-指标简介" class="headerlink" title="FID 指标简介"></a>FID 指标简介</h2><p>FID 的全称是 Fréchet Inception Distance，它用于衡量两个图像分布之间的差距。如果令一个图像分布是训练集，再用生成模型输出的图像构成另一个分布，那么 FID 指标就表示了生成出来的图片和训练集整体上的相似度，也就间接反映了模型对训练集的拟合程度。FID 名字中的 Fréchet Distance 是一种描述两个样本分布的距离的指标，其定位和 KL 散度一样，但某些情况下会比 KL 散度更加合适。FID 用来算 Fréchet Distance 的样本来自预训练 InceptionV3 模型，它名称中的 Inception 由此而来。</p>
<p>计算 FID 的过程如下：</p>
<ol>
<li>准备两个图片文件夹。一般一个是训练集，另一个存储了生成模型随机生成的图片。 </li>
<li>用预训练的 InceptionV3 模型把每个输入图片转换成一个 2048 维的向量。</li>
<li>计算训练集、生成集上输出向量的均值、协方差。</li>
<li>把均值、协方差代入进下面这个算 Fréchet Distance 的公式，就得到了 FID。</li>
</ol>
<script type="math/tex; mode=display">
FID = ||\mu - \mu_w||^2 + tr(\Sigma + \Sigma_w - 2(\Sigma\Sigma_w)^{\frac{1}{2}})</script><p>实际上，在用 FID 的时候我们完全不用管它的原理，只要知道它的值越小就越好，并且会调用相关接口即可。需注意的是，由于 FID 是一种和集合相关的指标，算 FID 时一定要给足图片。在构建自己模型的输出集合时，至少得有 10000 张图片，推荐生成 50000 张。否则 FID 的结果会不准确。</p>
<h2 id="用-PyTorch-计算-FID-的第三方库"><a href="#用-PyTorch-计算-FID-的第三方库" class="headerlink" title="用 PyTorch 计算 FID 的第三方库"></a>用 PyTorch 计算 FID 的第三方库</h2><p>由于 FID 的计算需要用到一个预训练的 InceptionV3 模型，只有在模型实现完全一致的情况下，FID 的输出结果才是可比的。因此，所有论文汇报的 FID 都基于提出 FID 的作者的官方实现。这份官方实现是用 TensorFlow 写的，后来也有完全等价的 PyTorch 实现。在这一节里，我们就来学习如何用这些基于 PyTorch 的库算 FID。</p>
<p>GitHub 上点赞最多的 PyTorch FID 库是 <code>pytorch-fid</code>。这个库被 FID 官方仓库推荐，且 Stable Diffusion 论文也用了这个库，结果绝对可靠。使用该库的方法很简单，只需要先安装它。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pytorch-fid</span><br></pre></td></tr></table></figure>
<p>再准备好两个用于计算 FID 的文件夹，将文件夹路径传给脚本即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pytorch_fid path/to/dataset1 path/to/dataset2</span><br></pre></td></tr></table></figure>
<p>另一个较为常见的用 PyTorch 算指标的库叫做 <code>torch-fidelity</code>。它用起来和 <code>pytorch-fid</code> 一样简单。一开始，需要用 pip 安装它。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch-fidelity</span><br></pre></td></tr></table></figure>
<p>之后，同样是准备好两个图片文件夹，将文件夹路径传给脚本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fidelity --gpu <span class="number">0</span> --fid --input1 path/to/dataset1 --input2 path/to/dataset2</span><br></pre></td></tr></table></figure>
<p>除了命令行脚本外，<code>torch-fidelity</code> 还提供了 Python API。我们可以在 Python 脚本里加入算 FID 的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch_fidelity</span><br><span class="line"></span><br><span class="line">metrics_dict = torch_fidelity.calculate_metrics(</span><br><span class="line">    input1=<span class="string">&#x27;path1&#x27;</span>,</span><br><span class="line">    input2=<span class="string">&#x27;path2&#x27;</span>,</span><br><span class="line">    fid=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(metrics_dict)</span><br></pre></td></tr></table></figure>
<p><code>torch-fidelity</code> 还提供了其他便捷的功能。比如直接以某个生成模型为 API 的输入 <code>input1</code>，而不是先把图像生成到一个文件夹里，再把文件夹路径传给 <code>input1</code>。同时，<code>torch-fidelity</code> 还支持计算其他指标，我们只需要在命令行脚本或者 API 里多加几个参数就行了。</p>
<h2 id="修正-TorchEval-里的-FID-计算接口"><a href="#修正-TorchEval-里的-FID-计算接口" class="headerlink" title="修正 TorchEval 里的 FID 计算接口"></a>修正 TorchEval 里的 FID 计算接口</h2><p>尽管这些第三方库已经足够好用了，我还是想用 PyTorch 官方近年来推出的指标计算库 TorchEval 来算 FID 指标。原因有两点：</p>
<ol>
<li>我的项目其他地方都是用 PyTorch 官方库实现的 （<code>torch</code> 以及 <code>torchvision</code>），算指标也用官方库会让整体代码风格更加统一。我已经用 TorchEval 算了 PSNR、SSIM，使用体验还可以。</li>
<li>目前，似乎只有 TorchEval 支持在线更新指标的值。也就是说，我可以先生成一部分图片，储存算 FID 需要的中间结果；再生成一部分图片，最终计算此前所有图片与训练集的 FID。这种计算方法的好处我会在文章后面介绍。</li>
</ol>
<p>以前我都是用 pytorch-fid 来算 FID。而当我换成用 TorchEval 后，却发现结果对不齐。于是，漫长的调试之路开始了。</p>
<p>当你有两块时间不一样的手表时，应该怎样确认时间呢？答案是，再找到第三块表。如果三块表中能有两块表时间一样，那么它们的时间就是正确的。一开始，我并不能确定是哪个库写错了，所以我又测试了 torch-fidelity 的结果。实验发现，torch-fidelity 和 pytorch-fid 的结果是一致的。并且我去确认了 Stable Diffusion 的论文，其中用来计算 FID 的库也是 pytorch-fid。看来，是 TorchEval 结果不对。</p>
<p>像 FID 这么常见的指标，大家的中间计算过程肯定都没错，就是一些细微的预处理不太一样。抱着这样的想法，我随意地比对了一下二者的代码，很快就发现 TorchEval 把输入尺寸调成 <code>[299, 299]</code> 了，而 pytorch-fid 没做。可删掉这段代码，程序直接报错了。我深入阅读了 pytorch-fid 的代码，发现它的写法和 TorchEval 不一样，把调整尺寸为 <code>[299, 299]</code> 写到了另一个地方。且通过调查发现，InceptionV3 网络的输入尺寸必须是 <code>[299, 299]</code> 的，是我孤陋寡闻了。唉，看来这次的调试不能太随意啊。</p>
<p>我准备拿出我的真实实力来调 bug。我认真整理了一下算 FID 的步骤，将其主要过程总结为以下几步：</p>
<ol>
<li>用预训练权重初始化 InceptionV3</li>
<li>用 InceptionV3 算两个数据集输出的均值、协方差</li>
<li>根据均值、协方差算距离</li>
</ol>
<p>最后那个算距离的过程不涉及任何神经网络，输出该是什么就是什么。这一块是最不容易出错，且最容易调试的。于是，我决定先排除第三步是否对齐。我把 TorchEval 得到的均值、协方差存下来，用 pytorch-fid 算距离。发现结果和原 TorchEval 的输出差不多。看来算距离这一步没有问题。</p>
<p>接下来，我很自然地想到是不是均值和协方差算错了。我存下了两个库得到的均值、协方差，算了两个库输出之间的误差。结果发现，均值的误差在 0.09 左右，协方差的误差在 0.0002 左右。图像的数据范围在 0~1 之间，0.09 算是一个很大的误差了。可见，第一步和第二步一定存在着无法对齐的部分。</p>
<p>模型输出不同，最容易想到的是模型权重不同。于是，我尝试交换使用二者的模型权重，再比较输出的 FID。两个库的模型定义不太一样，不能直接换模型文件名。我用强大的代码魔改实力强行让新权重分别都跑起来了。结果非常神奇，算上之前的两个 FID，我一共得到了 4 个不一样的 FID 结果。也就是说，A 库 A 模型、B 库 B 模型、A 库 B 模型，B 库 A 模型，结果均不一样。</p>
<p>我被这两个库气得不行，决定认真研究对比二者的模型定义。眼尖的我发现初始化 pytorch-fid 的 InceptionV3 时有一个参数叫 <code>use_fid_inception</code>。作者对此的注释写道：「如果设置为 true，则用 TensorFlow 版 FID 实现；否则，用 torchvision 版 Inception 模型。TensorFlow 的 FID Inception 模型和 torchvision 的在权重和结构上有细微的差别。如果你要计算 FID，强烈推荐将此值设置为 true，以得到和其他论文可比的结果。」总结来说，TorchEval 用的是 torchvision 里的标准 PyTorch 版 InceptionV3，而 pytorch-fid 在标准 PyTorch 版 InceptionV3 外又封装了一层，改了一些模块的定义。为什么要改这些东西呢？这是因为原来的 FID Inception 模型是在 TensorFlow 里实现的，需要改一些结构来将 PyTorch 模型对齐过去。除了模型结构外，二者的权重也有一定差别。大家都是用 TensorFlow 版模型算 FID，一切都应该以 pytorch-fid 的为准。这个 TorchEval 太离谱了，我也懒得认真修改了，直接注释掉 TorchEval 里原 <code>FIDInceptionV3</code> 的定义，然后大笔一挥：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_fid.inception <span class="keyword">import</span> \</span><br><span class="line">    InceptionV3 <span class="keyword">as</span> FIDInceptionV3</span><br></pre></td></tr></table></figure>
<p>按理说，这下权重和模型结构都对齐了。FID 计算的第一、第二步绝对不会有错。而开始的结果表明，FID 计算的第三步也没有错。那么，两个库就应该对齐了。我激动地又测了 TorchEval 的结果，发现结果还是无法对齐！</p>
<p>这不应该啊？难道哪步测错了？人生就是在不断自我怀疑中度过的。而怀疑自我，首先会怀疑最久远的自我。所以，我感觉是最早测第三步的时候有问题。之前我是把 TorchEval 的均值、协方差放到 pytorch-fid 里，结果与 TorchEval 自己的输出一致。这次我反过来，把 pytorch-fid 的均值、协方差放到 TorchEval 的算距离函数里算。这次，我第一次见到 TorchEval 输出了正确的 FID。由此可见，第三步没错。难道是均值和协方差又没对齐了？</p>
<p>自我怀疑开始进一步推进，我开始怀疑第二步输出的均值、协方差还是没有对齐。我再次计算了 pytorch-fid 和 TorchEval 的输出之间的误差，发现误差这次仅有 1e-16，可以认为没有区别。我花了很多时间复习协方差的计算，想找出 TorchEval 里的 bug。可是越学习，越觉得 TorchEval 写得很对。这一回，我找不到错误了。</p>
<p>调试代码，不怕到处有错，而怕「没错却有错」。「没错」，指的是每一步中间步骤都找不到错误；「有错」，指的是最终结果还是错了。没有错误，就得创造错误。我开启了随机乱调模式，希望能触发一个错误。回忆一下，算 FID 要用到两个数据集，一般一个是训练集，一个是模型输出的集合。在 TorchEval 最后一步算距离时，我乱改代码，让一个集合的均值、协方差不变，即来自原 TorchEval 的 Inception 模型的输出；而让另一个的集合的均值、协方差来自 pytorch-fid。理论上说，如果两个库的均值、协方差是对齐的，那么这次输出的 FID 也应该是正确的。欸，这回代码报错了，运行不了。报错说数据精度不统一。原来，TorchEval 的输出精度是 float32，而 pytorch-fid 的输出精度是 float64。之前测试距离计算函数时，数据要么全来自 TorchEval，要么全来自 pytorch-fid，所以没报过这个错。可是这个错只是一个运行上的错误，稍微改改就好了。</p>
<p>我把 pytorch-fid 相关数据的精度统一成了 float32。这下代码跑起来了，可 FID 不对了。调试过程中，如果上一次成功，而这一次失败，则应该想办法把代码退回上一次的，再次测试。因此，我又修改了最后用 TorchEval 计算距离的数据来源，让所有数据都来自 pytorch-fid。可是，修改后，FID 输出没变，还是错的。</p>
<p>为什么两轮测试之前，我全用 pytorch-fid 的输出、TorchEval 的距离计算函数没有错，这次却错了？到底是哪里不同？当测试两份差不多的代码后，一份对了，一份错了，那么错误就可以定位到两份代码的差异处。仔细回顾一下我的调试经历，相信你可以推理出 bug 出自哪了。</p>
<p>没错！我仔细比对了当前代码和我记忆中两轮测试前的代码，仅发现了一处不同——我把 pytorch-fid 的输出数据的精度改成了 float32。把精度改回 float64 就对了。同样，如果把 TorchEval 的输出数据的精度改成 float64，再扔进 TorchEval 的距离计算函数里算，结果也是对的。问题出在 TorchEval 的距离计算函数的数据精度上。</p>
<p>定位到了 bug 的位置，再找出 bug 的原因就很简单了。对比 pytorch-fid 的距离计算函数和 TorchEval 的，可以发现二者描述的计算公式完全相同。然而，pytorch-fid 是用 NumPy 算的，而 TorchEval 是用 PyTorch 算的。算 FID 的距离时，会涉及矩阵特征值等较为复杂的运算，它们对数据精度要求较高。像 NumPy 这种久经考验的库应该会自动把数据变成高精度再计算，而 PyTorch 就没做这么多细腻的处理了。</p>
<p>汇总一下我调试的结论。TorchEval 在权重初始化、模型计算、距离计算这三步中均有错误。前两步没有让 InceptionV3 模型和普遍使用的 TensorFlow 版对齐，最后一步没有考虑输入精度，用了不够稳定的 PyTorch API 来做复杂矩阵运算。要用 TorchEval 算出正确的 FID，需要做以下修改：</p>
<ul>
<li>安装 pytorch-fid 和 TorchEval</li>
<li>打开 torcheval/metrics/image/fid.py</li>
<li>注释掉 <code>FIDInceptionV3</code> 类，在文件开头加上 <code>from pytorch_fid.inception import InceptionV3 as FIDInceptionV3</code></li>
<li>在 <code>FrechetInceptionDistance</code> 类的构造函数中，在定义所有浮点数据时加上 <code>dtype=torch.float64</code></li>
</ul>
<p>这里点名批评 TorchEval。开源的时候吹得天花乱坠，结果根本没人用，这么简单的有关 FID 的 bug 也发现不了。我发了一个修正此 bug 的相关 issue <code>https://github.com/pytorch/torcheval/issues/192</code>，截至目前还是没有官方人员回复。这个库的开发水平实在太逆天了，希望他们能尽快维护好。</p>
<h2 id="在线计算-FID"><a href="#在线计算-FID" class="headerlink" title="在线计算 FID"></a>在线计算 FID</h2><p>前文提到，我用 TorchEval 的原因是它支持在线计算 FID。具体来说，可以建立一个 FID 管理类，之后用 <code>update</code> 方法来不断往某个集合加入新图片，并随时使用 <code>compute</code> 方法算出当前所有图片的 FID。我之前写代码忘了清空旧图片的中间结果时发现了一个相关应用。经我使用下来，这种应用非常有用，我们可以用它高效估计训练时的当前 FID。</p>
<p>回顾一下，要得到准确的 FID 值，一般需要 50000 张图片。而训练图像生成模型时，如果每次验证都要生成这么多图片，则大部分时间都会消耗在验证上了。为了加快 FID 的验证，我发现可以用一种 「全局 FID」来近似表示当前的模型拟合情况。具体来说，我先用训练集的所有图片初始化 FID 的集合 1 的中间结果，再在模型训练中每次验证时随机生成 500 张图片，将其中间结果加到 FID 的集合 2 中，并输出一次当前 FID。这样，随着训练不断推进，算 FID 的图片的数量会逐渐满足 50000 张的要求，但是这些图片并不是来自同一个模型，而是来自不同训练程度的模型。这样得到的 FID 仅能大致反映当前的真实 FID 值，有时偏高、有时偏低。但经我测试发现，这种全局 FID 的相对关系很能反映最终的真实 FID 的相对关系。训练两个不同超参的模型时，如果一个全局 FID 较大，那它最终的 FID 一般也会较大。同时，如果训练一切正常，则全局 FID 会随验证轮数单调递减（因为图片数量变多，且拟合情况不会变差）。如果某一次验证时全局 FID 增加了，则模型也一定在这段时间里变差了。通过这种验证方式，我们能够大致评估模型在训练中的拟合情况。这应该是一种很容易想到的工程技巧，但由于分享自己训练生成模型的经验帖较少，且重要性不足以写进论文，我没有在任何地方看到有人介绍这种技巧。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>FID 是评估图像生成模型的重要指标。通过 pytorch-fid 等库，我们能轻松地用 PyTorch 计算两个图像分布间的 FID。而通过计算输出分布和训练分布之间的 FID，我们就能评估当前模型的拟合情况。</p>
<p>FID 的计算本身是很简单的。所以在介绍 FID 的计算方法之外，我分享了我调试 TorchEval 的漫长过程。这段经历很有意思，我学到了不少调 bug 的新知识。此前我从来没想到过数据精度竟然会大幅影响某个值的结果。这段经历启示我们，做一些复杂运算时，不要用 PyTorch 算，最好拿 NumPy 等更稳定的库来计算。如果你调 bug 的经验不足，这段经历也能给你许多参考。</p>
<p>文章最后我分享了一种算全局 FID 的方法。它可以高效反映生成模型在训练时的拟合情况。该功能很容易实现，感兴趣的话可以自己尝试一下。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/03/09/20240228-DiffMorpher/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/03/09/20240228-DiffMorpher/" class="post-title-link" itemprop="url">CVPR 2024 | DiffMorpher：实现两张图像间的平滑变形</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-03-09 22:58:43" itemprop="dateCreated datePublished" datetime="2024-03-09T22:58:43+08:00">2024-03-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>相信大家都在网上看过这种「笑容逐渐消失」的表情包：一张图片经过经过平滑的变形，逐渐变成另一张截然不同的图片。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/1.png" alt></p>
<p>对此，计算机科学中有一种专门描述此应用的任务——图像变形（image morphing）。给定两张图像，图像变形算法会输出一系列合理的插值图像。当按顺序显示这些插值图像时，它们应该能构成一个描述两张输入图像平滑变换的视频。</p>
<p>图像变形可以广泛运用于创意制作中。比如在做 PPT 时，我们可以在翻页处用图像变形做出炫酷的过渡效果。当然，图像变形也可以用在更严谨的场合。比如在制作游戏中的 2D 人物动画时，可以让画师只画好一系列关键帧，再用图像变形来补足中间帧。可是，这种任务对于中间插值图像的质量有着很高的要求。而传统的基于优化的图像变形算法只能对两张输入图像的像素进行一定程度的变形与混合，难以生成高质量的中间帧。有没有一种更好的图像变形算法呢？</p>
<p>针对这一需求，我们提出了 DiffMorpher —— 一种基于预训练扩散模型 （Stable Diiffusion）的图像变形算法。该研究由 DragGAN 作者潘新钢教授指导，经清华大学、上海人工智能实验室、南洋理工大学 S-Lab 合作完成。目前，该工作已经被 CVPR 2024 接收。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/2.png" alt></p>
<p>我们可以借助 DiffMorpher 实现许多应用。最简单的玩法是输入两张人脸，生成人脸的渐变图。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/yifei.gif" alt></p>
<p>如果输入一系列图片，我们还能制作更长更丰富的渐变图。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/human.gif" alt></p>
<p>而当输入的两张图片很相似时，我们可以用该工具制作出质量尚可的补间动画。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/anime.gif" alt></p>
<p>在这篇文章中，让我们来浏览一下 DiffMorpher 的工作原理，并学习如何使用这一工具。学习 DiffMorpher 的一些技术也能为我们开发其他基于扩散模型的编辑工具提供启发。</p>
<p>项目官网：<a target="_blank" rel="noopener" href="https://kevin-thu.github.io/DiffMorpher_page/">https://kevin-thu.github.io/DiffMorpher_page/</a></p>
<p>代码仓库：<a target="_blank" rel="noopener" href="https://github.com/Kevin-thu/DiffMorpher">https://github.com/Kevin-thu/DiffMorpher</a></p>
<h2 id="隐变量插值"><a href="#隐变量插值" class="headerlink" title="隐变量插值"></a>隐变量插值</h2><p>如前所述，图像变形任务在生成插值图像时不仅需要混合输入图像的内容，还需要补充生成一些内容。用预训练的图像生成模型来完成图像变形是再自然不过的想法了。前几年，已经有一些工作探究了如何用 GAN 来完成图像变形。使用 GAN 做图像变形的方法非常直接：在 GAN 中，每张被生成的图片都由一个高维隐变量决定。可以说，隐变量蕴含了生成一张图像所需的所有信息。那么，只要先使用 GAN 反演（inversion）把输入图片变成隐变量，再对隐变量做插值，就能用其生成两张输入图像的一系列中间过渡图像了。</p>
<blockquote>
<p>对于隐变量，我们一般使用球面插值（slerp）而不是线性插值。</p>
</blockquote>
<p><img src="/2024/03/09/20240228-DiffMorpher/3.png" alt></p>
<p>然而，GAN 生成的图像往往局限于某一类别，泛用性差。因此，用 GAN 做图像变形时，往往得不到高质量的图像插值结果。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/gan.gif" alt></p>
<p>而以 Stable Diffusion（SD）为代表的图像生成扩散模型以能生成各式各样的图像而著称。我们可以在 SD 上也用类似的过程来实现图像插值。具体来说，我们需要对 DDIM 反演得到的纯噪声图像（隐变量）进行插值，并对输入文本的嵌入进行插值，最后根据插值结果生成图像。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/4.png" alt></p>
<p>可是，扩散模型也存在缺陷：扩散模型的隐变量没有 GAN 的那么适合编辑。如下面的动图所示，如果仅使用简单的隐变量插值，会存在着两个问题：1）早期和晚期的中间帧和输入图像非常相近，而中期的中间帧又变化过快，图像的过渡非常突兀；2）中间帧的图像质量较低。这样的结果无法满足实际应用的要求。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/thu_nolora.gif" alt></p>
<h2 id="LoRA-插值"><a href="#LoRA-插值" class="headerlink" title="LoRA 插值"></a>LoRA 插值</h2><p>扩散模型的隐变量不适合编辑，准确来说是其所在隐空间的性质导致的。模型只能处理隐空间中部分区域的隐变量。如果对隐变量稍加修改，让隐变量「跑」到了一个模型处理不了的区域，那模型就生成不了高质量的结果。而在对两个输入隐变量做插值时，插值的隐变量很可能就位于一个模型处理不了的区域。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/6.png" alt></p>
<p>想解决此问题，我们需要参考一些其他的工作。为了提升扩散模型的编辑单张图像的能力，一些往期工作会在单张图片上微调预训练扩散模型（即训练集只由同一张图片构成，让模型在单张图片上过拟合）。这样，无论是调整初始的隐变量还是文本输入，模型总是能够生成一些和该图片很相近的图片。比如在 Imagic 工作中，为了编辑输入图片，算法会先在输入图片上微调扩散模型，再用新的文本描述重新生成一次图片。这样，最终生成的图片既和原图很接近（鸟的外观差不多），又符合新的文本描述（鸟张开了翅膀）。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/5.png" alt></p>
<p>后来，许多工作用 LoRA 代替了全参数微调。LoRA 是一种高效的模型微调技术。在训练 LoRA 时，原来的模型权重不用修改，只需要训练额外引入的少量参数即可。假设原模型的参数为$W$，则 LoRA 参数可以表示为 $\Delta W$，新模型可以表示为$W + \Delta W$，其中 $\Delta W$ 里的参数比 $W$ 要少得多。训练 LoRA 的目的和全参数微调是一样的，只不过 LoRA 相对而言更加高效。</p>
<p>对单张图片训练 LoRA，其实就是在把整个隐空间都变成一个能生成高质量图像的空间。但付出的代价是，模型只能生成和该图片差不多的图片。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/7.png" alt></p>
<p>我们能不能把 LoRA 的这种性质放在隐变量插值上呢？我们可以认为，LoRA 的参数 $\Delta W$ 存储了新的隐空间的一些信息。如果我们不仅对两个输入图片的隐变量做插值，还对分别对两个输入图片训练一个 LoRA，得到$\Delta W_1, \Delta W_2$，再对两个 LoRA 的参数进行插值，得到$\Delta W = \alpha \Delta W_1 + (1-\alpha) \Delta W_2$，就能让中间的插值隐变量也能生成有意义的图片，且该图片会保留两个输入图片的性质。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/8.png" alt></p>
<p>相关实验结果能支撑我们的假设。下图展示了不同 LoRA 配置下，对隐变量做插值得到的结果。第一行和第二行表示分别仅使用左图或右图的 LoRA，第三行表示对 LoRA 也进行插值。可以看出，使用 LoRA 后，所有图片的质量都还不错。固定 LoRA，对隐变量做插值时，图像的风格会随隐变量变化而变化，而图像的语义内容会与训练该 LoRA 的图像相同。而对 LoRA 也进行插值的话，图像的风格、语义都会平滑过渡。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/9.png" alt></p>
<p>下图是前文那个例子的某些中间帧不使用 LoRA 插值和使用 LoRA 插值的结果。可以看出，使用了 LoRA 后，图像质量提升了很多。而通过对 LoRA 的插值，输出图像也会保留两个输入图像的特征。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/10.png" alt></p>
<h2 id="自注意力输入的插值与替换"><a href="#自注意力输入的插值与替换" class="headerlink" title="自注意力输入的插值与替换"></a>自注意力输入的插值与替换</h2><p>使用 LoRA 插值后，中间帧的图像质量得到了大幅提升，可是图像变形不连贯的问题还是没有得到解决。要提升图像变换的连贯性，还需要使用到一项和自注意力相关的技术。</p>
<p>深度学习中常见的注意力运算都可以表示成交叉注意力 $CrossAttn(\mathbf{x}, \mathbf{y})$，它表示数据 $\mathbf{x}$ 从数据 $\mathbf{y}$ 中获取了一次信息。交叉注意力的特例是自注意力 $SelfAttn(\mathbf{x}) = CrossAttn(\mathbf{x}, \mathbf{x})$，它表示数据 $\mathbf{x}$ 自己内部做了一次信息聚合。多数扩散模型的 U-Net 都使用了自注意力层。</p>
<p>由于自注意力本质上是一种交叉注意力，我们可以把另一个图像的自注意力输入替换某图像的自注意力输入。具体来说，我们可以先生成一张参考图像，将自注意力输入$\mathbf{x’}$缓存下来。再开始生成当前图片，对于原来的自注意力计算 $CrossAttn(\mathbf{x}, \mathbf{x})$，我们把第二个 $\mathbf{x}$ 换成 $\mathbf{x’}$，让计算变成 $CrossAttn(\mathbf{x}, \mathbf{x’})$。这样，在生成当前图片时，当前图片会和参考图片更加相似一些。</p>
<p>在扩散模型生成图像时，每个去噪时刻的每个自注意力模块的输入都有自己的意义。在替换输入时，我们必须用参考图像当前时刻当前模块的输入来做替换。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/11.png" alt></p>
<p>这种注意力替换技巧通常用在基于图像扩散模型的视频编辑任务里。一般我们会以输出视频的第一帧为参考图像，让生成后续帧的自注意力模块参考第一帧的信息。这样视频每一帧的风格都会更加一致。</p>
<p>我们可以把视频编辑任务的这种技巧挪用到图像变形任务里。在图像变形中，每一个中间帧要以一定的混合比例参考两个输入图像。那么，我们也可以先分别生成两个输入图像，缓存它们的自注意力输入$\mathbf{x}_0, \mathbf{x}_1$。在生成混合比例为 $\alpha$ 的中间帧时，我们先混合自注意力输入$\mathbf{x’} = \alpha \mathbf{x}_0 + (1-\alpha) \mathbf{x}_1$，再以 $\mathbf{x’}$ 为自注意力的第二个参数，计算 $CrossAttn(\mathbf{x_{\alpha}}, \mathbf{x’})$。</p>
<p>下面是不使用/使用自注意力替换的结果。可以看出，不使用注意力替换时，视频中间某帧会出现突变。而使用了注意力替换后，视频平滑了很多。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/cmp_attn.gif" alt></p>
<p>在实验中我们也发现，直接用 $\mathbf{x’}$ 来替换注意力输入会降低中间帧的质量。为了权衡质量与过渡性，我们会让替换的注意力输入在原输入 $\mathbf{x_{\alpha}}$ 和 $\mathbf{x’}$ 之间做一个混合，即令插入的注意力输入为 $\mathbf{x’} \gets \lambda \mathbf{x’} + (1-\lambda) \mathbf{x_{\alpha}}$。最终实验中我们令 $\lambda=0.6$。</p>
<h2 id="重调度采样"><a href="#重调度采样" class="headerlink" title="重调度采样"></a>重调度采样</h2><p>通过注意力插值，我们解决了中间帧跳变的问题。然而，视频的变换速度还是不够平均。在开始和结束时，视频的变化速度较慢；而在中间时刻，视频的变化又过快。</p>
<p>我们使用了一种重新选择混合比例 $\alpha$ 的重调度策略来解决这一问题。之前，我们在选择混合比例时，是均匀地在 0~1 之间采样。比如要生成 10 段过渡，9个中间帧，我们就可以令混合比例为 $[0, 0.1, 0.2, …, 0.9, 1]$。但是，由于不同比例处插值图像的变化率不同，这样选取混合比例会导致每两帧之间变化量不均匀。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/12.png" alt></p>
<p>上图是一个可能的变化率分布图。图的横坐标是插值的混合比例，或者说视频渐变的时刻，图的纵坐标是图像内容随时间的变化率。每个矩形的面积表示相邻两帧之间的 LPIPS 感知误差。如果等间距采样混合比例的话，由于每个时刻的变化率不同，矩形的面积也不同，图像的变化会时快时慢。</p>
<p>我们希望重新选择一些采样的横坐标，使得相邻帧构成的矩形的面积尽可能一致。通过使用类似于平均颜色分布的直方图均衡化（histogram equalization）算法，我们可以得到重采样的混合比例 $[0, \alpha_1, \alpha_2, …, \alpha_{n-1}, 1]$，达到下面这种相邻帧变化量几乎相同的结果。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/13.png" alt></p>
<p>下面是不使用/使用重采样的结果。可以看出，二者生成的中间图像几乎是一致的，但左边的视频在开头和结尾会停顿一会儿，而右边的视频的内容一直都在均匀地变化。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/res_cmp.gif" alt></p>
<h2 id="在线示例与代码"><a href="#在线示例与代码" class="headerlink" title="在线示例与代码"></a>在线示例与代码</h2><p>看完了该工作的原理，我们来动手使用一下 DiffMorpher。我们先来运行一下在线示例。在线示例可以在 OpenXLab (<a target="_blank" rel="noopener" href="https://openxlab.org.cn/apps/detail/KaiwenZhang/DiffMorpher">https://openxlab.org.cn/apps/detail/KaiwenZhang/DiffMorpher</a> ) 或者 HuggingFace（<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kevin-thu/DiffMorpher">https://huggingface.co/spaces/Kevin-thu/DiffMorpher</a> ）上访问。</p>
<p>使用 WebUI 时，可以直接点击 Run 直接运行示例，或者手动上传两张图片并给定 prompt 再运行。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/14.png" alt></p>
<p>如果你对一些细节感兴趣，也可以手动 clone GitHub 仓库。配置环境的过程也很简单，只需要准备一个有 PyTorch 的运行环境，再安装相关 Pip 包即可。注意，该项目用的 Diffusers 版本较旧，最新的 Diffusers 可能无法成功运行，建议直接照着 <code>requirements.txt</code> 里的版本来。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/Kevin-thu/DiffMorpher.git</span><br><span class="line">cd DiffMorpher</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<p>配置好了环境后，可以直接尝试仓库里自带的示例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python main.py \</span><br><span class="line">  --image_path_0 ./assets/Trump.jpg --image_path_1 ./assets/Biden.jpg \ </span><br><span class="line">  --prompt_0 &quot;A photo of an American man&quot; --prompt_1 &quot;A photo of an American man&quot; \</span><br><span class="line">  --output_path &quot;./results/Trump_Biden&quot; \</span><br><span class="line">  --use_adain --use_reschedule --save_inter</span><br></pre></td></tr></table></figure>
<p>运行后，就能得到下面的结果：</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/biden.gif" alt></p>
<h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><p>图像变形任务的目标是在两个有对应关系的图像之间产生一系列合理的过渡帧。传统基于像素变形与混合的方法无法在中间帧里生成新内容。我们希望用包含丰富图像信息的预训练扩散模型来完成图像变形任务。然而，直接对扩散模型的隐变量插值，会出现中间帧质量低、结果不连贯这两个问题。为了解决这两个问题，我们对扩散模型生成两个输入图像时的诸多属性进行了插值，包括 LoRA 插值、自注意力插值，分别解决了中间帧质量与结果连贯性的问题。另外，加入了重调度采样后，输出视频的连贯性得到了进一步的提升。</p>
<p>受限于图像变形这一任务本身的上限，DiffMorpher 在实际应用中的质量难以比拟专门面向某一任务的方法（比如只做拖拽式编辑，或者只做视频插帧）。这篇工作在科研上的贡献会远大于其在应用上的贡献。方法中一些较为新颖的插值手段或许会帮助到未来的图像编辑工作。</p>
<p>尽管 DiffMorpher 已经算是一个不错的图像变形工具了，该方法并没有从本质上提升扩散模型的可编辑性。相比 GAN 而言，逐渐对扩散模型的隐变量修改难以产生平滑的输出结果。比如在拖拽式编辑任务中，DragGAN 只需要优化 GAN 的隐变量就能产生合理的编辑效果，而扩散模型中的类似工具（如 DragDiffusion, DragonDiffusion）需要更多设计才能达到同样的结果。从本质上提升扩散模型的可编辑性依然是一个值得研究的问题。</p>
<p>出于可读性的考虑，本文没有过多探讨技术细节。如果你对相关技术感兴趣，欢迎阅读我之前的文章：</p>
<p>LoRA 在 Stable Diffusion 中的三种应用：原理讲解与代码示例</p>
<p>Stable Diffusion 中的自注意力替换技术与 Diffusers 实现</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/02/21/20240216-Sora-Comment/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/02/21/20240216-Sora-Comment/" class="post-title-link" itemprop="url">OpenAI 视频模型 Sora 科研贡献速览</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-02-21 16:46:58" itemprop="dateCreated datePublished" datetime="2024-02-21T16:46:58+08:00">2024-02-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>今天，一则重磅消息席卷了 AI 圈：OpenAI 发布了视频模型 Sora，能根据文本生成长达一分钟的高质量 1920x1080 视频，生成能力远超此前只能生成 25 帧 576x1024 图像的顶尖视频生成模型 Stable Video Diffusion。</p>
<p>同时，OpenAI 也公布了一篇非常简短的技术报告。报告仅大致介绍了 Sora 的架构及应用场景，并未对模型的原理详加介绍。让我们来快速浏览一下这份报告，看看科研人员从这份报告中能学到什么。</p>
<p>官网链接：<a target="_blank" rel="noopener" href="https://openai.com/sora">https://openai.com/sora</a></p>
<p>技术报告链接：<a target="_blank" rel="noopener" href="https://openai.com/research/video-generation-models-as-world-simulators">https://openai.com/research/video-generation-models-as-world-simulators</a></p>
<p>这篇文章没怎么贴视频，感兴趣的话可以对照着原报告中的视频阅读。</p>
<h2 id="LDM-与-DiT-的结合"><a href="#LDM-与-DiT-的结合" class="headerlink" title="LDM 与 DiT 的结合"></a>LDM 与 DiT 的结合</h2><p>简单来说，Sora 就是 Latent Diffusion Model (LDM) [1] 加上 Diffusion Transformer (DiT) [2]。我们先简要回顾一下这两种模型架构。</p>
<p>LDM 就是 Stable Diffusion 使用的模型架构。扩散模型的一大问题是计算需求大，难以拟合高分辨率图像。为了解决这一问题，实现 LDM时，会先训练一个几乎能无损压缩图像的自编码器，能把 512x512 的真实图像压缩成 64x64 的压缩图像并还原。接着，再训练一个扩散模型去拟合分辨率更低的压缩图像。这样，仅需少量计算资源就能训练出高分辨率的图像生成模型。</p>
<p>LDM 的扩散模型使用的模型是 U-Net。而根据其他深度学习任务中的经验，相比 U-Net，Transformer 架构的参数可拓展性强，即随着参数量的增加，Transformer 架构的性能提升会更加明显。这也是为什么大模型普遍都采用了 Transformer 架构。从这一动机出发，DiT 应运而生。DiT 在 LDM 的基础上，把 U-Net 换成了 Transformer。</p>
<p>顺带一提，Transformer 本来是用于文本任务的，它只能处理一维的序列数据。为了让 Transformer 处理二维图像，通常会把输入图像先切成边长为 $p$ 的图块，再把每个图块处理成一项数据。也就是说，原来边长为 $I$ 的正方形图片，经图块化后，变成了长度为 $(I/p)^2$ 的一维序列数据。</p>
<p><img src="/2024/02/21/20240216-Sora-Comment/1.png" alt></p>
<p>Transformer 是一种和顺序无关的计算。比如对于输入”abc”和”bca”，Transformer 会输出一模一样的值。为了描述数据的先后顺序，使用 Transformer 时，一般会给数据加一个位置编码。</p>
<p>Sora 是一个视频版的 DiT 模型。让我们看一下 Sora 在 DiT 上做了哪些改进。</p>
<h2 id="时空自编码器"><a href="#时空自编码器" class="headerlink" title="时空自编码器"></a>时空自编码器</h2><p>在此之前，许多工作都尝试把预训练 Stable Diffusion 拓展成视频生成模型。在拓展时，视频的每一帧都会单独输入进 Stable Diffusion 的自编码器，再重新构成一个压缩过的图像序列。而 VideoLDM[3] 工作发现，直接对视频使用之前的图像自编码器，会令输出视频出现闪烁的现象。为此，该工作对自编码器的解码器进行了微调，加入了一些能够处理时间维度的模块，使之能一次性处理整段压缩视频，并输出连贯的真实视频。</p>
<p>Sora 则是从头训练了一套能直接压缩视频的自编码器。相比之前的工作，Sora 的自编码器不仅能在空间上压缩图像，还能在时间上压缩视频长度。这估计是为什么 Sora 能生成长达一分钟的视频。</p>
<blockquote>
<p>报告中提到，Sora 也能处理图像，即长度为1的视频。那么，自编码器怎么在时间上压缩长度为1的视频呢？报告中并没有给出细节。我猜测该自编码器在时间维度做了填充（比如时间被压缩成原来的 1/2，那么就对输入视频填充空数据直至视频长度为偶数），也可能是输入了视频长度这一额外约束信息。</p>
</blockquote>
<h2 id="时空压缩图块"><a href="#时空压缩图块" class="headerlink" title="时空压缩图块"></a>时空压缩图块</h2><p>输入视频经过自编码器后，会被转换成一段空间和时间维度上都变小的压缩视频。这段压缩视频就是 Sora 的 DiT 的拟合对象。在处理视频数据时，DiT 较 U-Net 又有一些优势。</p>
<p>之前基于 U-Net 的去噪模型在处理视频数据时（如 [3])，都需要额外加入一些和时间维度有关的操作，比如时间维度上的卷积、自注意力。而 Sora 的 DiT 是一种完全基于图块的 Transformer 架构。要用 DiT 处理视频数据，不需要这种设计，只要把视频看成一个 3D 物体，再把 3D 物体分割成「图块」，并重组成一维数据输入进 DiT 即可。和原本图像 DiT 一样，假设视频边长为 $I$，时长也为 $I$，要切成边长为 $p$ 的图块，最后会得到 $(I/p)^3$ 个数据。</p>
<blockquote>
<p>报告没有给出视频图块化的细节。</p>
</blockquote>
<h2 id="处理任意分辨率、时长的视频"><a href="#处理任意分辨率、时长的视频" class="headerlink" title="处理任意分辨率、时长的视频"></a>处理任意分辨率、时长的视频</h2><p>报告中反复提及，Sora 在训练和生成时使用的视频可以是任何分辨率（在 1920x1080 以内）、任何长宽比、任何时长的。这意味着视频训练数据不需要做缩放、裁剪等预处理。这些特性是绝大多数其他视频生成模型做不到的，让我们来着重分析一下这一特性的原理。</p>
<p>Sora 的这种性质还是得益于 Transformer 架构。前文提到，Transformer 的计算与输入顺序无关，必须用位置编码来指明每个数据的位置。尽管报告没有提及，我觉得 Sora 的 DiT 使用了类似于 $(x, y, t)$ 的位置编码来表示一个图块的时空位置。这样，不管输入的视频的大小如何，长度如何，只要给每个图块都分配一个位置编码，DiT 就能分清图块间的相对关系了。</p>
<p>相比以前的工作，Sora 的这种设计是十分新颖的。之前基于 U-Net 的 Stable Diffusion 为了保证所有训练数据可以统一被处理，输入图像都会被缩放与裁剪至同一大小。由于训练数据中有被裁剪的图像，模型偶尔也会生成被裁剪的图像。生成训练分辨率以外的图像时，模型的表现有时也会不太好。SDXL [4] 的解决方式是把裁剪的长宽做为额外信息输入进 U-Net。为了生成没有裁剪的图像，只要令输入的裁剪长宽为 0 即可。类似地，SDXL 也把图像分辨率做为额外输入，使得 U-Net 学习不同分辨率、长宽比的图像。相比 SDXL，Sora 的做法就简洁多了。</p>
<p>之前基于 DiT 的模型 （比如华为的 PixArt [5]）似乎都没有利用到 Transformer 可以随意设置位置编码这一性质。DiT 在处理输入图块时，会先把图块变形成一维数据，再从左到右编号，即从从左到右，从上到下地给二维图块组编号。这种位置编码并没有保留图像的二维空间信息，因此，在这种编码下，模型的输入分辨率必须固定。比如对于下面这个$4\times4$的图块组，如果是从左到右、从上到下编码，模型等于是强行学习到了「1号在0号右边、4号在0号下面」这样的位置信息。如果输入的图块形状为 $4 \times 5$，那么图块间的相对关系就完全对不上了。而如果像 Sora 这样以视频图块的 $(x, y, t)$ 来生成位置编码的话，就没有这种问题了，输入视频可以是任何分辨率、任何长度。</p>
<p><img src="/2024/02/21/20240216-Sora-Comment/2.png" alt></p>
<h2 id="Transformer-在视频生成的可拓展性"><a href="#Transformer-在视频生成的可拓展性" class="headerlink" title="Transformer 在视频生成的可拓展性"></a>Transformer 在视频生成的可拓展性</h2><p>前文提过，Transformer 的特点就是可拓展性强，即模型越大，训练越久，效果越好。报告中展示了1倍、4倍、16倍某单位训练时间下的生成结果，可以看出模型确实一直有进步。</p>
<p><img src="/2024/02/21/20240216-Sora-Comment/3.png" alt></p>
<h2 id="语言理解能力"><a href="#语言理解能力" class="headerlink" title="语言理解能力"></a>语言理解能力</h2><p>之前大部分文生图扩散模型都是在人工标注的图片-文字数据集上训练的。后来大家发现，人工标注的图片描述质量较低，纷纷提出了各种提升标注质量的方法。Sora 复用了自家 DALL·E 3 的重标注技术，用一个训练的能生成详细描述的标注器来重新为训练视频生成标注。这种做法不仅解决了视频缺乏标注的问题，且相比人工标注质量更高。Sora 的部分结果展示了其强大了抽象理解能力（如理解人和猫之间的交互），这多半是因为视频标注模型足够强大，视频生成模型学到了视频标注模型的知识。但同样，视频标注模型的相关细节完全没有公开。</p>
<h2 id="其他生成功能"><a href="#其他生成功能" class="headerlink" title="其他生成功能"></a>其他生成功能</h2><ul>
<li>基于已有图像和视频进行生成：除了约束文本外，Sora 还支持在一个视频前后补充内容（如果是在一张图片后面补充内容，就是图生视频）。报告没有给出实现细节，我猜测是直接做了反演（inversion）再把反演得到的隐变量替换到随机初始隐变量中。</li>
<li>视频编辑：报告明确写出，只用简单的 SDEdit （即目前 Stable Diffusion 中的图生图）即可实现视频编辑。</li>
<li>视频内容融合：可能是对两个视频的初始隐变量做了插值。</li>
<li>图像生成：当然，Sora 也可以生成图像。报告表明，Sora 可以生成最大 2048x2048 的图像。</li>
</ul>
<h2 id="涌现出的能力"><a href="#涌现出的能力" class="headerlink" title="涌现出的能力"></a>涌现出的能力</h2><p>通过学习大量数据，Sora 还涌现出一些意想不到的能力。</p>
<ul>
<li>3D 一致性：视频中包含自然的相机视角变换。之前的 Stable Video Diffusion 也有类似发现。</li>
<li>长距离连贯性：AI 生成出来的视频往往有物体在中途突然消失的情况。而 Sora 有时候能克服这一问题。</li>
<li>与世界的交互：比如在描述画画的视频中，画纸上的内容随画笔生成。</li>
<li>模拟数字世界：报告展示了在输入文本有”Minecraft”时，模型能生成非常真实的 Minecraft 游戏视频。这大概只能说明模型的拟合能力太强了，以至于学会了生成 Minecraft 这一种特定风格的视频。 </li>
</ul>
<h2 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h2><p>报告结尾还是给出了一些失败的生成示例，比如玻璃杯在桌子上没有摔碎。这表明模型还不能完全学会某些物理性质。然而，我觉得现阶段 Sora 已经展示了足够强大的学习能力。想模拟现有视频中已经包含的物理现象，只需要增加数据就行了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Sora 是一个惊艳的视频生成模型，它以卓越的生成能力（高分辨率、长时间）与生成质量令一众同期的视频生成模型黯然失色。Sora 的技术报告非常简短，不过我们从中还是可以学到一些东西。从技术贡献上来看，Sora 的创新主要有两点：</p>
<ol>
<li>让 LDM 的自编码器也在视频时间维度上压缩。</li>
<li>使用了一种不限制输入形状的 DiT</li>
</ol>
<p>其中，第二点贡献是非常有启发性的。DiT 能支持不同形状的输入，大概率是因为它以视频的3D位置生成位置编码，打破了一维编码的分辨率限制。后续大家或许会逐渐从 U-Net 转向 DiT 来建模扩散模型的去噪模型。</p>
<p>我认为 Sora 的成功有三个原因。前两个原因对应两项创新。第一，由于在时间维度上也进行了压缩，Sora 最终能生成长达一分钟的视频；第二，使用 DiT 不仅去除了视频空间、时间长度上的限制，还充分利用了 Transformer 本身的可拓展性，使训练一个视频生成大模型变得可能。第三个原因来自于视频标注模型。之前 Stable Diffusion 能够成功，很大程度上是因为有一个能够关联图像与文本的 CLIP 模型，且有足够多的带标注图片。相比图像，视频训练本来就少，带标注的视频就更难获得了。一个能够理解视频内容，生成详细视频标注的标注器，一定是让视频生成模型理解复杂文本描述的关键。除了这几点原因外，剩下的就是砸钱、扩大模型、加数据了。</p>
<p>Sora 显然会对 AIGC 社区产生一定影响。对于 AIGC 爱好者而言，他们或许会多了一些生成创意视频的方法，比如给部分帧让 Sora 来根据文本补全剩余帧。当然，目前 Sora 依然不能取代视频创作者，长视频的质量依然有待观察。对于正在开发相似应用的公司，我觉得他们应该要连夜撤销之前的方案，转换为这套没有分辨率限制的 DiT 的方案。他们的压力应该会很大。对于相关科研人员而言，除了学习这种较为新颖的 DiT 用法外，也没有太多收获了。这份技术报告透露出一股「我绝对不会开源」的意思。没有开源模型，普通的研究者也就什么都做不了。新技术的诞生绝对不可能靠一家公司，一个模型就搞定。像之前的 Stable Diffusion，也是先开源了一个基础模型，科研者和爱好者再补充了各种丰富的应用。我呼吁各大公司尽快训练并开源一个这种不限分辨率的 DiT，这样科研界或许会抛开 U-Net，基于 DiT 开发出新的扩散模型应用。</p>
<h2 id="参考论文"><a href="#参考论文" class="headerlink" title="参考论文"></a>参考论文</h2><ol>
<li>Latent Diffusion Model, Stable Difusion: High-Resolution Image Synthesis with Latent Diffusion Models</li>
<li>DiT: Scalable Diffusion Models with Transformers</li>
<li>VideoLDM: Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</li>
<li>SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis</li>
<li>PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/01/27/20240123-SD-Attn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/27/20240123-SD-Attn/" class="post-title-link" itemprop="url">Stable Diffusion 中的自注意力替换技术与 Diffusers 实现</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-27 16:48:55" itemprop="dateCreated datePublished" datetime="2024-01-27T16:48:55+08:00">2024-01-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在使用预训练 Stable Diffusion (SD) 生成图像时，如果将其 U-Net 的自注意力层在某去噪时刻的输入 K, V 替换成另一幅参考图像的，则输出图像会和参考图像更加相似。许多无需训练的 SD 编辑科研工作都运用了此性质。尤其对于是对于视频编辑任务，如果在生成某一帧时将注意力输入替换成之前帧的，则输出视频会更加连贯。在这篇文章中，我们将快速学习 SD 自注意力替换技术的原理，并在 Diffusers 里实现一个基于此技术的视频编辑流水线。</p>
<h2 id="注意力计算"><a href="#注意力计算" class="headerlink" title="注意力计算"></a>注意力计算</h2><p>我们先来回顾一下 Transformer 论文中提出的注意力机制。所有注意力机制都基于一种叫做放缩点乘注意力（Scaled Dot-Product Attention）的运算：</p>
<script type="math/tex; mode=display">
Attention(Q, K, V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>其中，$Q \in \mathbb{R}^{a \times d_k}, K \in \mathbb{R}^{b \times d_k}, V \in \mathbb{R}^{b \times d_v}$。注意力计算可以理解成先算 $a$ 个长度为 $d_k$ 的向量对 $b$ 个长度为 $d_k$ 的向量的相似度，再以此相似度为权重算 $a$ 个向量对 $b$ 个长度为 $d_v$ 的向量的加权和。</p>
<p>注意力计算是没有可学习参数的。为了加入参数，Transformer 设计了如下所示的注意力层，其中 $W^Q, W^K, W^V, W^O$ 都是参数。</p>
<script type="math/tex; mode=display">
AttnLayer(Q, K, V) = Attention(QW^Q, KW^K, VW^V)W^O</script><p>一般在使用注意力层时，会让$K=V$。这种注意力叫做交叉注意力。交叉注意力可以理解成数据 $A$ 想从数据 $B$ 里提取信息，提取的根据是 $A$ 里每个向量和 $B$ 里每个向量的相似度。 </p>
<script type="math/tex; mode=display">
CrossAttnLayer(A, B) = Attention(AW^Q, BW^K, BW^V)W^O</script><p>交叉注意力的特例是自注意力，此时 $Q=K=V$ 。这表示数据里的向量两两之间交换了一次信息。</p>
<script type="math/tex; mode=display">
SelfAttnLayer(A) = Attention(AW^Q, AW^K, AW^V)W^O</script><h2 id="SD-中的自注意力替换"><a href="#SD-中的自注意力替换" class="headerlink" title="SD 中的自注意力替换"></a>SD 中的自注意力替换</h2><p>SD 的 U-Net 既用到了自注意力，也用到了交叉注意力。自注意力用于图像特征自己内部信息聚合。交叉注意力用于让生成图像对齐文本，其 Q 来自图像特征，K, V 来自文本编码。</p>
<p><img src="/2024/01/27/20240123-SD-Attn/1.png" alt></p>
<p>由于自注意力其实可以看成一种特殊的交叉注意力，我们可以把自注意力的 K, V 替换成来自另一幅参考图像的特征。这样，扩散模型的生成图片会既和原本要生成的图像相似，又和参考图像相似。当然，用来替换的特征必须和原来的特征「格式一致」，不然就生成不了有意义的结果了。</p>
<p><img src="/2024/01/27/20240123-SD-Attn/2.png" alt></p>
<p>什么叫「格式一致」呢？我们知道，扩散模型在采样时有很多步，U-Net 中又有许多自注意力层。每一步时的每一个自注意力层的输入都有自己的「格式」。也就是说，如果你要把某时刻某自注意力层的 K, V 替换，就得先生成参考图像，用生成参考图像过程中此时刻此自注意力层的输入替换，而不能用其他时刻或者其他自注意力层的。</p>
<p><img src="/2024/01/27/20240123-SD-Attn/3.png" alt></p>
<p><img src="/2024/01/27/20240123-SD-Attn/4.png" alt></p>
<p>一般这种编辑技术只会用在自注意力层而不是交叉注意力层上，这是因为 SD 中的交叉注意力是用来关联图像与文字的，另一幅图像的信息无法输入。当然，除了 SD，只要是用到了自注意力模块的扩散模型，都能用此方法编辑，只不过大部分工作都是基于 SD 开发的。</p>
<h2 id="自注意力替换的应用"><a href="#自注意力替换的应用" class="headerlink" title="自注意力替换的应用"></a>自注意力替换的应用</h2><p>自注意力替换最常见的应用是提升 SD 视频编辑的连续性。在此任务中，一般会先正常编辑第一帧，再将后续帧的自注意力的 K, V 替换成第一帧的。这种技术在文献中一般被称为帧间注意力（cross-frame attention）。较早提出此论文的工作是 Text2Video-Zero。</p>
<p>自注意力替换也可以用于提升单幅图像编辑的保真度。一个例子是拖拽单幅图像的 DragonDiffusion。此应用可以拓展到图像插值上，比如 DiffMorpher 在图像插值时对两幅参考图像的自注意力输入等比例插值，再替换掉对应插值图像的自注意力的 K, V。</p>
<h2 id="在-Diffusers-里实现自注意力替换"><a href="#在-Diffusers-里实现自注意力替换" class="headerlink" title="在 Diffusers 里实现自注意力替换"></a>在 Diffusers 里实现自注意力替换</h2><p>Diffusers 的 U-Net 专门提供了用于修改注意力计算的 <code>AttentionProcessor</code> 类。借助相关接口，我们可以方便地修改注意力的计算方法。在这个示例项目中，我们来用 Diffusers 实现一个参考第一帧和上一帧的注意力输入的 SD 视频编辑流水线。相比逐帧生成编辑图片，该流水线的结果会更加平滑一点。项目网址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample/tree/main/ReplaceAttn">https://github.com/SingleZombie/DiffusersExample/tree/main/ReplaceAttn</a> 。</p>
<h3 id="AttentionProcessor"><a href="#AttentionProcessor" class="headerlink" title="AttentionProcessor"></a><code>AttentionProcessor</code></h3><p>在 Diffusers 中，U-Net 的每一个注意力模块都有一个 <code>AttentionProcessor</code> 类的实例。<code>AttentionProcessor</code> 类的 <code>__call__</code> 方法描述了注意力计算的过程。如果我们想修改某些注意力模块的计算，就需要自己定义一个注意力处理类，其 <code>__call__</code> 方法的参数需与 <code>AttentionProcessor</code> 的兼容。之后，我们再调用相关接口把原来的处理类换成我们自己写的处理类。下面我们将先看一下 <code>AttentionProcessor</code> 类的实现细节，再实现我们自己的<br>注意力处理类。</p>
<p><code>AttentionProcessor</code> 类在 <code>diffusers/models/attention_processor.py</code> 文件里。它只有一个 <code>__call__</code> 方法，其主要内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttnProcessor</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        attn: Attention,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states: torch.FloatTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        temb: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        scale: <span class="built_in">float</span> = <span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; torch.Tensor:</span></span><br><span class="line">        residual = hidden_states</span><br><span class="line">        query = attn.to_q(hidden_states, *args)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            encoder_hidden_states = hidden_states</span><br><span class="line"></span><br><span class="line">        key = attn.to_k(encoder_hidden_states, *args)</span><br><span class="line">        value = attn.to_v(encoder_hidden_states, *args)</span><br><span class="line"></span><br><span class="line">        query = attn.head_to_batch_dim(query)</span><br><span class="line">        key = attn.head_to_batch_dim(key)</span><br><span class="line">        value = attn.head_to_batch_dim(value)</span><br><span class="line"></span><br><span class="line">        attention_probs = attn.get_attention_scores(query, key, attention_mask)</span><br><span class="line">        hidden_states = torch.bmm(attention_probs, value)</span><br><span class="line">        hidden_states = attn.batch_to_head_dim(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear proj</span></span><br><span class="line">        hidden_states = attn.to_out[<span class="number">0</span>](hidden_states, *args)</span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        hidden_states = attn.to_out[<span class="number">1</span>](hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attn.residual_connection:</span><br><span class="line">            hidden_states = hidden_states + residual</span><br><span class="line"></span><br><span class="line">        hidden_states = hidden_states / attn.rescale_output_factor</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<p>方法参数中，<code>hidden_states</code> 是 Q， <code>encoder_hidden_states</code> 是 K, V。如果 K, V 没有传入（为 <code>None</code>），则 K, V 会被赋值成 Q。该方法的实现细节和 Tranformer 中的注意力层完全一样，此处就不多加解释了。一般替换注意力的输入时，我们不用改这个方法的实现，只会在需要的时候调用这个方法。</p>
<script type="math/tex; mode=display">
AttnLayer(Q, K, V) = Attention(QW^Q, KW^K, VW^V)W^O</script><p><code>attention_processor.py</code> 文件中还有一个功能类似的类 <code>AttnProcessor2_0</code>，它和 <code>AttentionProcessor</code> 的区别在于它调用了 PyTorch 2.0 起启用的算子 <code>F.scaled_dot_product_attention</code> 代替手动实现的注意力计算。这个算子更加高效，如果你确定 PyTorch 版本至少为 2.0，就可以用 <code>AttnProcessor2_0</code> 代替 <code>AttentionProcessor</code>。</p>
<p>看完了 <code>AttentionProcessor</code> 类后，我们来看该怎么在 U-Net 里将原注意力处理类替换成我们自己写的。U-Net 类的 <code>attn_processors</code> 属性会返回一个词典，它的 key 是每个处理类所在位置，比如 <code>down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor</code>，它的 value 是每个处理类的实例。为了替换处理类，我们需要构建一个格式一样的词典<code>attn_processor_dict</code>，再调用 <code>unet.set_attn_processor(attn_processor_dict)</code> ，取代原来的 <code>attn_processors</code>。假如我们自己实现了处理类 <code>MyAttnProcessor</code>，我们可以编写下面的代码来实现替换：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">attn_processor_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> unet.attn_processors.keys():</span><br><span class="line">    <span class="keyword">if</span> we_want_to_modify(k):</span><br><span class="line">        attn_processor_dict[k] = MyAttnProcessor()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        attn_processor_dict[k] = AttnProcessor()</span><br><span class="line"></span><br><span class="line">unet.set_attn_processor(attn_processor_dict)</span><br></pre></td></tr></table></figure>
<h3 id="实现帧间注意力处理类"><a href="#实现帧间注意力处理类" class="headerlink" title="实现帧间注意力处理类"></a>实现帧间注意力处理类</h3><p>熟悉了 <code>AttentionProcessor</code> 类的相关内容，我们来编写自己的帧间注意力处理类。在处理第一帧时，该类的行为不变。对于之后的每一帧，该类的 K, V 输入会被替换成视频第一帧和上一帧的输入在序列长度维度上的拼接结果，即：</p>
<script type="math/tex; mode=display">
CrossFrameAttn(A, A_1, A_{prev}) = CrossAttnLayer(A, [A_1, A_{prev}])</script><blockquote>
<p>你是否会感到疑惑：为什么 K, V 的序列长度可以修改？别忘了，在注意力计算中，Q, K, V 的形状分别是：$Q \in \mathbb{R}^{a \times d_k}, K \in \mathbb{R}^{b \times d_k}, V \in \mathbb{R}^{b \times d_v}$。注意力计算只要求 K，V 的序列长度 $b$ 相同，并没有要求 Q, K 的序列长度相同。</p>
</blockquote>
<p>现在，注意力计算不再是一个没有状态的计算，它的运算结果取决于第一帧和上一帧的输入。因此，我们在注意力处理类中需要额外维护这两个变量。我们可以按照如下代码编写类的构造函数。除了处理继承外，我们还需要创建两个数据词典来存储不同时间戳下第一帧和上一帧的注意力输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossFrameAttnProcessor</span>(<span class="params">AttnProcessor</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.first_maps = &#123;&#125;</span><br><span class="line">        self.prev_maps = &#123;&#125;</span><br></pre></td></tr></table></figure>
<p>在运行方法中，我们根据 <code>encoder_hidden_states</code> 是否为空来判断该注意力是自注意力还是交叉注意力。我们仅修改自注意力。当该注意力为自注意力时，假设我们知道了当前时刻 <code>t</code>，我们就可以根据 <code>t</code> 获取当前时刻第一帧和前一帧的输入，并将它们拼接起来得到 <code>cross_map</code>。以此 <code>cross_map</code> 为当前注意力的 K, V，我们就实现了帧间注意力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, attn: Attention, hidden_states, encoder_hidden_states=<span class="literal">None</span>, **kwargs</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Is self attention</span></span><br><span class="line">        cross_map = torch.cat(</span><br><span class="line">            (self.first_maps[t], self.prev_maps[t]), dim=<span class="number">1</span>)</span><br><span class="line">        res = <span class="built_in">super</span>().__call__(attn, hidden_states, cross_map, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Is cross attention</span></span><br><span class="line">        res = <span class="built_in">super</span>().__call__(attn, hidden_states, encoder_hidden_states, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<blockquote>
<p>由于 Diffusers 经常修改函数接口，在调用普通的注意力计算接口时，最好原封不动地按照 <code>super().__call__(..., **kwargs)</code> 写，不然这份代码就不能兼容后续版本的 Diffusers。</p>
</blockquote>
<p>上述代码只描述了后续帧的行为。如前所述，我们的注意力计算有两种行为：对于第一帧，我们不修改注意力的计算过程，只缓存其输入；对于之后每一帧，我们替换注意力的输入，同时维护当前「上一帧」的输入。既然注意力在不同情况下有不同行为，我们就应该用一个变量来记录当前状态，让 <code>__call__</code> 能根据此变量决定当前的行为。相关的伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, attn: Attention, hidden_states, encoder_hidden_states=<span class="literal">None</span>, **kwargs</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Is self attention</span></span><br><span class="line">        <span class="keyword">if</span> self.state == FIRST_FRAME:</span><br><span class="line">            res = <span class="built_in">super</span>().__call__(attn, hidden_states, cross_map, **kwargs)</span><br><span class="line">            <span class="comment"># update maps</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cross_map = torch.cat(</span><br><span class="line">                (self.first_maps[t], self.prev_maps[t]), dim=<span class="number">1</span>)</span><br><span class="line">            res = <span class="built_in">super</span>().__call__(attn, hidden_states, cross_map, **kwargs)</span><br><span class="line">            <span class="comment"># update maps</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Is cross attention</span></span><br><span class="line">        res = <span class="built_in">super</span>().__call__(attn, hidden_states, encoder_hidden_states, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>在伪代码中，<code>self.state</code> 表示当前注意力的状态，它的值表明注意力计算是在处理第一帧还是后续帧。在视频编辑流水线中，我们应按照下面的伪代码，先编辑第一帧，再修改注意力状态后编辑后续帧。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">edit(frames[<span class="number">0</span>])</span><br><span class="line">set_attn_state(SUBSEQUENT_FRAMES)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(frames)):</span><br><span class="line">    edit(frames[i])</span><br></pre></td></tr></table></figure>
<p>现在，有一个问题：我们该怎么修改怎么每一个注意力模块的处理器的状态呢？显然，最直接的方式是想办法访问每一个注意力模块的处理器，再直接修改对象的属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">modules = unet.get_attn_moduels</span><br><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> modules:</span><br><span class="line">    <span class="keyword">if</span> we_want_to_modify(module):</span><br><span class="line">        module.processor.state = ...</span><br></pre></td></tr></table></figure>
<p>但是，每次都去遍历所有模块会让代码更加凌乱。同时，这样写也会带来代码维护上的问题：我们每次遍历注意力模块时，都可能要判断该注意力模块是否应该修改。而在用前面讲过的处理类替换方法 <code>unet.set_attn_processor</code> 时，我们也得判断一遍。同一段逻辑重复写在两个地方，非常不利于代码更新。</p>
<p>一种更优雅的实现方式是：我们定义一个状态管理类，所有注意力处理器都从同一个全局状态管理类对象里获取当前的状态信息。想修改每一个处理器的状态，不需要遍历所有对象，只需要改一次全局状态管理类对象就行了。</p>
<p>按照这种实现方式，我们先编写一个状态类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttnState</span>:</span></span><br><span class="line">    STORE = <span class="number">0</span></span><br><span class="line">    LOAD = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.reset()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">state</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.__state = AttnState.STORE</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_load</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.__state = AttnState.LOAD</span><br></pre></td></tr></table></figure>
<p>在注意力处理类中，我们在初始化时保存状态类对象的引用，在运行时根据状态类对象获取当前状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossFrameAttnProcessor</span>(<span class="params">AttnProcessor</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, attn_state: AttnState</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.attn_state = attn_state</span><br><span class="line">        self.first_maps = &#123;&#125;</span><br><span class="line">        self.prev_maps = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, attn: Attention, hidden_states, encoder_hidden_states=<span class="literal">None</span>, **kwargs</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Is self attention</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.attn_state.state == AttnState.STORE:</span><br><span class="line">                res = <span class="built_in">super</span>().__call__(attn, hidden_states, encoder_hidden_states, **kwargs)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cross_map = torch.cat(</span><br><span class="line">                    (self.first_maps[t], self.prev_maps[t]), dim=<span class="number">1</span>)</span><br><span class="line">                res = <span class="built_in">super</span>().__call__(attn, hidden_states, cross_map, **kwargs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Is cross attention</span></span><br><span class="line">            res = <span class="built_in">super</span>().__call__(attn, hidden_states, encoder_hidden_states, **kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>到目前为止，假设已经维护好了之前的输入，我们的注意力处理类能执行两种不同的行为了。现在，我们来实现之前输入的维护。使用之前的注意力输入时，我们其实需要知道当前的时刻 <code>t</code>。当前的时刻也算是另一个状态，最好是也在状态管理类里维护。但为了简化我们的代码，我们可以偷懒让每个处理类自己维护当前时刻。具体做法是：如果知道了去噪迭代的总时刻数，我们就可以令当前时刻从0开始不断自增，直到最大时刻时，再重置为0。加入了时刻处理及之前输入维护的完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttnState</span>:</span></span><br><span class="line">    STORE = <span class="number">0</span></span><br><span class="line">    LOAD = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.reset()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">state</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__state</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timestep</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__timestep</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_timestep</span>(<span class="params">self, t</span>):</span></span><br><span class="line">        self.__timestep = t</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.__state = AttnState.STORE</span><br><span class="line">        self.__timestep = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_load</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.__state = AttnState.LOAD</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossFrameAttnProcessor</span>(<span class="params">AttnProcessor</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, attn_state: AttnState</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.attn_state = attn_state</span><br><span class="line">        self.cur_timestep = <span class="number">0</span></span><br><span class="line">        self.first_maps = &#123;&#125;</span><br><span class="line">        self.prev_maps = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, attn: Attention, hidden_states, encoder_hidden_states=<span class="literal">None</span>, **kwargs</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Is self attention</span></span><br><span class="line"></span><br><span class="line">            tot_timestep = self.attn_state.timestep</span><br><span class="line">            <span class="keyword">if</span> self.attn_state.state == AttnState.STORE:</span><br><span class="line">                self.first_maps[self.cur_timestep] = hidden_states.detach()</span><br><span class="line">                self.prev_maps[self.cur_timestep] = hidden_states.detach()</span><br><span class="line">                res = <span class="built_in">super</span>().__call__(attn, hidden_states, encoder_hidden_states, **kwargs)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tmp = hidden_states.detach()</span><br><span class="line">                cross_map = torch.cat(</span><br><span class="line">                    (self.first_maps[self.cur_timestep], self.prev_maps[self.cur_timestep]), dim=<span class="number">1</span>)</span><br><span class="line">                res = <span class="built_in">super</span>().__call__(attn, hidden_states, cross_map, **kwargs)</span><br><span class="line">                self.prev_maps[self.cur_timestep] = tmp</span><br><span class="line"></span><br><span class="line">            self.cur_timestep += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> self.cur_timestep == tot_timestep:</span><br><span class="line">                self.cur_timestep = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Is cross attention</span></span><br><span class="line">            res = <span class="built_in">super</span>().__call__(attn, hidden_states, encoder_hidden_states, **kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>代码中，<code>tot_timestep</code> 为总时刻数，<code>cur_timestep</code> 为当前时刻。每运算一次，<code>cur_timestep</code> 加一，直至总时刻时再归零。在处理第一帧时，我们把当前时刻的输入同时存入第一帧缓存 <code>first_maps</code> 和上一帧缓存 <code>prev_maps</code> 中。对于后续帧，我们先做替换过输入的注意力计算，再更新上一帧缓存 <code>prev_maps</code>。</p>
<h3 id="视频编辑流水线"><a href="#视频编辑流水线" class="headerlink" title="视频编辑流水线"></a>视频编辑流水线</h3><p>准备好了我们自己写的帧间注意力处理类后，我们来编写一个简单的 Diffusers 视频处理流水线。该流水线基于 ControlNet 与图生图流水线，其主要代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VideoEditingPipeline</span>(<span class="params">StableDiffusionControlNetImg2ImgPipeline</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(...)</span><br><span class="line">        self.attn_state = AttnState()</span><br><span class="line">        attn_processor_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> unet.attn_processors.keys():</span><br><span class="line">            <span class="keyword">if</span> k.startswith(<span class="string">&quot;up&quot;</span>):</span><br><span class="line">                attn_processor_dict[k] = CrossFrameAttnProcessor(</span><br><span class="line">                    self.attn_state)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                attn_processor_dict[k] = AttnProcessor()</span><br><span class="line"></span><br><span class="line">        self.unet.set_attn_processor(attn_processor_dict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, *args, images=<span class="literal">None</span>, control_images=<span class="literal">None</span>,  **kwargs</span>):</span></span><br><span class="line">        self.attn_state.reset()</span><br><span class="line">        self.attn_state.set_timestep(</span><br><span class="line">            <span class="built_in">int</span>(kwargs[<span class="string">&#x27;num_inference_steps&#x27;</span>] * kwargs[<span class="string">&#x27;strength&#x27;</span>]))</span><br><span class="line">        outputs = [<span class="built_in">super</span>().__call__(</span><br><span class="line">            *args, **kwargs, image=images[<span class="number">0</span>], control_image=control_images[<span class="number">0</span>]).images[<span class="number">0</span>]]</span><br><span class="line">        self.attn_state.to_load()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(images)):</span><br><span class="line">            image = images[i]</span><br><span class="line">            control_image = control_images[i]</span><br><span class="line">            outputs.append(<span class="built_in">super</span>().__call__(</span><br><span class="line">                *args, **kwargs, image=image, control_image=control_image).images[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>在构造函数中，我们创建了一个全局注意力状态对象 <code>attn_state</code>。它的引用会传给每一个帧间注意力处理对象。一般修改自注意力模块时，只会修改 U-Net 上采样部分的，而不会动下采样部分和中间部分的。因此，在过滤注意力模块时，我们的判断条件是 <code>k.startswith(&quot;up&quot;)</code>。把新的注意力处理器词典填完后，用 <code>unet.set_attn_processor</code> 更新所有的处理类对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">self.attn_state = AttnState()</span><br><span class="line">attn_processor_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> unet.attn_processors.keys():</span><br><span class="line">    <span class="keyword">if</span> k.startswith(<span class="string">&quot;up&quot;</span>):</span><br><span class="line">        attn_processor_dict[k] = CrossFrameAttnProcessor(</span><br><span class="line">            self.attn_state)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        attn_processor_dict[k] = AttnProcessor()</span><br><span class="line"></span><br><span class="line">self.unet.set_attn_processor(attn_processor_dict)</span><br></pre></td></tr></table></figure>
<p>在 <code>__call__</code> 方法中，我们要基于原图像编辑流水线 <code>super().__call__()</code>，实现我们的视频编辑流水线。在这个过程中，我们的主要任务是维护好注意力管理对象中的状态。一开始，我们要把管理类重置，根据参数设置最大去噪时刻数。经重置后，注意力处理器的状态默认为 <code>STORE</code>，即会保存第一帧的输入。处理完第一帧后，我们运行 <code>attn_state.to_load()</code> 改变注意力处理器的状态，让它们每次做注意力运算时先读第一帧和上一帧的输入，再维护上一帧输入的缓存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, *args, images=<span class="literal">None</span>, control_images=<span class="literal">None</span>,  **kwargs</span>):</span></span><br><span class="line">    self.attn_state.reset()</span><br><span class="line">    self.attn_state.set_timestep(</span><br><span class="line">        <span class="built_in">int</span>(kwargs[<span class="string">&#x27;num_inference_steps&#x27;</span>] * kwargs[<span class="string">&#x27;strength&#x27;</span>]))</span><br><span class="line">    outputs = [<span class="built_in">super</span>().__call__(</span><br><span class="line">        *args, **kwargs, image=images[<span class="number">0</span>], control_image=control_images[<span class="number">0</span>]).images[<span class="number">0</span>]]</span><br><span class="line">    self.attn_state.to_load()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(images)):</span><br><span class="line">        image = images[i]</span><br><span class="line">        control_image = control_images[i]</span><br><span class="line">        outputs.append(<span class="built_in">super</span>().__call__(</span><br><span class="line">            *args, **kwargs, image=image, control_image=control_image).images[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>运行该流水线的示例脚本在项目根目录下的 <code>replace_attn.py</code> 文件中。示例中使用的视频可以在 <code>https://github.com/williamyang1991/Rerender_A_Video/blob/main/videos/pexels-koolshooters-7322716.mp4</code> 下载，下载后应重命名为 <code>woman.mp4</code>。不使用和使用新注意力处理器的输出结果如下：</p>
<p><img src="/2024/01/27/20240123-SD-Attn/output.gif" alt></p>
<p>可以看出，虽然注意力替换不能解决生成视频的闪烁问题，但帧间的一致性提升了不少。将注意力替换技术和其他技术结合起来的话，我们就能得到一个不错的 SD 视频生成工具。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>扩散模型中的自注意力替换是一种常见的提升图片一致性的技术。该技术的实现方法是将扩散模型 U-Net 中自注意力的 K, V 输入替换成另一幅图片的。在这篇文章中，我们学习了一个较为复杂的基于 Diffusers 开发的自注意力替换示例项目，用于提升 SD 视频生成的一致性。在这个过程中，我们学习了和 <code>AttentionProcessor</code> 相关接口函数的使用，并了解了如何基于全局管理类实现一个代码可维护性强的多行为注意力处理类。如果你能看懂这篇文章的示例，那你在开发 Diffusers 的注意力处理类时基本上不会碰到任何难题。</p>
<p>项目网址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample/tree/main/ReplaceAttn">https://github.com/SingleZombie/DiffusersExample/tree/main/ReplaceAttn</a></p>
<p>如果你想进一步学习 Diffusers 中视频编辑流水线的开发，可以参考我给 Diffusers 写的流水线：<a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/tree/main/examples/community#Rerender_A_Video">https://github.com/huggingface/diffusers/tree/main/examples/community#Rerender_A_Video</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">Designer, artist, philosopher, researcher.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">142</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → /atom.xml"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
