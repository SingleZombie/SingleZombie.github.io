<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Designer, artist, philosopher, researcher.">
<meta property="og:type" content="website">
<meta property="og:title" content="周弈帆的博客">
<meta property="og:url" content="https://zhouyifan.net/page/7/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="Designer, artist, philosopher, researcher.">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Zhou Yifan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhouyifan.net/page/7/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="周弈帆的博客" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-switch_lang">

    <a href="https://zhouyifan.net/blog-en" rel="section"><i class="fa fa-language fa-fw"></i>English</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/09/21/DLS-note-14-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/21/DLS-note-14-2/" class="post-title-link" itemprop="url">你的第一个PyTorch RNN模型——字母级语言模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-21 17:46:13" itemprop="dateCreated datePublished" datetime="2022-09-21T17:46:13+08:00">2022-09-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/%E9%A1%B9%E7%9B%AE/" itemprop="url" rel="index"><span itemprop="name">项目</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>想要入门一项新技术，最快的方法就是写一个”Hello World”程序。入门CNN，大家一般会写一个简单的图片分类项目。可是，RNN的入门项目就比较少见了。自然语言处理任务要求的数据量都比较大，不是那么好设计一个入门项目。</p>
<p>在这篇文章中，我将展示一个入门级的RNN项目——字母级语言模型。这个项目的逻辑比较简单，要求的数据量不大，几分钟就可以训练完，非常适合新手入门。</p>
<p>这个项目使用的框架是PyTorch。首先，我会抛弃PyTorch的高级组件，仅使用线性层、自动求导机制来从头实现一个简单的RNN。之后，我还会用PyTorch的高级组件搭一个更通用的RNN。相信通过阅读这篇教程，大家不仅能够理解RNN的底层原理，还能够学到PyTorch中RNN组件的用法，能够自己搭建出各种各样的NLP任务模型。</p>
<h2 id="知识背景"><a href="#知识背景" class="headerlink" title="知识背景"></a>知识背景</h2><p>详细的知识介绍可以参考我的上篇文章：<a href>循环神经网络基础</a>。</p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>RNN 适用于处理序列数据。令$x^{&lt; i &gt;}$是序列的第$i$个元素，那么$x^{&lt; 1 &gt;} x^{&lt; 2 &gt;}…x^{&lt; T_x &gt;}$就是一个长度为$T_x$的序列。NLP中最常见的元素是单词，对应的序列是句子。</p>
<p>RNN使用同一个神经网络处理序列中的每一个元素。同时，为了表示序列的先后关系，RNN还有表示记忆的隐变量$a$，它记录了前几个元素的信息。对第$t$个元素的运算如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{< t >} &= g_1(W_{ax} x^{< t >} + W_{aa} a^{< t - 1 >} + b_a) \\
\hat{y}^{< t >} &=g_2(W_{ya} a^{< t >} + b_y)
\end{aligned}</script><p>其中，$W, b$都是线性运算的参数，$g$是激活函数。隐藏层的激活函数一般用tanh，输出层的激活函数根据实际情况选用。另外，$a$得有一个初始值$a^{&lt; 1 &gt;}$，一般令$a^{&lt; 1 &gt;}=\vec0$。</p>
<h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>语言模型是NLP中的一个基础任务。假设我们以单词为基本元素，句子为序列，那么一个语言模型能够输出某句话的出现概率。通过比较不同句子的出现概率，我们能够开发出很多应用。比如在英语里，同音的”apple and pear”比”apple and pair”的出现概率高（更可能是一个合理的句子）。当一个语音识别软件听到这句话时，可以分别写下这两句发音相近的句子，再根据语言模型断定这句话应该写成前者。</p>
<p>规范地说，对于序列$x^{&lt; 1 &gt;}…x^{&lt; T_x &gt;}$，语言模型的输出是$P(x^{&lt; 1 &gt;},…, x^{&lt; T_x &gt;})$。这个式子也可以写成$P(x^{&lt; 1 &gt;}) \times P(x^{&lt; 2 &gt;} |x^{&lt; 1 &gt;}) \times  P(x^{&lt; 3 &gt;} |x^{&lt; 1 &gt;}, x^{&lt; 2 &gt;}) … \times  P(x^{&lt; T_x &gt;} |x^{&lt; 1 &gt;}, x^{&lt; 2 &gt;}, …, x^{&lt; T_x-1 &gt;})$，即一句话的出现概率，等于第一个单词出现在句首的概率，乘上第二个单词在第一个单词之后的概率，乘上第三个单词再第一、二个单词之后的概率，这样一直乘下去。</p>
<p>单词级的语言模型需要的数据量比较大，在这个项目中，我们将搭建一个字母级语言模型。即我们以字母为基本元素，单词为序列。语言模型会输出每个单词的概率。比如我们输入”apple”和”appll”，语言模型会告诉我们单词”apple”的概率更高，这个单词更可能是一个正确的英文单词。</p>
<h3 id="RNN-语言模型"><a href="#RNN-语言模型" class="headerlink" title="RNN 语言模型"></a>RNN 语言模型</h3><p>为了计算语言模型的概率，我们可以用RNN分别输出$P(x^{&lt; 1 &gt;})$, $P(x^{&lt; 2 &gt;} |x^{&lt; 1 &gt;})$, …，最后把这些概率乘起来。</p>
<p>$P(x^{&lt; t &gt;} |x^{&lt; 1 &gt;}, x^{&lt; 2 &gt;}, …, x^{&lt; t-1 &gt;})$这个式子，说白了就是给定前$t-1$个字母，猜一猜第$t$个字母最可能是哪个。比如给定了前四个字母”appl”，第五个单词构成”apply”, “apple”的概率比较大，构成”appll”, “appla”的概率较小。</p>
<p>为了让神经网络学会这个概率，我们可以令RNN的输入为<code>&lt;sos&gt; x_1, x_2, ..., x_T</code>，RNN的标签为<code>x_1, x_2, ..., x_T, &lt;eos&gt;</code>（<code>&lt;sos&gt;</code>和<code>&lt;eos&gt;</code>是句子开始和结束的特殊字符，实际实现中可以都用空格<code>&#39; &#39;</code>表示。<code>&lt;sos&gt;</code>也可以粗暴地用全零向量表示），即输入和标签都是同一个单词，只是它们的位置差了一格。模型每次要输出一个softmax的多分类概率，预测给定前几个字母时下一个字母的概率。这样，这个模型就能学习到前面那个条件概率了。</p>
<p><img src="/2022/09/21/DLS-note-14-2/DLS-note-14/5.jpg" alt></p>
<h2 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h2><p>项目地址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/BasicRNN。">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/BasicRNN。</a></p>
<h3 id="数据集获取"><a href="#数据集获取" class="headerlink" title="数据集获取"></a>数据集获取</h3><p>为了搭建字母级语言模型，我们只需要随便找一个有很多单词的数据集。这里我选择了斯坦福大学的<a target="_blank" rel="noopener" href="https://ai.stanford.edu/~amaas/data/sentiment/">大型电影数据集</a>，它收录了IMDb上的电影评论，正面评论和负面评论各25000条。这个数据集本来是用于情感分类这一比较简单的NLP任务，拿来搭字母级语言模型肯定是没问题的。</p>
<p>这个数据集的文件结构大致如下：</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">├─test</span><br><span class="line">│  ├─neg</span><br><span class="line">│  │  ├ 0_2.txt</span><br><span class="line">│  │  ├ 1_3.txt</span><br><span class="line">│  │  └ ...</span><br><span class="line">│  └─pos</span><br><span class="line">├─train</span><br><span class="line">│   ├─neg</span><br><span class="line">│   └─pos</span><br><span class="line">└─imdb.vocab</span><br></pre></td></tr></table></figure>
<p>其中，<code>imdb.vocab</code>记录了数据集中的所有单词，一行一个。<code>test</code>和<code>train</code>是测试集和训练集，它们的<code>neg</code>和<code>pos</code>子文件夹分别记录了负面评论和正面评论。每一条评论都是一句话，存在一个txt文件里。</p>
<p>训练字母级语言模型时，直接拿词汇表来训练也行，从评论中截取一个个单词也行。我已经写好了这些读取数据集的代码，在<code>dldemos/BasicRNN/read_imdb.py</code>文件中。</p>
<p>在读取单词时，我们只需要26个字母和空格这一共27个字符。其他的字符全可以过滤掉。为了方便，我使用了正则表达式过滤出这27个字符：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words = re.sub(<span class="string">u&#x27;([^\u0020\u0061-\u007a])&#x27;</span>, <span class="string">&#x27;&#x27;</span>, words)</span><br></pre></td></tr></table></figure>
<p>这样，一个读取词汇表文件的函数就长这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_imdb_vocab</span>(<span class="params"><span class="built_in">dir</span>=<span class="string">&#x27;data/aclImdb&#x27;</span></span>):</span></span><br><span class="line">    fn = os.path.join(<span class="built_in">dir</span>, <span class="string">&#x27;imdb.vocab&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fn, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        word = f.read().decode(<span class="string">&#x27;utf-8&#x27;</span>).replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        words = re.sub(<span class="string">u&#x27;([^\u0020\u0061-\u007a])&#x27;</span>, <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">                       word.lower()).split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        filtered_words = [w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> <span class="built_in">len</span>(w) &gt; <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> filtered_words</span><br></pre></td></tr></table></figure>
<p>我写好了读取词汇表的函数<code>read_imdb_vocab</code>和<code>read_imdb_words</code>，它们都会返回一个单词的列表。我还写了一个读数据集整个句子的函数<code>read_imdb</code>。它们的用法和输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    vocab = read_imdb_vocab()</span><br><span class="line">    <span class="built_in">print</span>(vocab[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(vocab[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    lines = read_imdb()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Length of the file:&#x27;</span>, <span class="built_in">len</span>(lines))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;lines[0]:&#x27;</span>, lines[<span class="number">0</span>])</span><br><span class="line">    words = read_imdb_words(n_files=<span class="number">100</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Length of the words:&#x27;</span>, <span class="built_in">len</span>(words))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        <span class="built_in">print</span>(words[i])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">the</span><br><span class="line">and</span><br><span class="line">Length of the file: 12500</span><br><span class="line">lines[0]: Bromwell High is a cartoon ...</span><br><span class="line">Length of the words: 23425</span><br><span class="line">bromwell</span><br><span class="line">high</span><br><span class="line">is</span><br><span class="line">a</span><br><span class="line">cartoon</span><br></pre></td></tr></table></figure>
<h3 id="数据集读取"><a href="#数据集读取" class="headerlink" title="数据集读取"></a>数据集读取</h3><p>RNN的输入不是字母，而是表示字母的向量。最简单的字母表示方式是one-hot编码，每一个字母用一个某一维度为1，其他维度为0的向量表示。比如我有a, b, c三个字母，它们的one-hot编码分别为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a: [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">b: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">c: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>现在，我们只有单词数组。我们要把每个单词转换成这种one-hot编码的形式。</p>
<p>在转换之前，我准备了一些常量（<code>dldemos/BasicRNN/constant.py</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">EMBEDDING_LENGTH = <span class="number">27</span></span><br><span class="line">LETTER_MAP = &#123;<span class="string">&#x27; &#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line">ENCODING_MAP = [<span class="string">&#x27; &#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">26</span>):</span><br><span class="line">    LETTER_MAP[<span class="built_in">chr</span>(<span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>) + i)] = i + <span class="number">1</span></span><br><span class="line">    ENCODING_MAP.append(<span class="built_in">chr</span>(<span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>) + i))</span><br><span class="line">LETTER_LIST = <span class="built_in">list</span>(LETTER_MAP.keys())</span><br></pre></td></tr></table></figure>
<p>我们一共有27个字符，0号字符是空格，剩余字母按照字母表顺序排列。<code>LETTER_MAP</code>和<code>ENCODING_MAP</code>分别完成了字母到数字的正向和反向映射。<code>LETTER_LIST</code>是所有字母的列表。</p>
<p>PyTorch提供了用于管理数据集读取的Dataset类。Dataset一般只会存储获取数据的信息，而非原始数据，比如存储图片路径。而每次读取时，Dataset才会去实际读取数据。在这个项目里，我们用Dataset存储原始的单词数组，实际读取时，每次返回一个one-hot编码的向量。</p>
<p>使用Dataset时，要继承这个类，实现<code>__len__</code>和<code>__getitem__</code>方法。前者表示获取数据集的长度，后者表示获取某项数据。我们的单词数据集<code>WordDataset</code>应该这样写（<code>dldemos/BasicRNN/main.py</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dldemos.BasicRNN.constant <span class="keyword">import</span> EMBEDDING_LENGTH, LETTER_MAP</span><br><span class="line"><span class="keyword">from</span> dldemos.BasicRNN.read_imdb <span class="keyword">import</span> read_imdb_vocab, read_imdb_words</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, words, max_length, is_onehot=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="function">        <span class="title">super</span>().<span class="title">__init__</span>()</span></span><br><span class="line"><span class="function">        <span class="title">n_words</span> = <span class="title">len</span>(<span class="params">words</span>)</span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">words</span> = <span class="title">words</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">n_words</span> = <span class="title">n_words</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">max_length</span> = <span class="title">max_length</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">is_onehot</span> = <span class="title">is_onehot</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.n_words</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;return the (one-hot) encoding vector of a word&quot;&quot;&quot;</span></span><br><span class="line">        word = self.words[index] + <span class="string">&#x27; &#x27;</span></span><br><span class="line">        word_length = <span class="built_in">len</span>(word)</span><br><span class="line">        <span class="keyword">if</span> self.is_onehot:</span><br><span class="line">            tensor = torch.zeros(self.max_length, EMBEDDING_LENGTH)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_length):</span><br><span class="line">                <span class="keyword">if</span> i &lt; word_length:</span><br><span class="line">                    tensor[i][LETTER_MAP[word[i]]] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    tensor[i][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tensor = torch.zeros(self.max_length, dtype=torch.long)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(word_length):</span><br><span class="line">                tensor[i] = LETTER_MAP[word[i]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tensor</span><br></pre></td></tr></table></figure>
<p>构造数据集的参数是<code>words, max_length, is_onehot</code>。<code>words</code>是单词数组。<code>max_length</code>表示单词的最大长度。在训练时，我们一般要传入一个batch的单词。可是，单词有长有短，我们不可能拿一个动态长度的数组去表示单词。为了统一地表达所有单词，我们可以记录单词的最大长度，把较短的单词填充空字符，直到最大长度。<code>is_onehot</code>表示是不是one-hot编码，我设计的这个数据集既能输出用数字标签表示的单词（比如abc表示成<code>[0, 1, 2]</code>），也能输出one-hoe编码表示的单词（比如abc表示成<code>[[1, 0, 0], [0, 1, 0], [0, 0, 1]]</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, words, max_length, is_onehot=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="function">    <span class="title">super</span>().<span class="title">__init__</span>()</span></span><br><span class="line"><span class="function">    <span class="title">n_words</span> = <span class="title">len</span>(<span class="params">words</span>)</span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">words</span> = <span class="title">words</span></span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">n_words</span> = <span class="title">n_words</span></span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">max_length</span> = <span class="title">max_length</span></span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">is_onehot</span> = <span class="title">is_onehot</span></span></span><br></pre></td></tr></table></figure>
<p>在获取数据集时，我们要根据是不是one-hot编码，先准备好一个全是0的输出张量。如果存的是one-hot编码，张量的形状是<code>[MAX_LENGTH, EMBEDDING_LENGTH]</code>，第一维是单词的最大长度，第二维是one-hot编码的长度。而如果是普通的标签数组，则张量的形状是<code>[MAX_LENGTH]</code>。准备好张量后，遍历每一个位置，令one-hot编码的对应位为1，或者填入数字标签。</p>
<p>另外，我们用空格表示单词的结束。要在处理前给单词加一个<code>&#39; &#39;</code>，保证哪怕最长的单词也会至少有一个空格。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;return the (one-hot) encoding vector of a word&quot;&quot;&quot;</span></span><br><span class="line">    word = self.words[index] + <span class="string">&#x27; &#x27;</span></span><br><span class="line">    word_length = <span class="built_in">len</span>(word)</span><br><span class="line">    <span class="keyword">if</span> self.is_onehot:</span><br><span class="line">        tensor = torch.zeros(self.max_length, EMBEDDING_LENGTH)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_length):</span><br><span class="line">            <span class="keyword">if</span> i &lt; word_length:</span><br><span class="line">                tensor[i][LETTER_MAP[word[i]]] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tensor[i][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tensor = torch.zeros(self.max_length, dtype=torch.long)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(word_length):</span><br><span class="line">            tensor[i] = LETTER_MAP[word[i]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br></pre></td></tr></table></figure>
<p>注意！短单词的填充部分应该全是空字符。千万不要忘记给空字符的one-hot编码赋值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_length):</span><br><span class="line">    <span class="keyword">if</span> i &lt; word_length:</span><br><span class="line">        tensor[i][LETTER_MAP[word[i]]] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tensor[i][<span class="number">0</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>有了数据集类，结合之前写好的数据集获取函数，可以搭建一个DataLoader。DataLoader是PyTorch提供的数据读取类，它可以方便地从Dataset的子类里读取一个batch的数据，或者以更高级的方式取数据（比如随机取数据）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataloader_and_max_length</span>(<span class="params">limit_length=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  is_onehot=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  is_vocab=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> is_vocab:</span><br><span class="line">        words = read_imdb_vocab()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        words = read_imdb_words(n_files=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">    max_length = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        max_length = <span class="built_in">max</span>(max_length, <span class="built_in">len</span>(word))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> limit_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> max_length &gt; limit_length:</span><br><span class="line">        words = [w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> <span class="built_in">len</span>(w) &lt;= limit_length]</span><br><span class="line">        max_length = limit_length</span><br><span class="line"></span><br><span class="line">    <span class="comment"># for &lt;EOS&gt; (space)</span></span><br><span class="line">    max_length += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    dataset = WordDataset(words, max_length, is_onehot)</span><br><span class="line">    <span class="keyword">return</span> DataLoader(dataset, batch_size=<span class="number">256</span>), max_length</span><br></pre></td></tr></table></figure>
<p>这个函数会先调用之前编写的数据读取API获取单词数组。之后，函数会计算最长的单词长度。这里，我用<code>limit_length</code>过滤了过长的单词。据实验，这个数据集里最长的单词竟然有60多个字母，把短单词填充至60需要浪费大量的计算资源。因此，我设置了<code>limit_length</code>这个参数，不去读取那些过长的单词。</p>
<p>计算完最大长度后，别忘了+1，保证每个单词后面都有一个表示单词结束的空格。</p>
<p>最后，用<code>DataLoader(dataset, batch_size=256)</code>就可以得到一个DataLoader。<code>batch_size</code>就是指定batch size的参数。我们这个神经网络很小，输入数据也很小，可以选一个很大的batch size加速训练。</p>
<h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><p>模型的初始化函数和训练函数定义如下（<code>dldemos/BasicRNN/models.py</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dldemos.BasicRNN.constant <span class="keyword">import</span> EMBEDDING_LENGTH, LETTER_LIST, LETTER_MAP</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN1</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_units=<span class="number">32</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden_units = hidden_units</span><br><span class="line">        self.linear_a = nn.Linear(hidden_units + EMBEDDING_LENGTH,</span><br><span class="line">                                  hidden_units)</span><br><span class="line">        self.linear_y = nn.Linear(hidden_units, EMBEDDING_LENGTH)</span><br><span class="line">        self.tanh = nn.Tanh()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">        <span class="comment"># word shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">        batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># word shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">        word = torch.transpose(word, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">        output = torch.empty_like(word)</span><br><span class="line"></span><br><span class="line">        a = torch.zeros(batch, self.hidden_units, device=word.device)</span><br><span class="line">        x = torch.zeros(batch, EMBEDDING_LENGTH, device=word.device)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Tx):</span><br><span class="line">            next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">            hat_y = self.linear_y(next_a)</span><br><span class="line">            output[i] = hat_y</span><br><span class="line">            x = word[i]</span><br><span class="line">            a = next_a</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">        <span class="keyword">return</span> torch.transpose(output, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>我们来一点一点地看看这个模型是怎么搭起来的。</p>
<p>回忆一下RNN的公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{< t >} &= g_1(W_{ax} x^{< t >} + W_{aa} a^{< t - 1 >} + b_a) \\
\hat{y}^{< t >} &=g_2(W_{ya} a^{< t >} + b_y)
\end{aligned}</script><p>我们可以把第一行公式里的两个$W$合并一下，$x, a$拼接一下。这样，只需要两个线性层就可以描述RNN了。</p>
<p>因此，在初始化函数中，我们定义两个线性层<code>linear_a</code>，<code>linear_y</code>。另外，<code>hidden_units</code>表示隐藏层<code>linear_a</code>的神经元数目。<code>tanh</code>就是普通的tanh函数，它用作第一层的激活函数。</p>
<p><code>linear_a</code>就是公式的第一行，由于我们把输入<code>x</code>和状态<code>a</code>拼接起来了，这一层的输入通道数是<code>hidden_units + EMBEDDING_LENGTH</code>，输出通道数是<code>hidden_units</code>。第二层<code>linear_y</code>表示公式的第二行。我们希望RNN能预测下一个字母的出现概率，因此这一层的输出通道数是<code>EMBEDDING_LENGTH=27</code>，即字符个数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_units=<span class="number">32</span></span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.hidden_units = hidden_units</span><br><span class="line">    self.linear_a = nn.Linear(hidden_units + EMBEDDING_LENGTH,</span><br><span class="line">                              hidden_units)</span><br><span class="line">    self.linear_y = nn.Linear(hidden_units, EMBEDDING_LENGTH)</span><br><span class="line">    self.tanh = nn.Tanh()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在描述模型运行的<code>forward</code>函数中，我们先准备好输出张量，再初始化好隐变量<code>a</code>和第一轮的输入<code>x</code>。根据公式，循环遍历序列的每一个字母，用<code>a, x</code>计算<code>hat_y</code>，并维护每一轮的<code>a, x</code>。最后，所有<code>hat_y</code>拼接成的<code>output</code>就是返回结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">    <span class="comment"># word shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">    batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># word shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">    word = torch.transpose(word, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># output shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">    output = torch.empty_like(word)</span><br><span class="line"></span><br><span class="line">    a = torch.zeros(batch, self.hidden_units, device=word.device)</span><br><span class="line">    x = torch.zeros(batch, EMBEDDING_LENGTH, device=word.device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Tx):</span><br><span class="line">        next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">        hat_y = self.linear_y(next_a)</span><br><span class="line">        output[i] = hat_y</span><br><span class="line">        x = word[i]</span><br><span class="line">        a = next_a</span><br><span class="line"></span><br><span class="line">    <span class="comment"># output shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">    <span class="keyword">return</span> torch.transpose(output, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><br>我们来看一看这个函数的细节。一开始，输入张量<code>word</code>的形状是<code>[batch数，最大单词长度，字符数=27]</code>。我们提前获取好形状信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># word shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<p>我们循环遍历的其实是单词长度那一维。为了方便理解代码，我们可以把单词长度那一维转置成第一维。根据这个新的形状，我们准备好同形状的输出张量。输出张量<code>output[i][j]</code>表示第j个batch的序列的第i个元素的27个字符预测结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># word shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">word = torch.transpose(word, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">output = torch.empty_like(word)</span><br></pre></td></tr></table></figure>
<p>按照前文知识准备的描述，第一轮的输入是空字符，期待的输出是句子里的第一个字母；第二轮的输入的第一个字母，期待的输出是第二个字母……。因此，我们要把输入<code>x</code>初始化为空。理论上<code>x</code>应该是一个空字符，其one-hot编码是<code>[1, 0, 0, ...]</code>，但这里我们拿一个全0的向量表示句首也是可行的。除了初始化<code>x</code>，还要初始化一个全零隐变量<code>a</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.zeros(batch, self.hidden_units, device=word.device)</span><br><span class="line">x = torch.zeros(batch, EMBEDDING_LENGTH, device=word.device)</span><br></pre></td></tr></table></figure>
<p>之后，按照顺序遍历每一个元素，计算<code>y_hat</code>并维护<code>a, x</code>。最后输出结果前别忘了把转置过的维度复原回去。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Tx):</span><br><span class="line">    next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">    hat_y = self.linear_y(next_a)</span><br><span class="line">    output[i] = hat_y</span><br><span class="line">    x = word[i]</span><br><span class="line">    a = next_a</span><br><span class="line"></span><br><span class="line"><span class="comment"># output shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line"><span class="keyword">return</span> torch.transpose(output, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>从逻辑上讲，模型应该输出softmax的结果。但是，PyTorch的<code>CrossEntropyLoss</code>已经包含了softmax的计算，我们不用在模型里加softmax。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>main函数中完整的训练代码如下（<code>dldemos/BasicRNN/models.py</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_rnn1</span>():</span></span><br><span class="line">    device = <span class="string">&#x27;cuda:0&#x27;</span></span><br><span class="line">    dataloader, max_length = get_dataloader_and_max_length(<span class="number">19</span>)</span><br><span class="line"></span><br><span class="line">    model = RNN1().to(device)</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">    citerion = torch.nn.CrossEntropyLoss()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line"></span><br><span class="line">        loss_sum = <span class="number">0</span></span><br><span class="line">        dataset_len = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> dataloader:</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            hat_y = model(y)</span><br><span class="line">            n, Tx, _ = hat_y.shape</span><br><span class="line">            hat_y = torch.reshape(hat_y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">            y = torch.reshape(y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">            label_y = torch.argmax(y, <span class="number">1</span>)</span><br><span class="line">            loss = citerion(hat_y, label_y)</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            loss_sum += loss</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>. loss: <span class="subst">&#123;loss_sum / dataset_len&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    torch.save(model.state_dict(), <span class="string">&#x27;dldemos/BasicRNN/rnn1.pth&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>首先，调用之前编写的函数，准备好<code>dataloader</code>和<code>model</code>。同时，准备好优化器<code>optimizer</code>和损失函数<code>citerion</code>。优化器和损失函数按照常见配置选择即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&#x27;cuda:0&#x27;</span></span><br><span class="line">dataloader, max_length = get_dataloader_and_max_length(<span class="number">19</span>)</span><br><span class="line"></span><br><span class="line">model = RNN1().to(device)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">citerion = torch.nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<p>这个语言模型一下就能训练完，做5个epoch就差不多了。每一代训练中，<br>先调用模型求出<code>hat_y</code>，再调用损失函数<code>citerion</code>，最后反向传播并优化模型参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    loss_sum = <span class="number">0</span></span><br><span class="line">    dataset_len = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> dataloader:</span><br><span class="line">        y = y.to(device)</span><br><span class="line">        hat_y = model(y)</span><br><span class="line"></span><br><span class="line">        n, Tx, _ = hat_y.shape</span><br><span class="line">        hat_y = torch.reshape(hat_y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">        y = torch.reshape(y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">        label_y = torch.argmax(y, <span class="number">1</span>)</span><br><span class="line">        loss = citerion(hat_y, label_y)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        loss_sum += loss</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>. loss: <span class="subst">&#123;loss_sum / dataset_len&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<p>算损失函数前需要预处理一下数据，交叉熵损失函数默认<code>hat_y</code>的维度是<code>[batch数，类型数]</code>，<code>label_y</code>是一个一维整形标签数组。而模型的输出形状是<code>[batch数，最大单词长度，字符数]</code>，我们要把前两个维度融合在一起。另外，我们并没有提前准备好<code>label_y</code>，需要调用<code>argmax</code>把one-hot编码转换回标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hat_y = model(y)</span><br><span class="line">n, Tx, _ = hat_y.shape</span><br><span class="line">hat_y = torch.reshape(hat_y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">y = torch.reshape(y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">label_y = torch.argmax(y, <span class="number">1</span>)</span><br><span class="line">loss = citerion(hat_y, label_y)</span><br></pre></td></tr></table></figure>
<p>之后就是调用PyTorch的自动求导功能。注意，为了防止RNN梯度过大，我们可以用<code>clip_grad_norm_</code>截取梯度的最大值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p>我还顺带输出了每一代的loss。当然这里我偷了个懒，这个loss并不能表示每一个样本的平均loss。不过，我们能通过这个loss得到模型的训练进度，这就够了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>. loss: <span class="subst">&#123;loss_sum / dataset_len&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>我们可以手动为字母级语言模型写几个测试用例，看看每一个单词的概率是否和期望的一样。我的测试单词列表是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">test_words = [</span><br><span class="line">    <span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;appll&#x27;</span>, <span class="string">&#x27;appla&#x27;</span>, <span class="string">&#x27;apply&#x27;</span>, <span class="string">&#x27;bear&#x27;</span>, <span class="string">&#x27;beer&#x27;</span>, <span class="string">&#x27;berr&#x27;</span>, <span class="string">&#x27;beee&#x27;</span>, <span class="string">&#x27;car&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;cae&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;cac&#x27;</span>, <span class="string">&#x27;caq&#x27;</span>, <span class="string">&#x27;query&#x27;</span>, <span class="string">&#x27;queee&#x27;</span>, <span class="string">&#x27;queue&#x27;</span>, <span class="string">&#x27;queen&#x27;</span>, <span class="string">&#x27;quest&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;quess&#x27;</span>, <span class="string">&#x27;quees&#x27;</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>我构筑了几组长度一样，但是最后几个字母不太一样的“单词”。通过观察这些词的概率，我们能够验证语言模型的正确性。理论上来说，英文里的正确单词的概率会更高。</p>
<p>我们的模型只能输出每一个单词的softmax前结果。我们还要为模型另写一个求语言模型概率的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">language_model</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">    <span class="comment"># word shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">    batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># word shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">    <span class="comment"># word_label shape: [max_word_length, batch]</span></span><br><span class="line">    word = torch.transpose(word, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    word_label = torch.argmax(word, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># output shape: [batch]</span></span><br><span class="line">    output = torch.ones(batch, device=word.device)</span><br><span class="line"></span><br><span class="line">    a = torch.zeros(batch, self.hidden_units, device=word.device)</span><br><span class="line">    x = torch.zeros(batch, EMBEDDING_LENGTH, device=word.device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Tx):</span><br><span class="line">        next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">        tmp = self.linear_y(next_a)</span><br><span class="line">        hat_y = F.softmax(tmp, <span class="number">1</span>)</span><br><span class="line">        probs = hat_y[torch.arange(batch), word_label[i]]</span><br><span class="line">        output *= probs</span><br><span class="line">        x = word[i]</span><br><span class="line">        a = next_a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>这个函数和<code>forward</code>大致相同。只不过，这次我们的输出<code>output</code>要表示每一个单词的概率。因此，它被初始化成一个全1的向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output shape: [batch]</span></span><br><span class="line">output = torch.ones(batch, device=word.device)</span><br></pre></td></tr></table></figure>
<p>每轮算完最后一层的输出后，我们手动调用<code>F.softmax</code>得到softmax的概率值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tmp = self.linear_y(next_a)</span><br><span class="line">hat_y = F.softmax(tmp, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>接下来，我们要根据每一个batch当前位置的单词，去<code>hat_y</code>里取出需要的概率。比如第2个batch当前的字母是<code>b</code>，我们就要取出<code>hat_y[2][2]</code>。</p>
<p>第<code>i</code>轮所有batch的字母可以用<code>word_label[i]</code>表示。根据这个信息，我们可以用<code>probs = hat_y[torch.arange(batch), word_label[i]]</code>神奇地从<code>hat_y</code>里取出每一个batch里<code>word_label[i]</code>处的概率。把这个概率乘到<code>output</code>上就算完成了一轮计算。</p>
<p>有了语言模型函数，我们可以测试一下开始那些单词的概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_language_model</span>(<span class="params">model, is_onehot=<span class="literal">True</span>, device=<span class="string">&#x27;cuda:0&#x27;</span></span>):</span></span><br><span class="line">    _, max_length = get_dataloader_and_max_length(<span class="number">19</span>)</span><br><span class="line">    <span class="keyword">if</span> is_onehot:</span><br><span class="line">        test_word = words_to_onehot(test_words, max_length)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        test_word = words_to_label_array(test_words, max_length)</span><br><span class="line">    test_word = test_word.to(device)</span><br><span class="line">    probs = model.language_model(test_word)</span><br><span class="line">    <span class="keyword">for</span> word, prob <span class="keyword">in</span> <span class="built_in">zip</span>(test_words, probs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;word&#125;</span>: <span class="subst">&#123;prob&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">apple: <span class="number">9.39846032110836e-08</span></span><br><span class="line">appll: <span class="number">6.516307937687316e-09</span></span><br><span class="line">appla: <span class="number">3.6599331565412285e-08</span></span><br><span class="line">apply: <span class="number">1.2422759709806996e-07</span></span><br><span class="line">bear: <span class="number">1.6009346381906653e-06</span></span><br><span class="line">beer: <span class="number">1.6936465954131563e-06</span></span><br><span class="line">berr: <span class="number">9.99331746243115e-07</span></span><br><span class="line">beee: <span class="number">1.5601625591443735e-07</span></span><br><span class="line">car: <span class="number">1.8536804418545216e-05</span></span><br><span class="line">cae: <span class="number">1.8946409454656532e-06</span></span><br><span class="line">cat: <span class="number">1.875695670605637e-05</span></span><br><span class="line">cac: <span class="number">6.04180786467623e-06</span></span><br><span class="line">caq: <span class="number">3.6483314147517376e-08</span></span><br><span class="line">query: <span class="number">1.6811516161396867e-06</span></span><br><span class="line">queee: <span class="number">5.9459132728534314e-08</span></span><br><span class="line">queue: <span class="number">9.488831942405795e-09</span></span><br><span class="line">queen: <span class="number">5.990783051856852e-07</span></span><br><span class="line">quest: <span class="number">2.737341446845676e-06</span></span><br><span class="line">quess: <span class="number">4.7091912165342364e-06</span></span><br><span class="line">quees: <span class="number">1.3468336419464322e-06</span></span><br></pre></td></tr></table></figure>
<p>通过观察每一组用例，我们能发现，<code>apple, apply, bear, beer</code>这些正确的单词的概率确实会高一些。这个语言模型训练得不错。有趣的是，<code>caq</code>这种英语里几乎不存在的字母组合的概率也偏低。当然，语言模型对难一点的单词的判断就不太准了。<code>queen</code>和<code>queue</code>的出现概率就比较低。</p>
<h3 id="采样单词"><a href="#采样单词" class="headerlink" title="采样单词"></a>采样单词</h3><p>语言模型有一个很好玩的应用：我们可以根据语言模型输出的概率分布，采样出下一个单词；输入这一个单词，再采样下一个单词。这样一直采样，直到采样出空格为止。使用这种采样算法，我们能够让模型自动生成单词，甚至是英文里不存在，却看上去很像那么回事的单词。</p>
<p>我们要为模型编写一个新的方法<code>sample_word</code>，采样出一个最大长度为10的单词。这段代码的运行逻辑和之前的<code>forward</code>也很相似。只不过，这一次我们没有输入张量，每一轮的<code>x</code>要靠采样获得。<code>np.random.choice(LETTER_LIST, p=np_prob)</code>可以根据概率分布<code>np_prob</code>对列表<code>LETTER_LIST</code>进行采样。根据每一轮采样出的单词<code>letter</code>，我们重新生成一个<code>x</code>，给one-hot编码的对应位置赋值1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_word</span>(<span class="params">self, device=<span class="string">&#x27;cuda:0&#x27;</span></span>):</span></span><br><span class="line">    batch = <span class="number">1</span></span><br><span class="line">    output = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    a = torch.zeros(batch, self.hidden_units, device=device)</span><br><span class="line">    x = torch.zeros(batch, EMBEDDING_LENGTH, device=device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">        tmp = self.linear_y(next_a)</span><br><span class="line">        hat_y = F.softmax(tmp, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        np_prob = hat_y[<span class="number">0</span>].detach().cpu().numpy()</span><br><span class="line">        letter = np.random.choice(LETTER_LIST, p=np_prob)</span><br><span class="line">        output += letter</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> letter == <span class="string">&#x27; &#x27;</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        x = torch.zeros(batch, EMBEDDING_LENGTH, device=device)</span><br><span class="line">        x[<span class="number">0</span>][LETTER_MAP[letter]] = <span class="number">1</span></span><br><span class="line">        a = next_a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>使用这个方法，我们可以写一个采样20次的脚本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">model</span>):</span></span><br><span class="line">    words = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        word = model.sample_word()</span><br><span class="line">        words.append(word)</span><br><span class="line">    <span class="built_in">print</span>(*words)</span><br></pre></td></tr></table></figure>
<p>我的一次输出是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">movine  oaceniefke xumedfasss tinkly  cerawedaus meblilesen douteni  ttingieftu sinsceered inelid  tniblicl  krouthyych mochonalos memp  dendusmani sttywima  dosmmek  dring  diummitt  pormoxthin</span><br></pre></td></tr></table></figure>
<p>采样出来的单词几乎不会是英文里的正确单词。不过，这些单词的词缀很符合英文的造词规则，非常好玩。如果为采样函数加一些限制，比如只考虑概率前3的字母，那么算法应该能够采样出更正确的单词。</p>
<h2 id="PyTorch里的RNN函数"><a href="#PyTorch里的RNN函数" class="headerlink" title="PyTorch里的RNN函数"></a>PyTorch里的RNN函数</h2><p>刚刚我们手动编写了RNN的实现细节。实际上，PyTorch提供了更高级的函数，我们能够更加轻松地实现RNN。其他部分的代码逻辑都不怎么要改，我这里只展示一下要改动的关键部分。</p>
<blockquote>
<p>写这份代码时我参考了 <a target="_blank" rel="noopener" href="https://github.com/floydhub/word-language-model">https://github.com/floydhub/word-language-model</a></p>
</blockquote>
<p>新的模型的主要函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN2</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_units=<span class="number">64</span>, embeding_dim=<span class="number">64</span>, dropout_rate=<span class="number">0.2</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.drop = nn.Dropout(dropout_rate)</span><br><span class="line">        self.encoder = nn.Embedding(EMBEDDING_LENGTH, embeding_dim)</span><br><span class="line">        self.rnn = nn.GRU(embeding_dim, hidden_units, <span class="number">1</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.decoder = torch.nn.Linear(hidden_units, EMBEDDING_LENGTH)</span><br><span class="line">        self.hidden_units = hidden_units</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        nn.init.uniform_(self.encoder.weight, -initrange, initrange)</span><br><span class="line">        nn.init.zeros_(self.decoder.bias)</span><br><span class="line">        nn.init.uniform_(self.decoder.weight, -initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">        <span class="comment"># word shape: [batch, max_word_length]</span></span><br><span class="line">        batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">        first_letter = word.new_zeros(batch, <span class="number">1</span>)</span><br><span class="line">        x = torch.cat((first_letter, word[:, <span class="number">0</span>:-<span class="number">1</span>]), <span class="number">1</span>)</span><br><span class="line">        hidden = torch.zeros(<span class="number">1</span>, batch, self.hidden_units, device=word.device)</span><br><span class="line">        emb = self.drop(self.encoder(x))</span><br><span class="line">        output, hidden = self.rnn(emb, hidden)</span><br><span class="line">        y = self.decoder(output.reshape(batch * Tx, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> y.reshape(batch, Tx, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>初始化时，我们用<code>nn.Embedding</code>表示单词的向量。词嵌入（Embedding）是《深度学习专项-RNN》第二门课的内容，我会在下一篇笔记里介绍。这里我们把<code>nn.Embedding</code>看成一种代替one-hot编码的更高级的向量就行。这些向量和线性层参数<code>W</code>一样，是可以被梯度下降优化的。这样，不仅是RNN可以优化，每一个单词的表示方法也可以被优化。</p>
<p>注意，使用<code>nn.Embedding</code>后，输入的张量不再是one-hot编码，而是数字标签。代码中的其他地方也要跟着修改。</p>
<p><code>nn.GRU</code>可以创建GRU。其第一个参数是输入的维度，第二个参数是隐变量<code>a</code>的维度，第三个参数是层数，这里我们只构建1层RNN，<code>batch_first</code>表示输入张量的格式是<code>[batch, Tx, embedding_length]</code>还是<code>[Tx,  batch, embedding_length]</code>。</p>
<p>貌似RNN中常用的正则化是靠dropout实现的。我们要提前准备好dropout层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_units=<span class="number">64</span>, embeding_dim=<span class="number">64</span>, dropout_rate=<span class="number">0.2</span></span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.drop = nn.Dropout(dropout_rate)</span><br><span class="line">    self.encoder = nn.Embedding(EMBEDDING_LENGTH, embeding_dim)</span><br><span class="line">    self.rnn = nn.GRU(embeding_dim, hidden_units, <span class="number">1</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">    self.decoder = torch.nn.Linear(hidden_units, EMBEDDING_LENGTH)</span><br><span class="line">    self.hidden_units = hidden_units</span><br><span class="line"></span><br><span class="line">    self.init_weights()</span><br></pre></td></tr></table></figure>
<p>准备好了计算层后，在forward里只要依次调用它们就行了。其底层原理和我们之前手写的是一样的。其中，<code>self.rnn(emb, hidden)</code>这个调用完成了循环遍历的计算。</p>
<p>由于输入格式改了，令第一轮输入为空字符的操作也更繁琐了一点。我们要先定义一个空字符张量，再把它和输入的第一至倒数第二个元素拼接起来，作为网络的真正输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">    <span class="comment"># word shape: [batch, max_word_length]</span></span><br><span class="line">    batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    first_letter = word.new_zeros(batch, <span class="number">1</span>)</span><br><span class="line">    x = torch.cat((first_letter, word[:, <span class="number">0</span>:-<span class="number">1</span>]), <span class="number">1</span>)</span><br><span class="line">    hidden = torch.zeros(<span class="number">1</span>, batch, self.hidden_units, device=word.device)</span><br><span class="line">    emb = self.drop(self.encoder(x))</span><br><span class="line">    output, hidden = self.rnn(emb, hidden)</span><br><span class="line">    y = self.decoder(output.reshape(batch * Tx, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y.reshape(batch, Tx, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>PyTorch里的RNN用起来非常灵活。我们不仅能够给它一个序列，一次输出序列的所有结果，还可以只输入一个元素，得到一轮的结果。在采样单词时，我们不得不每次输入一个元素。有关采样的逻辑如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_word</span>(<span class="params">self, device=<span class="string">&#x27;cuda:0&#x27;</span></span>):</span></span><br><span class="line">    batch = <span class="number">1</span></span><br><span class="line">    output = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    hidden = torch.zeros(<span class="number">1</span>, batch, self.hidden_units, device=device)</span><br><span class="line">    x = torch.zeros(batch, <span class="number">1</span>, device=device, dtype=torch.long)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        emb = self.drop(self.encoder(x))</span><br><span class="line">        rnn_output, hidden = self.rnn(emb, hidden)</span><br><span class="line">        hat_y = self.decoder(rnn_output)</span><br><span class="line">        hat_y = F.softmax(hat_y, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        np_prob = hat_y[<span class="number">0</span>, <span class="number">0</span>].detach().cpu().numpy()</span><br><span class="line">        letter = np.random.choice(LETTER_LIST, p=np_prob)</span><br><span class="line">        output += letter</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> letter == <span class="string">&#x27; &#x27;</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        x = torch.zeros(batch, <span class="number">1</span>, device=device, dtype=torch.long)</span><br><span class="line">        x[<span class="number">0</span>] = LETTER_MAP[letter]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>以上就是PyTorch高级RNN组件的使用方法。在使用PyTorch的RNN时，主要的改变就是输入从one-hot向量变成了标签，数据预处理会更加方便一些。另外，PyTorch的RNN会自动完成循环，可以给它输入任意长度的序列。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我展示了一个字母级语言模型项目。这个项目涉及到的编程知识有：</p>
<ul>
<li>one-hot编码的处理</li>
<li>RNN的底层实现</li>
<li>如何用RNN对语言模型任务建模</li>
<li>如何用RNN求出语言模型的概率</li>
<li>如何对语言模型采样</li>
<li>PyTorch的RNN组件</li>
</ul>
<p>这篇文章只展示了部分关键代码。想阅读整个项目完整的代码，可以访问该项目的<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/BasicRNN">GitHub链接</a>。</p>
<p>如果大家正在学深度学习，强烈建议大家从头写一遍这个项目。编写代码能够学到很多细节，加深对RNN的理解。</p>
<p>在编写这个项目时，我总结了项目中几个比较有挑战性的部分。大家阅读代码或自己动手时可以格外注意这些部分。第一个比较难的部分是和batch有关的计算。RNN本身必须得顺序处理序列，效率较低，同时处理一个batch的数据是一个很重要的加速手段。我们的代码都得尽量符合向量化编程要求，一次处理一个batch。</p>
<p>另外，相比一般的数据，序列数据多了一个时间维度（或者说序列维度），在向量化计算中考虑这个维度是很耗费脑力的。我们可以在代码中加入对中间变量形状的注释。在使用PyTorch或者其他框架时，要注意是batch维度在前面，还是时间维度在前面。注意初始化RNN的<code>batch_first</code>这个参数。还有，一个张量到底是one-hot编码，还是embedding，还是标签序列，这个也要想清楚来。</p>
<blockquote>
<p>PyTorch里的<code>CrossEntropyLoss</code>自带了softmax操作，千万不能和softmax混用！我之前写了这个bug，调了很久才调出来，真是气死人了。</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/08/15/20220813-SRGAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/15/20220813-SRGAN/" class="post-title-link" itemprop="url">图像超分经典网络 SRGAN 解析 ~ 如何把 GAN 运用在其他视觉任务上</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-15 17:10:46" itemprop="dateCreated datePublished" datetime="2022-08-15T17:10:46+08:00">2022-08-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>生成对抗网络(GAN)是一类非常有趣的神经网络。借助GAN，计算机能够生成逼真的图片。近年来有许多“AI绘画”的新闻，这些应用大多是通过GAN实现的。实际上，GAN不仅能做图像生成，还能辅助其他输入信息不足的视觉任务。比如SRGAN，就是把GAN应用在超分辨率(SR)任务上的代表之作。</p>
<p>在这篇文章中，我将主要面向深度学习的初学者，介绍SRGAN[1]这篇论文，同时分享以下知识：</p>
<ul>
<li>GAN的原理与训练过程</li>
<li>感知误差(Perceptual Loss)</li>
<li>基于的GAN的SR模型框架</li>
</ul>
<p>讲完了知识后，我还会解读一下MMEditing的SRGAN的训练代码。看懂这份代码能够加深对SRGAN训练算法的理解。</p>
<h2 id="SRGAN-核心思想"><a href="#SRGAN-核心思想" class="headerlink" title="SRGAN 核心思想"></a>SRGAN 核心思想</h2><p>早期超分辨率方法的优化目标都是降低低清图像和高清图像之间的均方误差。降低均方误差，确实让增强图像和原高清图像的相似度更高。但是，图像的相似度指标高并不能代表图像的增强质量就很高。下图显示了插值、优化均方误差、SRGAN、原图这四个图像输出结果（括号里的相似度指标是PSNR和SSIM）。</p>
<p><img src="/2022/08/15/20220813-SRGAN/1.jpg" alt></p>
<p>从图中可以看出，优化均方误差虽然能让相似度指标升高，但图像的细节十分模糊，尤其是纹理比较密集的高频区域。相比之下，SRGAN增强出来的图像虽然相似度不高，但看起来更加清晰。</p>
<p>为什么SRGAN的增强结果那么清楚呢？这是因为SRGAN使用了一套新的优化目标。SRGAN使用的损失函数既包括了<strong>GAN误差</strong>，也包括了<strong>感知误差</strong>。这套新的优化目标能够让网络生成看起来更清楚的图片，而不仅仅是和原高清图像相似度更高的图片。</p>
<p>下面，我们来一步一步学习SRGAN的框架。</p>
<h2 id="GAN-的原理"><a href="#GAN-的原理" class="headerlink" title="GAN 的原理"></a>GAN 的原理</h2><p>GAN[2]是一套搭建神经网络的框架。给定一个图片数据集$p_g$，GAN的目的是训练出一个<strong>生成网络</strong>$G$，使得G能够凭空生成出和$p_g$中大多数图片都类似的图片。比如说$p_g$是一个小猫图片数据集，那么$G$就应该能凭空生成出小猫图片。当然，$G$不是真的没有任何输入，真的能够凭空生成一幅图片。为了生成出不一样的图片，$G$要求输入一个随机量，这个随机量叫做噪声$z$。这样，只要输入的噪声$z$变了，$G$的输出$G(z)$就变了，就能画出长相不一样的小猫了。</p>
<p>为了指导图像生成，$G$应该有一个“老师”告诉它该怎么画出更像的图片。这个“老师”叫做<strong>判别网络</strong>$D$。$D$就是一个二分类网络，它能够严格地判定出一幅图片是否来自数据集$p_g$。如果$p_g$是一个小猫数据集，那么$D$就应该能判定一张图片是不是小猫。这样，如果$G$生成出来的图片$G(z)$已经非常逼真，连$D$都觉得$G(z)$来自数据集$p_g$，那么$G$就是一个很成功的网络了。</p>
<p>如果只是生成小猫，我们直接拿小猫图片和其他图片就能训练出一个$D$了。问题是，大多数情况下我们只有数据集$p_g$，而难以获得一个$p_g$的反例数据集。GAN的想法，则巧妙地解决了这个问题：刚开始，$G$生成出来的图片肯定是很差的，这些图片肯定不像$p_g$。所以，我们以$G(z)$为反例，和$p_g$一起训练出一个$D$来。等$D$的判定能力强了以后，又拿$D$回头训练$G$。这样，$D$的审美水平逐渐提高，$G$的绘画能力也逐渐提高。最终，$D$能成功分辨出一幅图片是否来自$p_g$，而$G$生成出来的图片和$p_g$中的看起来完全相同，连$D$也分辨不出来。就这样，我们得到了一个很棒的生成网络$G$。</p>
<p>规范地来说，给定一个数据集$p_g$，我们希望训练出两个网络$D, G$。$D$能够判断一幅输入图片是否来自$p_g$:</p>
<script type="math/tex; mode=display">
D(x) = \left\{
\begin{aligned}
&1 & x \in p_g \\
&0 & x \notin p_g
\end{aligned}
\right.</script><p>$G$则能够根据来自噪声分布$p_z$的$z$生成一个真假难辨的图片$G(z)$，使得$D(G(z))=1$。</p>
<p>为了达到这个目标，二分类器$D$应该最小化这样一个的交叉熵误差：</p>
<script type="math/tex; mode=display">L(\hat{y}, y)=-(y \ log\hat{y} + (1-y) \ log(1-\hat{y}))</script><p>其中，$\hat{y}=D(x)$是预测结果为真的概率，$y$是0或1的标签。</p>
<p>对于来自数据集的图片$x \sim p_g$，$D$使用的标签$y$应该是1，误差公式化简为：</p>
<script type="math/tex; mode=display">
L(x)=-logD(x), x \sim p_g</script><p>对于$G$生成的图片$G(z)$，$D$使用的标签$y$应该是0，误差公式化简为：</p>
<script type="math/tex; mode=display">
L(z)=-log(1-D(G(z))), z \sim p_z</script><p>我们每步拿一张真图$x$和一张假图$G(z)$训练$D$。这样，每步的误差公式就是上面两个式子加起来：</p>
<script type="math/tex; mode=display">
L_D(x, z)=-(logD(x) + log(1-D(G(z)))), x \sim p_g, z \sim p_z</script><p>反过来，$G$应该和$D$对抗，最大化上面那个误差，想办法骗过$D$。这个“对抗”就是GAN的名称“生成对抗网络”的由来。但是，$G$不能改变$D(x)$那一项。因此，$G$使用的误差函数是：</p>
<script type="math/tex; mode=display">
L_G(z)=log(1-D(G(z))), z \sim p_z</script><p>使用上面这两种误差，就可以训练神经网络了。训练GAN时，每轮一般会训练$k(k&gt;=1)$次$D$，再训练1次$G$。这是为了先得到一个好的判别器，再用判别器去指导生成器。</p>
<p>GAN只是一套通用的框架，并没有指定神经网络$D, G$的具体结构。在不同任务中，$D, G$一般有不同的结构。</p>
<h2 id="基于GAN的超分辨率网络"><a href="#基于GAN的超分辨率网络" class="headerlink" title="基于GAN的超分辨率网络"></a>基于GAN的超分辨率网络</h2><p>如前文所述，以优化均方误差为目标的超分辨率模型难以复原图像的细节。其实，超分辨率任务和图像生成任务类似，都需要一个“老师”来指导优化目标。SRGAN把GAN框架运用到了超分辨率任务上。原来的生成器$G$随机生成图像，现在用来输出高清图像；原来的判定器$D$用来判定图像是否属于某数据集，现在$D$用来判断一幅图像是否是高清图像。</p>
<p>具体来说，相比基础的GAN，在SRGAN中，$D$的输入是高清图像$I^{HR}$。而$G$的输入从随机噪声$z$变成了高清图像退化后的低清图像$I^{LR}$。这样，$G$就不是在随机生成图像，而是在根据一幅低清图像生成一幅高清图像了。它们的误差函数分别是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L_D&=-(logD(I^{HR}) + log(1-D(G(I^{LR}))))\\
L_G&=log(1-D(G(I^{LR})))
\end{aligned}</script><p>借助GAN的架构，SRGAN能够利用$D$指导高清图像生成。但是，超分辨率任务毕竟和图像生成任务有一些区别，不能只用这种对抗误差来约束网络。因此，除了使用对抗误差外，SRGAN还使用了一种内容误差。这种内容误差用于让低清图片和高清图片的内容对齐，起到了和原均方误差一样的作用。</p>
<h2 id="基于感知的内容误差"><a href="#基于感知的内容误差" class="headerlink" title="基于感知的内容误差"></a>基于感知的内容误差</h2><p>在介绍SRGAN的内容误差之前，需要对“内容误差”和“感知误差”这两个名词做一个澄清。在SRGAN的原文章中，作者把内容误差和对抗误差之和叫做感知误差。但是，后续的大部分文献只把这种内容误差叫做感知误差，不会把内容误差和对抗误差放在一起称呼。在后文中，我也会用“感知误差”来指代SRGAN中的“内容误差”。</p>
<p>在深度卷积神经网络（CNN）火起来后，人们开始研究为什么CNN能够和人类一样识别出图像。经实验，人们发现两幅图像经VGG（一个经典的CNN）的某些中间层的输出越相似，两幅图像从观感上也越相似。这种相似度并不是基于某种数学指标，而是和人的感知非常类似。</p>
<p>VGG的这种“感知性”被运用在了风格迁移等任务上。也有人考虑把这种感知上的误差运用到超分辨率任务上，并取得了不错的结果[3]。下图是真值、插值、基于逐像素误差、基于感知误差的四个超分辨率结果。</p>
<p><img src="/2022/08/15/20220813-SRGAN/2.jpg" alt></p>
<p>SRGAN也使用了这种感知误差，以取代之前常常使用的逐像素均方误差。这种感知误差的计算方法如下：VGG有很多中间层，用于计算感知误差的中间层$i$是可调的。假如我们用$\phi_{i}(I)$表示图像$I$经VGG的第$i$层的中间输出结果，$\phi_{i}(I)_{x, y}$表示中间输出结果在坐标$(x, y)$处的值，则感知误差的公式如下：</p>
<script type="math/tex; mode=display">
L_{p}(I^{HR}, I^{LR})_{i}=\frac{1}{WH}\Sigma_{x=1}^{W}\Sigma_{y=1}^{H}(\phi_{i}(I^{HR})_{x, y}-\phi_{i}(G(I^{LR}))_{x, y})^2</script><p>直观上解释这个公式，就是先把高清图像$I^{HR}$送入VGG，再把高清图像退化出来的低清图像$I^{LR}$送入生成器，并把生成器的输出$G(I^{LR})$也送入VGG。两幅图片经VGG第$i$层生成的中间结果的逐像素均方误差，就是感知误差。</p>
<p>算上之前的对抗误差，一个图像超分辨率网络的总误差如下：</p>
<script type="math/tex; mode=display">
L_{SR}=L_p + w L_G</script><p>这里的$w$用于调整两个误差的相对权重，原论文使用$w=10^{-3}$。</p>
<h2 id="SRGAN的其他模块"><a href="#SRGAN的其他模块" class="headerlink" title="SRGAN的其他模块"></a>SRGAN的其他模块</h2><p>定义好了误差函数，只要在决定好网络结构就可以开始训练网络了。SRGAN使用的生成网络和判别网络的结构如下：</p>
<p><img src="/2022/08/15/20220813-SRGAN/3.jpg" alt></p>
<p>判别网络就是一个平平无奇的二分类网络，架构上没有什么创新。而生成网络则先用几个残差块提取特征，最后用一种超分辨率任务中常用的上采样模块PixelShuffle对原图像的尺寸翻倍两次，最后输出一个边长放大4倍的高清图像。</p>
<p>SRGAN的这种网络结构在当时确实取得了不错的结果。但是，很快就有后续研究提出了更好的网络架构。比如ESRGAN[4]去掉了生成网络的BN层，提出了一种叫做RRDB的高级模块。基于RRDB的生成网络有着更好的生成效果。</p>
<p>不仅是网络架构，SRGAN的其他细节也得到了后续研究的改进。GAN误差的公式、总误差的公式、高清图像退化成低清图像的数据增强算法……这些子模块都被后续研究改进了。但是，SRGAN这种基于GAN的训练架构一直没有发生改变。有了SRGAN的代码，想复现一些更新的超分辨率网络时，往往只需要换一下生成器的结构，或者改一改误差的公式就行了。大部分的训练代码是不用改变的。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>SRGAN是把GAN运用在超分辨率任务上的开山之作。如正文所述，SRGAN中的部分设计虽然已经过时，但它的整体训练架构被一直沿用了下来。现在去回顾SRGAN这篇论文时，只需要关注以下几点即可:</p>
<ul>
<li>如何把GAN套用在超分辨率任务上</li>
<li>GAN误差</li>
<li>感知误差</li>
</ul>
<p>通过阅读这篇论文，我们不仅应该学会GAN是怎样运用在SR上的，也应该能总结出如何把GAN应用在其他任务上。GAN的本质是去学习一个分布，令生成的$G(z)$看上去是来自分布$p_g$，而不是像图像分类等任务去学习一个$x \to y$的映射关系。因此，GAN会记忆一些和数据集相关的信息。在输入信息就已经比较完备的图像分类、目标检测等任务中，GAN可能没有什么用武之地。但是，在输入信息不足的超分辨率、图像补全等任务中，GAN记忆的数据集信息有很有用了。很多时候，GAN会“脑补”出输入图像中不够清楚的部分。</p>
<p>决定了要在某个任务中使用GAN时，我们可以在一个不使用GAN的架构上做以下改动：</p>
<ul>
<li>定义一个分类网络$D$。</li>
<li>在原loss中加一项由$D$算出来的GAN loss。</li>
<li>在训练流程中，加入训练$D$的逻辑。</li>
</ul>
<p>看完正文后，如果你对GAN在SR上的训练逻辑还是不太清楚，欢迎阅读附录中有关SRGAN训练代码的解读。</p>
<h2 id="附录：MMEditing-中的-SRGAN"><a href="#附录：MMEditing-中的-SRGAN" class="headerlink" title="附录：MMEditing 中的 SRGAN"></a>附录：MMEditing 中的 SRGAN</h2><p>MMEditing中的SRGAN写在<code>mmedit/models/restorers/srgan.py</code>这个文件里。学习训练逻辑时，我们只需要关注<code>SRGAN</code>类的<code>train_step</code>方法即可。</p>
<p>以下是<code>train_step</code>的源代码（我的mmedit版本是v0.15.1）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">self, data_batch, optimizer</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Train step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data_batch (dict): A batch of data.</span></span><br><span class="line"><span class="string">        optimizer (obj): Optimizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dict: Returned output.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># data</span></span><br><span class="line">    lq = data_batch[<span class="string">&#x27;lq&#x27;</span>]</span><br><span class="line">    gt = data_batch[<span class="string">&#x27;gt&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># generator</span></span><br><span class="line">    fake_g_output = self.generator(lq)</span><br><span class="line"></span><br><span class="line">    losses = <span class="built_in">dict</span>()</span><br><span class="line">    log_vars = <span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># no updates to discriminator parameters.</span></span><br><span class="line">    set_requires_grad(self.discriminator, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (self.step_counter % self.disc_steps == <span class="number">0</span></span><br><span class="line">            <span class="keyword">and</span> self.step_counter &gt;= self.disc_init_steps):</span><br><span class="line">        <span class="keyword">if</span> self.pixel_loss:</span><br><span class="line">            losses[<span class="string">&#x27;loss_pix&#x27;</span>] = self.pixel_loss(fake_g_output, gt)</span><br><span class="line">        <span class="keyword">if</span> self.perceptual_loss:</span><br><span class="line">            loss_percep, loss_style = self.perceptual_loss(</span><br><span class="line">                fake_g_output, gt)</span><br><span class="line">            <span class="keyword">if</span> loss_percep <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                losses[<span class="string">&#x27;loss_perceptual&#x27;</span>] = loss_percep</span><br><span class="line">            <span class="keyword">if</span> loss_style <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                losses[<span class="string">&#x27;loss_style&#x27;</span>] = loss_style</span><br><span class="line">        <span class="comment"># gan loss for generator</span></span><br><span class="line">        fake_g_pred = self.discriminator(fake_g_output)</span><br><span class="line">        losses[<span class="string">&#x27;loss_gan&#x27;</span>] = self.gan_loss(</span><br><span class="line">            fake_g_pred, target_is_real=<span class="literal">True</span>, is_disc=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># parse loss</span></span><br><span class="line">        loss_g, log_vars_g = self.parse_losses(losses)</span><br><span class="line">        log_vars.update(log_vars_g)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># optimize</span></span><br><span class="line">        optimizer[<span class="string">&#x27;generator&#x27;</span>].zero_grad()</span><br><span class="line">        loss_g.backward()</span><br><span class="line">        optimizer[<span class="string">&#x27;generator&#x27;</span>].step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># discriminator</span></span><br><span class="line">    set_requires_grad(self.discriminator, <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># real</span></span><br><span class="line">    real_d_pred = self.discriminator(gt)</span><br><span class="line">    loss_d_real = self.gan_loss(</span><br><span class="line">        real_d_pred, target_is_real=<span class="literal">True</span>, is_disc=<span class="literal">True</span>)</span><br><span class="line">    loss_d, log_vars_d = self.parse_losses(<span class="built_in">dict</span>(loss_d_real=loss_d_real))</span><br><span class="line">    optimizer[<span class="string">&#x27;discriminator&#x27;</span>].zero_grad()</span><br><span class="line">    loss_d.backward()</span><br><span class="line">    log_vars.update(log_vars_d)</span><br><span class="line">    <span class="comment"># fake</span></span><br><span class="line">    fake_d_pred = self.discriminator(fake_g_output.detach())</span><br><span class="line">    loss_d_fake = self.gan_loss(</span><br><span class="line">        fake_d_pred, target_is_real=<span class="literal">False</span>, is_disc=<span class="literal">True</span>)</span><br><span class="line">    loss_d, log_vars_d = self.parse_losses(<span class="built_in">dict</span>(loss_d_fake=loss_d_fake))</span><br><span class="line">    loss_d.backward()</span><br><span class="line">    log_vars.update(log_vars_d)</span><br><span class="line"></span><br><span class="line">    optimizer[<span class="string">&#x27;discriminator&#x27;</span>].step()</span><br><span class="line"></span><br><span class="line">    self.step_counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    log_vars.pop(<span class="string">&#x27;loss&#x27;</span>)  <span class="comment"># remove the unnecessary &#x27;loss&#x27;</span></span><br><span class="line">    outputs = <span class="built_in">dict</span>(</span><br><span class="line">        log_vars=log_vars,</span><br><span class="line">        num_samples=<span class="built_in">len</span>(gt.data),</span><br><span class="line">        results=<span class="built_in">dict</span>(lq=lq.cpu(), gt=gt.cpu(), output=fake_g_output.cpu()))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure></p>
<p>一开始，图像输出都在词典<code>data_batch</code>里。函数先把低清图<code>lq</code>和高清的真值<code>gt</code>从词典里取出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># data</span></span><br><span class="line">lq = data_batch[<span class="string">&#x27;lq&#x27;</span>]</span><br><span class="line">gt = data_batch[<span class="string">&#x27;gt&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>之后，函数计算了$G(I^{lq})$，为后续loss的计算做准备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generator</span></span><br><span class="line">fake_g_output = self.generator(lq)</span><br></pre></td></tr></table></figure>
<p>接下来，是优化生成器<code>self.generator</code>的逻辑。这里面有一些函数调用，我们可以不管它们的实现，大概理解整段代码的意思就行了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">losses = <span class="built_in">dict</span>()</span><br><span class="line">log_vars = <span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># no updates to discriminator parameters.</span></span><br><span class="line">set_requires_grad(self.discriminator, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (self.step_counter % self.disc_steps == <span class="number">0</span></span><br><span class="line">        <span class="keyword">and</span> self.step_counter &gt;= self.disc_init_steps):</span><br><span class="line">    <span class="keyword">if</span> self.pixel_loss:</span><br><span class="line">        losses[<span class="string">&#x27;loss_pix&#x27;</span>] = self.pixel_loss(fake_g_output, gt)</span><br><span class="line">    <span class="keyword">if</span> self.perceptual_loss:</span><br><span class="line">        loss_percep, loss_style = self.perceptual_loss(</span><br><span class="line">            fake_g_output, gt)</span><br><span class="line">        <span class="keyword">if</span> loss_percep <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            losses[<span class="string">&#x27;loss_perceptual&#x27;</span>] = loss_percep</span><br><span class="line">        <span class="keyword">if</span> loss_style <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            losses[<span class="string">&#x27;loss_style&#x27;</span>] = loss_style</span><br><span class="line">    <span class="comment"># gan loss for generator</span></span><br><span class="line">    fake_g_pred = self.discriminator(fake_g_output)</span><br><span class="line">    losses[<span class="string">&#x27;loss_gan&#x27;</span>] = self.gan_loss(</span><br><span class="line">        fake_g_pred, target_is_real=<span class="literal">True</span>, is_disc=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse loss</span></span><br><span class="line">    loss_g, log_vars_g = self.parse_losses(losses)</span><br><span class="line">    log_vars.update(log_vars_g)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># optimize</span></span><br><span class="line">    optimizer[<span class="string">&#x27;generator&#x27;</span>].zero_grad()</span><br><span class="line">    loss_g.backward()</span><br><span class="line">    optimizer[<span class="string">&#x27;generator&#x27;</span>].step()</span><br></pre></td></tr></table></figure>
<p>为了只训练生成器，要用下面的代码关闭判别器的训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># no updates to discriminator parameters.</span></span><br><span class="line">set_requires_grad(self.discriminator, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>正文说过，训练GAN时一般要先训好判别器，且训练判别器多于训练生成器。因此，下面的if语句可以让判别器训练了<code>self.disc_init_steps</code>步后，每训练<code>self.disc_steps</code>步判别器再训练一步生成器。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if (self.step_counter % self.disc_steps == 0</span><br><span class="line">    and self.step_counter &gt;= self.disc_init_steps):</span><br></pre></td></tr></table></figure><br>if语句块里分别计算了逐像素误差（比如均方误差和L1误差）、感知误差、GAN误差。虽然SRGAN完全抛弃了逐像素误差，但实际训练时我们还是可以按一定比例加上这个误差。这些误差最后会用于训练生成器。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.pixel_loss:</span><br><span class="line">    losses[<span class="string">&#x27;loss_pix&#x27;</span>] = self.pixel_loss(fake_g_output, gt)</span><br><span class="line"><span class="keyword">if</span> self.perceptual_loss:</span><br><span class="line">    loss_percep, loss_style = self.perceptual_loss(</span><br><span class="line">        fake_g_output, gt)</span><br><span class="line">    <span class="keyword">if</span> loss_percep <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        losses[<span class="string">&#x27;loss_perceptual&#x27;</span>] = loss_percep</span><br><span class="line">    <span class="keyword">if</span> loss_style <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        losses[<span class="string">&#x27;loss_style&#x27;</span>] = loss_style</span><br><span class="line"><span class="comment"># gan loss for generator</span></span><br><span class="line">fake_g_pred = self.discriminator(fake_g_output)</span><br><span class="line">losses[<span class="string">&#x27;loss_gan&#x27;</span>] = self.gan_loss(</span><br><span class="line">    fake_g_pred, target_is_real=<span class="literal">True</span>, is_disc=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># parse loss</span></span><br><span class="line">loss_g, log_vars_g = self.parse_losses(losses)</span><br><span class="line">log_vars.update(log_vars_g)</span><br><span class="line"></span><br><span class="line"><span class="comment"># optimize</span></span><br><span class="line">optimizer[<span class="string">&#x27;generator&#x27;</span>].zero_grad()</span><br><span class="line">loss_g.backward()</span><br><span class="line">optimizer[<span class="string">&#x27;generator&#x27;</span>].step()</span><br></pre></td></tr></table></figure></p>
<p>训练完生成器后，要训练判别器。和生成器的误差计算方法类似，判别器的训练代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># discriminator</span></span><br><span class="line">set_requires_grad(self.discriminator, <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># real</span></span><br><span class="line">real_d_pred = self.discriminator(gt)</span><br><span class="line">loss_d_real = self.gan_loss(</span><br><span class="line">    real_d_pred, target_is_real=<span class="literal">True</span>, is_disc=<span class="literal">True</span>)</span><br><span class="line">loss_d, log_vars_d = self.parse_losses(<span class="built_in">dict</span>(loss_d_real=loss_d_real))</span><br><span class="line">optimizer[<span class="string">&#x27;discriminator&#x27;</span>].zero_grad()</span><br><span class="line">loss_d.backward()</span><br><span class="line">log_vars.update(log_vars_d)</span><br><span class="line"><span class="comment"># fake</span></span><br><span class="line">fake_d_pred = self.discriminator(fake_g_output.detach())</span><br><span class="line">loss_d_fake = self.gan_loss(</span><br><span class="line">    fake_d_pred, target_is_real=<span class="literal">False</span>, is_disc=<span class="literal">True</span>)</span><br><span class="line">loss_d, log_vars_d = self.parse_losses(<span class="built_in">dict</span>(loss_d_fake=loss_d_fake))</span><br><span class="line">loss_d.backward()</span><br><span class="line">log_vars.update(log_vars_d)</span><br><span class="line"></span><br><span class="line">optimizer[<span class="string">&#x27;discriminator&#x27;</span>].step()</span><br></pre></td></tr></table></figure>
<p>这段代码有两个重点：</p>
<ol>
<li>在训练判别器时，要用<code>set_requires_grad(self.discriminator, True)</code>开启判别器的梯度计算。</li>
<li><code>fake_d_pred = self.discriminator(fake_g_output.detach())</code>这一行的<code>detach()</code>很关键。<code>detach()</code>可以中断某张量的梯度跟踪。<code>fake_g_output</code>是由生成器算出来的，如果不把这个张量的梯度跟踪切断掉，在优化判别器时生成器的参数也会跟着优化。</li>
</ol>
<p>函数的最后部分是一些和MMEditing其他代码逻辑的交互，和SRGAN本身没什么关联。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">self.step_counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">log_vars.pop(<span class="string">&#x27;loss&#x27;</span>)  <span class="comment"># remove the unnecessary &#x27;loss&#x27;</span></span><br><span class="line">outputs = <span class="built_in">dict</span>(</span><br><span class="line">    log_vars=log_vars,</span><br><span class="line">    num_samples=<span class="built_in">len</span>(gt.data),</span><br><span class="line">    results=<span class="built_in">dict</span>(lq=lq.cpu(), gt=gt.cpu(), output=fake_g_output.cpu()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>只要理解了本文的误差计算公式，再看懂了这段代码是如何训练判别器和生成器的，就算是完全理解了SRGAN的核心思想了。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] (SRGAN): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.04802">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</a></p>
<p>[2] (GAN): <a target="_blank" rel="noopener" href="http://papers.neurips.cc/paper/5423-generative-adversarial-nets.pdf">Generative Adversarial Nets</a></p>
<p>[3] (Perceptual Loss)：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.08155">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a></p>
<p>[4] (ESRGAN): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.00219">ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/08/09/20220712-custom-op-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/09/20220712-custom-op-2/" class="post-title-link" itemprop="url">PyTorch 自定义算子：复现CPU和CUDA版的二维卷积</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-09 14:20:00" itemprop="dateCreated datePublished" datetime="2022-08-09T14:20:00+08:00">2022-08-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>我之前的<a href="https://zhouyifan.net/2022/03/18/20220315-custom-op/">一篇文章</a>介绍了如何给PyTorch添加CPU上的简单的加法算子。在这篇文章里，我将继续展示一个更具体的PyTorch自定义算子示例——自己动手复现二维卷积算子。这个示例是基于PyTorch Extension的，在迁移项目时，不需要自己生成动态库，只需要用<code>setup.py</code>重新编译一遍即可。我会同时介绍CPU版和CUDA版的实现。</p>
<p>许多前沿的神经网络都会对卷积进行一些修改。比如大名鼎鼎的可变形卷积(deformable convolution)。相信看完这篇文章后，大家能看懂PyTorch卷积的实现代码，并大概了解如何修改卷积的实现细节，并把新写好的卷积运用到自己的PyTorch项目中。</p>
<h1 id="PyTorch-Extension-实现二维卷积"><a href="#PyTorch-Extension-实现二维卷积" class="headerlink" title="PyTorch Extension 实现二维卷积"></a>PyTorch Extension 实现二维卷积</h1><h2 id="搭建项目"><a href="#搭建项目" class="headerlink" title="搭建项目"></a>搭建项目</h2><p>在开始写代码前，要准备一个崭新的目录，在这个文件夹里搭建项目。</p>
<p>在根目录下，先创建一个<code>setup.py</code>，之后要填写这份安装文件。</p>
<p>之后，创建一个文件夹，其名字是项目名。在这个文件夹里合适的地方新建一个子文件夹，专门用来放和算子相关的文件。我的项目名叫做<code>panoflow</code>，算子相关文件放在了<code>panoflow/core/op</code>子文件夹下。</p>
<p>接下来，和算子实现相关的文件都应该放在算子文件夹里。使用和测试算子的文件可以放在项目文件夹的其他地方。</p>
<p>由于在实现中我借用了MMCV的代码，还要提前准备好一些头文件。首先新建一个文件<code>pytorch_cpp_helper.hpp</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> PYTORCH_CPP_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PYTORCH_CPP_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> at;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CUDA(x) \</span></span><br><span class="line"><span class="meta">  TORCH_CHECK(x.device().is_cuda(), #x <span class="meta-string">&quot; must be a CUDA tensor&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CPU(x) \</span></span><br><span class="line"><span class="meta">  TORCH_CHECK(!x.device().is_cuda(), #x <span class="meta-string">&quot; must be a CPU tensor&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CONTIGUOUS(x) \</span></span><br><span class="line"><span class="meta">  TORCH_CHECK(x.is_contiguous(), #x <span class="meta-string">&quot; must be contiguous&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CUDA_INPUT(x) \</span></span><br><span class="line"><span class="meta">  CHECK_CUDA(x);            \</span></span><br><span class="line"><span class="meta">  CHECK_CONTIGUOUS(x)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CPU_INPUT(x) \</span></span><br><span class="line"><span class="meta">  CHECK_CPU(x);            \</span></span><br><span class="line"><span class="meta">  CHECK_CONTIGUOUS(x)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// PYTORCH_CPP_HELPER</span></span></span><br></pre></td></tr></table></figure>
<p>再创建一个文件<code>pytorch_cuda_helper.hpp</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> PYTORCH_CUDA_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PYTORCH_CUDA_HELPER</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ATen/ATen.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ATen/cuda/CUDAContext.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;c10/cuda/CUDAGuard.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ATen/cuda/CUDAApplyUtils.cuh&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;THC/THCAtomics.cuh&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;common_cuda_helper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> at::Half;</span><br><span class="line"><span class="keyword">using</span> at::Tensor;</span><br><span class="line"><span class="keyword">using</span> phalf = at::Half;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __PHALF(x) (x)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// PYTORCH_CUDA_HELPER</span></span></span><br></pre></td></tr></table></figure>
<p>还有一个<code>common_cuda_helper.hpp</code>：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> COMMON_CUDA_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> COMMON_CUDA_HELPER</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CUDA_1D_KERNEL_LOOP(i, n)                              \</span></span><br><span class="line"><span class="meta">  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; (n); \</span></span><br><span class="line"><span class="meta">       i += blockDim.x * gridDim.x)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CUDA_2D_KERNEL_LOOP(i, n, j, m)                             \</span></span><br><span class="line"><span class="meta">  for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; (n);   \</span></span><br><span class="line"><span class="meta">       i += blockDim.x * gridDim.x)                                 \</span></span><br><span class="line"><span class="meta">    for (size_t j = blockIdx.y * blockDim.y + threadIdx.y; j &lt; (m); \</span></span><br><span class="line"><span class="meta">         j += blockDim.y * gridDim.y)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CUDA_2D_KERNEL_BLOCK_LOOP(i, n, j, m)          \</span></span><br><span class="line"><span class="meta">  for (size_t i = blockIdx.x; i &lt; (n); i += gridDim.x) \</span></span><br><span class="line"><span class="meta">    for (size_t j = blockIdx.y; j &lt; (m); j += gridDim.y)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THREADS_PER_BLOCK 512</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">GET_BLOCKS</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">int</span> num_threads = THREADS_PER_BLOCK)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> optimal_block_num = (N + num_threads - <span class="number">1</span>) / num_threads;</span><br><span class="line">  <span class="keyword">int</span> max_block_num = <span class="number">4096</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">min</span>(optimal_block_num, max_block_num);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ T <span class="title">bilinear_interpolate</span><span class="params">(<span class="keyword">const</span> T* input, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="keyword">const</span> <span class="keyword">int</span> width, T y, T x,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="keyword">const</span> <span class="keyword">int</span> index <span class="comment">/* index for debug only*/</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// deal with cases that inverse elements are out of feature map boundary</span></span><br><span class="line">  <span class="keyword">if</span> (y &lt; <span class="number">-1.0</span> || y &gt; height || x &lt; <span class="number">-1.0</span> || x &gt; width) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y &lt;= <span class="number">0</span>) y = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (x &lt;= <span class="number">0</span>) x = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> y_low = (<span class="keyword">int</span>)y;</span><br><span class="line">  <span class="keyword">int</span> x_low = (<span class="keyword">int</span>)x;</span><br><span class="line">  <span class="keyword">int</span> y_high;</span><br><span class="line">  <span class="keyword">int</span> x_high;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y_low &gt;= height - <span class="number">1</span>) &#123;</span><br><span class="line">    y_high = y_low = height - <span class="number">1</span>;</span><br><span class="line">    y = (T)y_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    y_high = y_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (x_low &gt;= width - <span class="number">1</span>) &#123;</span><br><span class="line">    x_high = x_low = width - <span class="number">1</span>;</span><br><span class="line">    x = (T)x_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    x_high = x_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  T ly = y - y_low;</span><br><span class="line">  T lx = x - x_low;</span><br><span class="line">  T hy = <span class="number">1.</span> - ly, hx = <span class="number">1.</span> - lx;</span><br><span class="line">  <span class="comment">// do bilinear interpolation</span></span><br><span class="line">  T v1 = input[y_low * width + x_low];</span><br><span class="line">  T v2 = input[y_low * width + x_high];</span><br><span class="line">  T v3 = input[y_high * width + x_low];</span><br><span class="line">  T v4 = input[y_high * width + x_high];</span><br><span class="line">  T w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;</span><br><span class="line"></span><br><span class="line">  T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> val;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ <span class="keyword">void</span> <span class="title">bilinear_interpolate_gradient</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> height, <span class="keyword">const</span> <span class="keyword">int</span> width, T y, T x, T&amp; w1, T&amp; w2, T&amp; w3, T&amp; w4,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">int</span>&amp; x_low, <span class="keyword">int</span>&amp; x_high, <span class="keyword">int</span>&amp; y_low, <span class="keyword">int</span>&amp; y_high,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> index <span class="comment">/* index for debug only*/</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// deal with cases that inverse elements are out of feature map boundary</span></span><br><span class="line">  <span class="keyword">if</span> (y &lt; <span class="number">-1.0</span> || y &gt; height || x &lt; <span class="number">-1.0</span> || x &gt; width) &#123;</span><br><span class="line">    <span class="comment">// empty</span></span><br><span class="line">    w1 = w2 = w3 = w4 = <span class="number">0.</span>;</span><br><span class="line">    x_low = x_high = y_low = y_high = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y &lt;= <span class="number">0</span>) y = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (x &lt;= <span class="number">0</span>) x = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  y_low = (<span class="keyword">int</span>)y;</span><br><span class="line">  x_low = (<span class="keyword">int</span>)x;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y_low &gt;= height - <span class="number">1</span>) &#123;</span><br><span class="line">    y_high = y_low = height - <span class="number">1</span>;</span><br><span class="line">    y = (T)y_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    y_high = y_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (x_low &gt;= width - <span class="number">1</span>) &#123;</span><br><span class="line">    x_high = x_low = width - <span class="number">1</span>;</span><br><span class="line">    x = (T)x_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    x_high = x_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  T ly = y - y_low;</span><br><span class="line">  T lx = x - x_low;</span><br><span class="line">  T hy = <span class="number">1.</span> - ly, hx = <span class="number">1.</span> - lx;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// reference in forward</span></span><br><span class="line">  <span class="comment">// T v1 = input[y_low * width + x_low];</span></span><br><span class="line">  <span class="comment">// T v2 = input[y_low * width + x_high];</span></span><br><span class="line">  <span class="comment">// T v3 = input[y_high * width + x_low];</span></span><br><span class="line">  <span class="comment">// T v4 = input[y_high * width + x_high];</span></span><br><span class="line">  <span class="comment">// T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);</span></span><br><span class="line"></span><br><span class="line">  w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// COMMON_CUDA_HELPER</span></span></span><br></pre></td></tr></table></figure></p>
<p>这些文件添加了CPU和CUDA实现时需要的头文件和定义，后面的C++源码会用到它们。</p>
<h2 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h2><h3 id="C-实现"><a href="#C-实现" class="headerlink" title="C++实现"></a>C++实现</h3><p>在用C++实现一个算子时，我们要编写一个形如这样的文件：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">my_add</span><span class="params">(torch::Tensor t1, torch::Tensor t2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">return</span> t1 + t2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TORCH_LIBRARY</span>(my_ops, m)</span><br><span class="line">&#123;</span><br><span class="line">	m.<span class="built_in">def</span>(<span class="string">&quot;my_add&quot;</span>, my_add);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个C++文件主要包含两部分内容：算子的实现函数和C++接口绑定。在实现卷积时，也是要实现这两部分内容。</p>
<p>在修改一个现有的算子时，最好的方法不是从头写一个，而是去开源库里找一份实现，并在这个基础上进行修改。</p>
<blockquote>
<p>我在MMCV的仓库里找到了<a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmcv/tree/master/mmcv/ops">可变形卷积的实现</a>，并把它拆解回了普通的卷积。我参考了这篇教程：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/464492627">手把手教你如何高效地在 MMCV 中贡献算子</a>。另外，这份笔记还参考了<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/cpp_extension.html#writing-a-mixed-c-cuda-extension">PyTorch官方Extension教程</a>。</p>
</blockquote>
<p>找到了卷积的实现后，在算子文件夹下新建一个cpp源文件。比如我的文件路径就是<code>panoflow/core/op/my_conv.cpp</code>。这样一个普通卷积的实现如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;pytorch_cpp_helper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cuda</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_shape_check</span><span class="params">(at::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">                         at::Tensor weight, <span class="keyword">int</span> kH,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">int</span> kW, <span class="keyword">int</span> dH, <span class="keyword">int</span> dW, <span class="keyword">int</span> padH, <span class="keyword">int</span> padW,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">int</span> dilationH, <span class="keyword">int</span> dilationW, <span class="keyword">int</span> group)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(</span><br><span class="line">        weight.<span class="built_in">ndimension</span>() == <span class="number">4</span>,</span><br><span class="line">        <span class="string">&quot;4D weight tensor (nOutputPlane,nInputPlane,kH,kW) expected, but got: %s&quot;</span>,</span><br><span class="line">        weight.<span class="built_in">ndimension</span>());</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(weight.<span class="built_in">is_contiguous</span>(), <span class="string">&quot;weight tensor has to be contiguous&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(kW &gt; <span class="number">0</span> &amp;&amp; kH &gt; <span class="number">0</span>,</span><br><span class="line">                <span class="string">&quot;kernel size should be greater than zero, but got kH: %d kW: %d&quot;</span>,</span><br><span class="line">                kH, kW);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>((weight.<span class="built_in">size</span>(<span class="number">2</span>) == kH &amp;&amp; weight.<span class="built_in">size</span>(<span class="number">3</span>) == kW),</span><br><span class="line">                <span class="string">&quot;kernel size should be consistent with weight, &quot;</span>,</span><br><span class="line">                <span class="string">&quot;but got kH: %d kW: %d weight.size(2): %d, weight.size(3): %d&quot;</span>,</span><br><span class="line">                kH, kW, weight.<span class="built_in">size</span>(<span class="number">2</span>), weight.<span class="built_in">size</span>(<span class="number">3</span>));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(dW &gt; <span class="number">0</span> &amp;&amp; dH &gt; <span class="number">0</span>,</span><br><span class="line">                <span class="string">&quot;stride should be greater than zero, but got dH: %d dW: %d&quot;</span>, dH,</span><br><span class="line">                dW);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(</span><br><span class="line">        dilationW &gt; <span class="number">0</span> &amp;&amp; dilationH &gt; <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;dilation should be greater than 0, but got dilationH: %d dilationW: %d&quot;</span>,</span><br><span class="line">        dilationH, dilationW);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> ndim = input.<span class="built_in">ndimension</span>();</span><br><span class="line">    <span class="keyword">int</span> dimf = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> dimh = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> dimw = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ndim == <span class="number">4</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        dimf++;</span><br><span class="line">        dimh++;</span><br><span class="line">        dimw++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(ndim == <span class="number">3</span> || ndim == <span class="number">4</span>,</span><br><span class="line">                <span class="string">&quot;3D or 4D input tensor expected but got: %s&quot;</span>, ndim);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> nInputPlane = weight.<span class="built_in">size</span>(<span class="number">1</span>) * group;</span><br><span class="line">    <span class="keyword">long</span> inputHeight = input.<span class="built_in">size</span>(dimh);</span><br><span class="line">    <span class="keyword">long</span> inputWidth = input.<span class="built_in">size</span>(dimw);</span><br><span class="line">    <span class="keyword">long</span> nOutputPlane = weight.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">long</span> outputHeight =</span><br><span class="line">        (inputHeight + <span class="number">2</span> * padH - (dilationH * (kH - <span class="number">1</span>) + <span class="number">1</span>)) / dH + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">long</span> outputWidth =</span><br><span class="line">        (inputWidth + <span class="number">2</span> * padW - (dilationW * (kW - <span class="number">1</span>) + <span class="number">1</span>)) / dW + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (outputWidth &lt; <span class="number">1</span> || outputHeight &lt; <span class="number">1</span>)</span><br><span class="line">        <span class="built_in">AT_ERROR</span>(</span><br><span class="line">            <span class="string">&quot;Given input size: (%ld x %ld x %ld). &quot;</span></span><br><span class="line">            <span class="string">&quot;Calculated output size: (%ld x %ld x %ld). Output size is too small&quot;</span>,</span><br><span class="line">            nInputPlane, inputHeight, inputWidth, nOutputPlane, outputHeight,</span><br><span class="line">            outputWidth);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(input.<span class="built_in">size</span>(<span class="number">1</span>) == nInputPlane,</span><br><span class="line">                <span class="string">&quot;invalid number of input planes, expected: %d, but got: %d&quot;</span>,</span><br><span class="line">                nInputPlane, input.<span class="built_in">size</span>(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>((inputHeight &gt;= kH &amp;&amp; inputWidth &gt;= kW),</span><br><span class="line">                <span class="string">&quot;input image is smaller than kernel&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_forward</span><span class="params">(Tensor input, Tensor weight, Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Tensor output, Tensor columns, <span class="keyword">int</span> kW,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> kH, <span class="keyword">int</span> dW, <span class="keyword">int</span> dH, <span class="keyword">int</span> padW, <span class="keyword">int</span> padH,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> dilationW, <span class="keyword">int</span> dilationH, <span class="keyword">int</span> group,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> im2col_step)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">bool</span> isCuda = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (input.<span class="built_in">device</span>().<span class="built_in">is_cuda</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(input);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(weight);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(bias);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(output);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(columns);</span><br><span class="line">        isCuda = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(input);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(weight);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(bias);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(output);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(columns);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">my_conv_shape_check</span>(input, weight, kH, kW, dH, dW, padH,</span><br><span class="line">                        padW, dilationH, dilationW, group);</span><br><span class="line">    <span class="function">at::DeviceGuard <span class="title">guard</span><span class="params">(input.device())</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> batch = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span> (input.<span class="built_in">ndimension</span>() == <span class="number">3</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Force batch</span></span><br><span class="line">        batch = <span class="number">0</span>;</span><br><span class="line">        input.<span class="built_in">unsqueeze_</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> batchSize = input.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">long</span> nInputPlane = input.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">long</span> inputHeight = input.<span class="built_in">size</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">long</span> inputWidth = input.<span class="built_in">size</span>(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> nOutputPlane = weight.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> outputWidth =</span><br><span class="line">        (inputWidth + <span class="number">2</span> * padW - (dilationW * (kW - <span class="number">1</span>) + <span class="number">1</span>)) / dW + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">long</span> outputHeight =</span><br><span class="line">        (inputHeight + <span class="number">2</span> * padH - (dilationH * (kH - <span class="number">1</span>) + <span class="number">1</span>)) / dH + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    output = output.<span class="built_in">view</span>(&#123;batchSize / im2col_step, im2col_step, nOutputPlane,</span><br><span class="line">                          outputHeight, outputWidth&#125;);</span><br><span class="line">    columns = at::<span class="built_in">zeros</span>(</span><br><span class="line">        &#123;nInputPlane * kW * kH, im2col_step * outputHeight * outputWidth&#125;,</span><br><span class="line">        input.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    input = input.<span class="built_in">view</span>(&#123;batchSize / im2col_step, im2col_step, nInputPlane,</span><br><span class="line">                        inputHeight, inputWidth&#125;);</span><br><span class="line"></span><br><span class="line">    Tensor output_buffer = at::<span class="built_in">zeros</span>(&#123;batchSize / im2col_step, nOutputPlane,</span><br><span class="line">                                      im2col_step * outputHeight, outputWidth&#125;,</span><br><span class="line">                                     output.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    output_buffer = output_buffer.<span class="built_in">view</span>(</span><br><span class="line">        &#123;output_buffer.<span class="built_in">size</span>(<span class="number">0</span>), group, output_buffer.<span class="built_in">size</span>(<span class="number">1</span>) / group,</span><br><span class="line">         output_buffer.<span class="built_in">size</span>(<span class="number">2</span>), output_buffer.<span class="built_in">size</span>(<span class="number">3</span>)&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> elt = <span class="number">0</span>; elt &lt; batchSize / im2col_step; elt++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (isCuda)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">my_conv_im2col_cuda</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                            inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                            dilationW, im2col_step, columns);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">my_conv_im2col_cpu</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                            inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                            dilationW, im2col_step, columns);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        columns = columns.<span class="built_in">view</span>(&#123;group, columns.<span class="built_in">size</span>(<span class="number">0</span>) / group, columns.<span class="built_in">size</span>(<span class="number">1</span>)&#125;);</span><br><span class="line">        weight = weight.<span class="built_in">view</span>(&#123;group, weight.<span class="built_in">size</span>(<span class="number">0</span>) / group, weight.<span class="built_in">size</span>(<span class="number">1</span>),</span><br><span class="line">                              weight.<span class="built_in">size</span>(<span class="number">2</span>), weight.<span class="built_in">size</span>(<span class="number">3</span>)&#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> g = <span class="number">0</span>; g &lt; group; g++)</span><br><span class="line">        &#123;</span><br><span class="line">            output_buffer[elt][g] = output_buffer[elt][g]</span><br><span class="line">                                        .<span class="built_in">flatten</span>(<span class="number">1</span>)</span><br><span class="line">                                        .<span class="built_in">addmm_</span>(weight[g].<span class="built_in">flatten</span>(<span class="number">1</span>), columns[g])</span><br><span class="line">                                        .<span class="built_in">view_as</span>(output_buffer[elt][g]);</span><br><span class="line">        &#125;</span><br><span class="line">        columns =</span><br><span class="line">            columns.<span class="built_in">view</span>(&#123;columns.<span class="built_in">size</span>(<span class="number">0</span>) * columns.<span class="built_in">size</span>(<span class="number">1</span>), columns.<span class="built_in">size</span>(<span class="number">2</span>)&#125;);</span><br><span class="line">        weight = weight.<span class="built_in">view</span>(&#123;weight.<span class="built_in">size</span>(<span class="number">0</span>) * weight.<span class="built_in">size</span>(<span class="number">1</span>), weight.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">                              weight.<span class="built_in">size</span>(<span class="number">3</span>), weight.<span class="built_in">size</span>(<span class="number">4</span>)&#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    output_buffer = output_buffer.<span class="built_in">view</span>(</span><br><span class="line">        &#123;output_buffer.<span class="built_in">size</span>(<span class="number">0</span>), output_buffer.<span class="built_in">size</span>(<span class="number">1</span>) * output_buffer.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">         output_buffer.<span class="built_in">size</span>(<span class="number">3</span>), output_buffer.<span class="built_in">size</span>(<span class="number">4</span>)&#125;);</span><br><span class="line"></span><br><span class="line">    output_buffer = output_buffer.<span class="built_in">view</span>(&#123;batchSize / im2col_step, nOutputPlane,</span><br><span class="line">                                        im2col_step, outputHeight, outputWidth&#125;);</span><br><span class="line">    output_buffer.<span class="built_in">transpose_</span>(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    output.<span class="built_in">copy_</span>(output_buffer);</span><br><span class="line">    output = output.<span class="built_in">view</span>(&#123;batchSize, nOutputPlane, outputHeight, outputWidth&#125;);</span><br><span class="line"></span><br><span class="line">    bias = bias.<span class="built_in">view</span>(&#123;<span class="number">1</span>, bias.<span class="built_in">size</span>(<span class="number">0</span>), <span class="number">1</span>, <span class="number">1</span>&#125;);</span><br><span class="line">    output.<span class="built_in">add_</span>(bias);</span><br><span class="line"></span><br><span class="line">    input = input.<span class="built_in">view</span>(&#123;batchSize, nInputPlane, inputHeight, inputWidth&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (batch == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        output = output.<span class="built_in">view</span>(&#123;nOutputPlane, outputHeight, outputWidth&#125;);</span><br><span class="line">        input = input.<span class="built_in">view</span>(&#123;nInputPlane, inputHeight, inputWidth&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> T *data_im, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> kernel_h, <span class="keyword">const</span> <span class="keyword">int</span> kernel_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> pad_w, <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> num_channels, <span class="keyword">const</span> <span class="keyword">int</span> height_col,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width_col, T *data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; n; index++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// index index of output matrix</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_col = index % width_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_col = (index / width_col) % height_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> b_col = (index / width_col / height_col) % batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_im = (index / width_col / height_col) / batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_col = c_im * kernel_h * kernel_w;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_in = h_col * stride_h - pad_h;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_in = w_col * stride_w - pad_w;</span><br><span class="line">        T *data_col_ptr =</span><br><span class="line">            data_col +</span><br><span class="line">            ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;</span><br><span class="line">        <span class="keyword">const</span> T *data_im_ptr =</span><br><span class="line">            data_im + (b_col * num_channels + c_im) * height * width;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; kernel_h; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; kernel_w; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                T val = <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>);</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> h_im = h_in + i * dilation_h;</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> w_im = w_in + j * dilation_w;</span><br><span class="line">                <span class="keyword">if</span> (h_im &gt; <span class="number">-1</span> &amp;&amp; w_im &gt; <span class="number">-1</span> &amp;&amp; h_im &lt; height &amp;&amp; w_im &lt; width)</span><br><span class="line">                &#123;</span><br><span class="line">                    val = data_im_ptr[h_im * width + w_im];</span><br><span class="line">                &#125;</span><br><span class="line">                *data_col_ptr = val;</span><br><span class="line">                data_col_ptr += batch_size * height_col * width_col;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> height_col =</span><br><span class="line">        (height + <span class="number">2</span> * pad_h - (dilation_h * (ksize_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> width_col =</span><br><span class="line">        (width + <span class="number">2</span> * pad_w - (dilation_w * (ksize_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> num_kernels = channels * height_col * width_col * parallel_imgs;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES_AND_HALF</span>(</span><br><span class="line">        data_im.<span class="built_in">scalar_type</span>(), <span class="string">&quot;&quot;</span>, [&amp;]</span><br><span class="line">        &#123; my_conv_im2col_cpu_kernel&lt;<span class="keyword">scalar_t</span>&gt;(</span><br><span class="line">              num_kernels, data_im.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;(),</span><br><span class="line">              height, width, ksize_h, ksize_w,</span><br><span class="line">              pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,</span><br><span class="line">              parallel_imgs, channels,</span><br><span class="line">              height_col, width_col, data_col.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;()); &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cuda</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(my_ops, m)</span><br><span class="line">&#123;</span><br><span class="line">      m.<span class="built_in">def</span>(<span class="string">&quot;my_conv_forward&quot;</span>, my_conv_forward, <span class="string">&quot;my_conv_forward&quot;</span>,</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;input&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;weight&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;bias&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;output&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;columns&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;kW&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;kH&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dW&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dH&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;padW&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;padH&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dilationW&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dilationH&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;group&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;im2col_step&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这份实现非常长，我挑一些重点的内容讲解。</p>
<p>从最下面的<code>PYBIND11_MODULE(my_ops, m)</code>看起。这里的<code>my_ops</code>是生成的库名，可以随便取名。待会要import这个库名。代码块里<code>m.def</code>用于定义C++函数的Python接口。<code>&quot;my_conv_forward&quot;</code>是Python调用时的函数名称，<code>my_conv_forward</code>是被Python代码调用的这份代码里的C++函数名称。也就是说，这份卷积实现的入口函数就是<code>my_conv_forward</code>。我们从这个函数看起。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_forward</span><span class="params">(Tensor input, Tensor weight, Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Tensor output, Tensor columns, <span class="keyword">int</span> kW,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> kH, <span class="keyword">int</span> dW, <span class="keyword">int</span> dH, <span class="keyword">int</span> padW, <span class="keyword">int</span> padH,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> dilationW, <span class="keyword">int</span> dilationH, <span class="keyword">int</span> group,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> im2col_step)</span></span></span><br></pre></td></tr></table></figure>
<p><code>my_conv_forward</code>就是卷积的主函数。它的参数除了PyTorch的<code>Conv2d</code>传入的参数外，还多了两个参数<code>output, columus</code>。这两个张量是保存中间结果的，在PyTorch侧是看不到的。<code>output</code>用于保存卷积输出，<code>columns</code>用于保存卷积时的列矩阵。底层实现卷积时，会先把图像转换成一个用列表示的矩阵，再把卷积操作当成一个矩阵乘法来完成。其中，第一步操作叫做”im2col”。对此原理不熟的话可以参考这篇文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/63974249。">https://zhuanlan.zhihu.com/p/63974249。</a></p>
<p><code>my_conv_forward</code>函数的大部分内容都是在做类型检查和张量形状转换。在修改卷积实现时，这些东西都可以不用改。整个卷积操作的核心都在这一部分：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> elt = <span class="number">0</span>; elt &lt; batchSize / im2col_step; elt++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (isCuda)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">my_conv_im2col_cuda</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                        inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                        dilationW, im2col_step, columns);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">my_conv_im2col_cpu</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                        inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                        dilationW, im2col_step, columns);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    columns = columns.<span class="built_in">view</span>(&#123;group, columns.<span class="built_in">size</span>(<span class="number">0</span>) / group, columns.<span class="built_in">size</span>(<span class="number">1</span>)&#125;);</span><br><span class="line">    weight = weight.<span class="built_in">view</span>(&#123;group, weight.<span class="built_in">size</span>(<span class="number">0</span>) / group, weight.<span class="built_in">size</span>(<span class="number">1</span>),</span><br><span class="line">                          weight.<span class="built_in">size</span>(<span class="number">2</span>), weight.<span class="built_in">size</span>(<span class="number">3</span>)&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> g = <span class="number">0</span>; g &lt; group; g++)</span><br><span class="line">    &#123;</span><br><span class="line">        output_buffer[elt][g] = output_buffer[elt][g]</span><br><span class="line">                                    .<span class="built_in">flatten</span>(<span class="number">1</span>)</span><br><span class="line">                                    .<span class="built_in">addmm_</span>(weight[g].<span class="built_in">flatten</span>(<span class="number">1</span>), columns[g])</span><br><span class="line">                                    .<span class="built_in">view_as</span>(output_buffer[elt][g]);</span><br><span class="line">    &#125;</span><br><span class="line">    columns =</span><br><span class="line">        columns.<span class="built_in">view</span>(&#123;columns.<span class="built_in">size</span>(<span class="number">0</span>) * columns.<span class="built_in">size</span>(<span class="number">1</span>), columns.<span class="built_in">size</span>(<span class="number">2</span>)&#125;);</span><br><span class="line">    weight = weight.<span class="built_in">view</span>(&#123;weight.<span class="built_in">size</span>(<span class="number">0</span>) * weight.<span class="built_in">size</span>(<span class="number">1</span>), weight.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">                          weight.<span class="built_in">size</span>(<span class="number">3</span>), weight.<span class="built_in">size</span>(<span class="number">4</span>)&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码先做了<code>im2col</code>操作，再做了矩阵乘法。其实，包括可变形卷积在内，各种稀奇古怪的卷积操作通过靠修改<code>im2col</code>来完成的。CPU和CUDA版卷积的主要区别，也体现在<code>im2col</code>中（后面的矩阵乘法在CPU和CUDA上都能用）。</p>
<p>由于是讲CPU实现，这里的CUDA实现我暂时放了一个空函数。<code>my_conv_im2col_cpu</code>的内容如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> height_col =</span><br><span class="line">        (height + <span class="number">2</span> * pad_h - (dilation_h * (ksize_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> width_col =</span><br><span class="line">        (width + <span class="number">2</span> * pad_w - (dilation_w * (ksize_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> num_kernels = channels * height_col * width_col * parallel_imgs;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES_AND_HALF</span>(</span><br><span class="line">        data_im.<span class="built_in">scalar_type</span>(), <span class="string">&quot;&quot;</span>, [&amp;]</span><br><span class="line">        &#123; my_conv_im2col_cpu_kernel&lt;<span class="keyword">scalar_t</span>&gt;(</span><br><span class="line">              num_kernels, data_im.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;(),</span><br><span class="line">              height, width, ksize_h, ksize_w,</span><br><span class="line">              pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,</span><br><span class="line">              parallel_imgs, channels,</span><br><span class="line">              height_col, width_col, data_col.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;()); &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数其实只是处理了一下输入，真正的实现在<code>my_conv_im2col_cpu_kernel</code>里。<code>AT_DISPATCH_FLOATING_TYPES_AND_HALF</code>可以让实现兼容半精度和普通float，所以实现<code>my_conv_im2col_cpu_kernel</code>得写成一个模板函数。</p>
<p><code>my_conv_im2col_cpu_kernel</code>的实现如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> T *data_im, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> kernel_h, <span class="keyword">const</span> <span class="keyword">int</span> kernel_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> pad_w, <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> num_channels, <span class="keyword">const</span> <span class="keyword">int</span> height_col,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width_col, T *data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; n; index++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// index index of output matrix</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_col = index % width_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_col = (index / width_col) % height_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> b_col = (index / width_col / height_col) % batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_im = (index / width_col / height_col) / batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_col = c_im * kernel_h * kernel_w;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_in = h_col * stride_h - pad_h;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_in = w_col * stride_w - pad_w;</span><br><span class="line">        T *data_col_ptr =</span><br><span class="line">            data_col +</span><br><span class="line">            ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;</span><br><span class="line">        <span class="keyword">const</span> T *data_im_ptr =</span><br><span class="line">            data_im + (b_col * num_channels + c_im) * height * width;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; kernel_h; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; kernel_w; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                T val = <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>);</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> h_im = h_in + i * dilation_h;</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> w_im = w_in + j * dilation_w;</span><br><span class="line">                <span class="keyword">if</span> (h_im &gt; <span class="number">-1</span> &amp;&amp; w_im &gt; <span class="number">-1</span> &amp;&amp; h_im &lt; height &amp;&amp; w_im &lt; width)</span><br><span class="line">                &#123;</span><br><span class="line">                    val = data_im_ptr[h_im * width + w_im];</span><br><span class="line">                &#125;</span><br><span class="line">                *data_col_ptr = val;</span><br><span class="line">                data_col_ptr += batch_size * height_col * width_col;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>它的作用就是把图像里的数据搬到做卷积运算的<code>column</code>里。循环遍历每一次卷积的每一个位置，把待运算的量填入<code>column</code>。卷积里的所有参数(pad, stride, …)都是在这段函数里生效的。想实现可变形卷积等改进，也要修改这个函数。</p>
<h3 id="Python封装"><a href="#Python封装" class="headerlink" title="Python封装"></a>Python封装</h3><p>实现好了后，如果编译完了的话，刚刚的卷积接口可以通过以下方式在Python里调用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> my_ops</span><br><span class="line">my_ops.my_conv_forward(...)</span><br></pre></td></tr></table></figure>
<p>这里的<code>my_ops</code>这个名称必须和开始<code>PYBIND11_MODULE(my_ops, m)</code>里面那个库名称对应。</p>
<p>基于这个接口，可以仿照PyTorch中<code>Conv2d</code>的接口，编写一个和<code>Conv2d</code>等价的<code>torch.nn.Module</code>出来。我的这个Python文件的路径是<code>panoflow/core/op/my_conv.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.utils <span class="keyword">import</span> _pair</span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> my_ops</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyConvF</span>(<span class="params">Function</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">ctx,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="built_in">input</span>: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                weight,</span></span></span><br><span class="line"><span class="params"><span class="function">                bias,</span></span></span><br><span class="line"><span class="params"><span class="function">                stride=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                padding=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                dilation=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                groups=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                im2col_step=<span class="number">32</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">input</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">input</span>.dim() != <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&#x27;Expected 4D tensor as input, got <span class="subst">&#123;<span class="built_in">input</span>.dim()&#125;</span>D tensor \</span></span><br><span class="line"><span class="string">                  instead.&#x27;</span>)</span><br><span class="line">        ctx.stride = _pair(stride)</span><br><span class="line">        ctx.padding = _pair(padding)</span><br><span class="line">        ctx.dilation = _pair(dilation)</span><br><span class="line">        ctx.groups = groups</span><br><span class="line">        ctx.im2col_step = im2col_step</span><br><span class="line"></span><br><span class="line">        weight = weight.type_as(<span class="built_in">input</span>)</span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>, weight)</span><br><span class="line"></span><br><span class="line">        output = <span class="built_in">input</span>.new_empty(MyConvF._output_size(ctx, <span class="built_in">input</span>, weight))</span><br><span class="line"></span><br><span class="line">        ctx.bufs_ = [<span class="built_in">input</span>.new_empty(<span class="number">0</span>), <span class="built_in">input</span>.new_empty(<span class="number">0</span>)]  <span class="comment"># columns, ones</span></span><br><span class="line"></span><br><span class="line">        cur_im2col_step = <span class="built_in">min</span>(ctx.im2col_step, <span class="built_in">input</span>.size(<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">assert</span> (<span class="built_in">input</span>.size(<span class="number">0</span>) % cur_im2col_step</span><br><span class="line">                ) == <span class="number">0</span>, <span class="string">&#x27;batch size must be divisible by im2col_step&#x27;</span></span><br><span class="line"></span><br><span class="line">        my_ops.my_conv_forward(</span><br><span class="line">            <span class="built_in">input</span>,</span><br><span class="line">            weight,</span><br><span class="line">            bias,</span><br><span class="line">            output,</span><br><span class="line">            ctx.bufs_[<span class="number">0</span>],</span><br><span class="line">            kW=weight.size(<span class="number">3</span>),</span><br><span class="line">            kH=weight.size(<span class="number">2</span>),</span><br><span class="line">            dW=ctx.stride[<span class="number">1</span>],</span><br><span class="line">            dH=ctx.stride[<span class="number">0</span>],</span><br><span class="line">            padW=ctx.padding[<span class="number">1</span>],</span><br><span class="line">            padH=ctx.padding[<span class="number">0</span>],</span><br><span class="line">            dilationW=ctx.dilation[<span class="number">1</span>],</span><br><span class="line">            dilationH=ctx.dilation[<span class="number">0</span>],</span><br><span class="line">            group=ctx.groups,</span><br><span class="line">            im2col_step=cur_im2col_step)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_output_size</span>(<span class="params">ctx, <span class="built_in">input</span>, weight</span>):</span></span><br><span class="line">        channels = weight.size(<span class="number">0</span>)</span><br><span class="line">        output_size = (<span class="built_in">input</span>.size(<span class="number">0</span>), channels)</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">input</span>.dim() - <span class="number">2</span>):</span><br><span class="line">            in_size = <span class="built_in">input</span>.size(d + <span class="number">2</span>)</span><br><span class="line">            pad = ctx.padding[d]</span><br><span class="line">            kernel = ctx.dilation[d] * (weight.size(d + <span class="number">2</span>) - <span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">            stride_ = ctx.stride[d]</span><br><span class="line">            output_size += ((in_size + (<span class="number">2</span> * pad) - kernel) // stride_ + <span class="number">1</span>, )</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">all</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> s: s &gt; <span class="number">0</span>, output_size)):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&#x27;convolution input is too small (output would be &#x27;</span> +</span><br><span class="line">                <span class="string">&#x27;x&#x27;</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, output_size)) + <span class="string">&#x27;)&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> output_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">my_conv = MyConvF.apply</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyConv2d</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 in_channels: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 out_channels: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 kernel_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 stride=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 padding=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dilation=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 bias: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        kernel_size_ = _pair(kernel_size)</span><br><span class="line">        stride_ = _pair(stride)</span><br><span class="line">        padding_ = _pair(padding)</span><br><span class="line">        dilation_ = _pair(dilation)</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line">        self.kernel_size = kernel_size_</span><br><span class="line">        self.stride = stride_</span><br><span class="line">        self.padding = padding_</span><br><span class="line">        self.dilation = dilation_</span><br><span class="line">        self.groups = groups</span><br><span class="line">        self.weight = Parameter(</span><br><span class="line">            torch.Tensor(out_channels, in_channels // groups, *kernel_size_))</span><br><span class="line">        self.bias = Parameter(torch.Tensor(out_channels))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Useless attributes</span></span><br><span class="line">        self.transposed = <span class="literal">None</span></span><br><span class="line">        self.output_padding = <span class="literal">None</span></span><br><span class="line">        self.padding_mode = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="keyword">return</span> my_conv(<span class="built_in">input</span>, self.weight, self.bias, self.stride,</span><br><span class="line">                       self.padding, self.dilation, self.groups)</span><br></pre></td></tr></table></figure>
<p>以后，用自己的卷积<code>MyConv2d</code>就和用普通的<code>Conv2d</code>一样了。</p>
<h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>打开外面的<code>setup.py</code>，填写以下内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> cpp_extension</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">src_root = <span class="string">&#x27;panoflow/core/op&#x27;</span></span><br><span class="line">cpp_src = [<span class="string">&#x27;my_conv.cpp&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    include_dirs = [<span class="string">&#x27;panoflow/core/op&#x27;</span>]</span><br><span class="line">    cpp_path = [os.path.join(src_root, src) <span class="keyword">for</span> src <span class="keyword">in</span> cpp_src]</span><br><span class="line"></span><br><span class="line">    setup(</span><br><span class="line">        name=<span class="string">&#x27;panoflow&#x27;</span>,</span><br><span class="line">        ext_modules=[</span><br><span class="line">            cpp_extension.CppExtension(</span><br><span class="line">                <span class="string">&#x27;my_ops&#x27;</span>, cpp_path, include_dirs=include_dirs)</span><br><span class="line">        ],</span><br><span class="line">        cmdclass=&#123;<span class="string">&#x27;build_ext&#x27;</span>: cpp_extension.BuildExtension&#125;)</span><br></pre></td></tr></table></figure>
<p>其中的路径要根据自己的实际情况修改。</p>
<p>和编译相关的内容都写在<code>cpp_extension.CppExtension</code>里。其中，源文件要写在第二个参数里，头文件目录要写在<code>include_dirs</code>。由于我的源文件放在<code>panoflow/core/op</code>里，我写了个源文件名数组<code>cpp_src</code>，在传参前把路径组合了一下。由于<code>include_dirs</code>和源文件在同一个目录下，我也填的是<code>panoflow/core/op</code>。</p>
<p>写完了<code>setup.py</code>后，运行<code>python setup.py develop</code>，就能一键编译和安装。如果运行后没有报编译错误，就可以把实现的卷积用起来了。</p>
<h3 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h3><p>用单元测试可以快速地验证卷积是否实现成功。我写了一个简单的单元测试文件，在任意一个文件夹下创建该文件即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> panoflow.core.op.my_conv <span class="keyword">import</span> MyConv2d</span><br><span class="line"></span><br><span class="line">inc = <span class="number">3</span></span><br><span class="line">outc = <span class="number">4</span></span><br><span class="line">img_shaspe = (<span class="number">50</span>, <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># device_name = &#x27;cuda:0&#x27;</span></span><br><span class="line">device_name = <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">open_bias = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_one</span>():</span></span><br><span class="line">    ts = torch.ones([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_two</span>():</span></span><br><span class="line">    ts = torch.rand([<span class="number">1</span>, inc, *img_shaspe]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    test_one()</span><br><span class="line">    test_two()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其中，<code>panoflow.core.op.my_conv</code>是我刚刚放<code>MyConv2d</code>的Python模块。</p>
<p>直接运行这个Python文件，如果没有任何输出（报错信息），就说明卷积实现成功了。</p>
<h2 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h2><h3 id="C-实现-1"><a href="#C-实现-1" class="headerlink" title="C++实现"></a>C++实现</h3><p>在刚刚的实现中，有一个<code>my_conv_im2col_cuda</code>的实现是空着的。在CUDA版本中，我们要实现这个函数。不过，这个函数要放在一个用<code>nvcc</code>编译的<code>.cu</code>文件里。<strong>注意！注意！注意！</strong> 因此，<code>my_conv.cpp</code>里那个空的<code>my_conv_im2col_cuda</code>实现应该全部删掉。</p>
<p>新建一个文件<code>my_conv_cuda.cu</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Modify from https://github.com/open-mmlab/mmcv/blob/my_conv/mmcv/ops/csrc/common/cuda/deform_conv_cuda_kernel.cuh</span></span><br><span class="line"><span class="comment">// Copyright (c) OpenMMLab. All rights reserved.</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/types.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;pytorch_cuda_helper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">my_conv_im2col_gpu_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> T *data_im, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> kernel_h, <span class="keyword">const</span> <span class="keyword">int</span> kernel_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> pad_w, <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> num_channels, <span class="keyword">const</span> <span class="keyword">int</span> height_col,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width_col, T *data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CUDA_1D_KERNEL_LOOP</span>(index, n)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// index index of output matrix</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_col = index % width_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_col = (index / width_col) % height_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> b_col = (index / width_col / height_col) % batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_im = (index / width_col / height_col) / batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_col = c_im * kernel_h * kernel_w;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_in = h_col * stride_h - pad_h;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_in = w_col * stride_w - pad_w;</span><br><span class="line">        T *data_col_ptr =</span><br><span class="line">            data_col +</span><br><span class="line">            ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;</span><br><span class="line">        <span class="keyword">const</span> T *data_im_ptr =</span><br><span class="line">            data_im + (b_col * num_channels + c_im) * height * width;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; kernel_h; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; kernel_w; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                T val = <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>);</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> h_im = h_in + i * dilation_h;</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> w_im = w_in + j * dilation_w;</span><br><span class="line">                <span class="keyword">if</span> (h_im &gt; <span class="number">-1</span> &amp;&amp; w_im &gt; <span class="number">-1</span> &amp;&amp; h_im &lt; height &amp;&amp; w_im &lt; width)</span><br><span class="line">                &#123;</span><br><span class="line">                    val = data_im_ptr[h_im * width + w_im];</span><br><span class="line">                &#125;</span><br><span class="line">                *data_col_ptr = val;</span><br><span class="line">                data_col_ptr += batch_size * height_col * width_col;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cuda</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> height_col =</span><br><span class="line">        (height + <span class="number">2</span> * pad_h - (dilation_h * (ksize_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> width_col =</span><br><span class="line">        (width + <span class="number">2</span> * pad_w - (dilation_w * (ksize_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> num_kernels = channels * height_col * width_col * parallel_imgs;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES_AND_HALF</span>(</span><br><span class="line">        data_im.<span class="built_in">scalar_type</span>(), <span class="string">&quot;my_conv_im2col_gpu&quot;</span>, [&amp;]</span><br><span class="line">        &#123; my_conv_im2col_gpu_kernel&lt;<span class="keyword">scalar_t</span>&gt;&lt;&lt;&lt;<span class="built_in">GET_BLOCKS</span>(num_kernels),</span><br><span class="line">                                                THREADS_PER_BLOCK, <span class="number">0</span>,</span><br><span class="line">                                                at::cuda::<span class="built_in">getCurrentCUDAStream</span>()&gt;&gt;&gt;(</span><br><span class="line">              num_kernels, data_im.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;(),</span><br><span class="line">              height, width, ksize_h, ksize_w,</span><br><span class="line">              pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,</span><br><span class="line">              parallel_imgs, channels,</span><br><span class="line">              height_col, width_col, data_col.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;()); &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_CUDA_CHECK</span>(<span class="built_in">cudaGetLastError</span>());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>和CPU版的类似，<code>my_conv_im2col_cuda</code>也是预处理了输入，并调用核函数<code>my_conv_im2col_gpu_kernel</code>来实现<code>im2col</code>。</p>
<p>CUDA实现和CPU几乎一样，唯一的区别就是for循环变成了<code>CUDA_1D_KERNEL_LOOP(index, n)</code>。这个宏是头文件里帮我们定义的，它简化了CUDA的一维循环。</p>
<h3 id="编译-1"><a href="#编译-1" class="headerlink" title="编译"></a>编译</h3><p>修改<code>setup.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> cpp_extension</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">src_root = <span class="string">&#x27;panoflow/core/op&#x27;</span></span><br><span class="line">cpp_src = [<span class="string">&#x27;my_conv.cpp&#x27;</span>, <span class="string">&#x27;my_conv_cuda.cu&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    include_dirs = [<span class="string">&#x27;panoflow/core/op&#x27;</span>]</span><br><span class="line">    cpp_path = [os.path.join(src_root, src) <span class="keyword">for</span> src <span class="keyword">in</span> cpp_src]</span><br><span class="line"></span><br><span class="line">    setup(</span><br><span class="line">        name=<span class="string">&#x27;panoflow&#x27;</span>,</span><br><span class="line">        ext_modules=[</span><br><span class="line">            cpp_extension.CUDAExtension(</span><br><span class="line">                <span class="string">&#x27;my_ops&#x27;</span>, cpp_path, include_dirs=include_dirs)</span><br><span class="line">        ],</span><br><span class="line">        cmdclass=&#123;<span class="string">&#x27;build_ext&#x27;</span>: cpp_extension.BuildExtension&#125;)</span><br></pre></td></tr></table></figure>
<p>首先，要把源文件加入<code>cpp_src</code>里。之后，把<code>CppExtension</code>改成<code>CUDAExtension</code>。这样，就能编译新写的CUDA文件了。</p>
<p>写完了之后，再次<code>python setup.py develop</code>编译即可。</p>
<blockquote>
<p>编译小技巧：不拿IDE直接写C++和CUDA源代码是很容易出错误的。但如果你想只用<code>setup.py</code>来验证代码的正确性，可以<code>python setup.py develop &gt; tmp.txt</code>把编译输出重定向到一个文件里来查看。由于编译时的信息过多，在命令行里很难从一堆编译warning里找到最重要的error。</p>
</blockquote>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>由于Python部分在之前都已经写好了，可以直接用刚刚的单元测试文件测试了。只要把刚刚那份文件的<code>device_name</code>改成<code>cuda:0</code>即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> panoflow.core.op.my_conv <span class="keyword">import</span> MyConv2d</span><br><span class="line"></span><br><span class="line">inc = <span class="number">3</span></span><br><span class="line">outc = <span class="number">4</span></span><br><span class="line">img_shaspe = (<span class="number">50</span>, <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">device_name = <span class="string">&#x27;cuda:0&#x27;</span></span><br><span class="line"><span class="comment"># device_name = &#x27;cpu&#x27;</span></span><br><span class="line">open_bias = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_one</span>():</span></span><br><span class="line">    ts = torch.ones([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_two</span>():</span></span><br><span class="line">    ts = torch.rand([<span class="number">1</span>, inc, *img_shaspe]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    test_one()</span><br><span class="line">    test_two()</span><br></pre></td></tr></table></figure>
<p>同样，没报错就说明写对了。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/08/09/20220807-ResNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/09/20220807-ResNet/" class="post-title-link" itemprop="url">ResNet 论文概览与精读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-09 14:19:36" itemprop="dateCreated datePublished" datetime="2022-08-09T14:19:36+08:00">2022-08-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>ResNet是CV中的经典网络。在这篇文章中，我将按照阅读论文的通用方法由粗至精地解读这篇文章。如果你对ResNet不熟，最好对着原论文阅读本文。如果你已经很熟悉ResNet了，也可以通过这篇文章查缺补漏。</p>
<h2 id="粗读"><a href="#粗读" class="headerlink" title="粗读"></a>粗读</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>摘要的前三句话开门见山地介绍本文要解决的问题以及解决的方法。</p>
<ul>
<li>问题：<strong>较深</strong>的神经网络很难训练。</li>
<li>本文的工作：提出了一种能够轻松训练比以往的网络要深得多的残差学习框架。</li>
<li>本文方法的进一步解释：神经网络的层将拟合一个基于输入的残差函数，而不是一个没有参考的函数。</li>
</ul>
<p>随后，摘要展示了这种方法取得的成就：</p>
<ul>
<li>经实验而非理论证明，增加深度后，模型的训练效果和测试效果都得到了提升。</li>
<li>精度打败了当时所有的分类模型。</li>
<li>深度高达152，大幅超过了当时较火的19层的VGG，同时并没有增加多少计算量。</li>
<li>精度打败了当时所有的检测、分割等任务的模型。这证明这种方法的泛化性强。</li>
</ul>
<p>这篇摘要没有罗列贡献，而是在提出问题后轻轻一点，介绍了本文的方法及其作用。随后，所有的篇幅都在秀这种方法的成效。看完这段话，我们可能还不知道“残差”是怎么算的，但我们知道了这种方法很厉害，测试精度、训练难易度、泛化性都十分优秀，成功解决了较深的模型难以训练的问题。</p>
<p>在这轮粗读中，我们可以带着以下问题去概览论文：</p>
<ul>
<li>文章要解决的具体是一个怎样的问题？为什么较深的神经网络很难训练？</li>
<li>文章提出的“残差”是什么？它为什么能解决深模型难训练的问题？</li>
<li>凭什么说深模型难训练的问题被解决了？实验是怎么做的？</li>
<li>这篇文章提出的模型比其他模型好在哪？</li>
</ul>
<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>引言用十分清晰的逻辑阐明了本文的核心思想——<strong>深度残差学习</strong>框架。引言先介绍“训练更深的神经网络”这一问题的由来，再抽丝剥茧地讨论该问题的解决方法，最后自然过渡到了本文的核心思想上。看完引言，我们就应该看懂这篇文章。</p>
<p>深度卷积神经网络的有效性，得益于它的“深度”。那么，是不是层数更深的网络就更好呢？过去的实验显示：不是的。</p>
<p>更深的网络面临的第一个问题是梯度弥散/爆炸，这些问题会令网络难以收敛。但是，通过参数归一初始化和归一化层，这一问题以及得到了有效缓解。证据就是，较深的网络能够收敛。</p>
<p>这时，较深网络又暴露出了第二个问题——<strong>退化</strong>：增加网络的深度，反而会降低网络的精度。这种退化和过拟合还不同，因为退化不仅导致精度降低，还提升了模型的训练误差。</p>
<p>这种退化，是不是说明较深的网络本质上就比不过较浅的网络呢？其实并不是，退化只是因为较深的网络不是那么容易优化。考虑一个较浅的网络，我们往它的后面增加几层全等映射(y=x)。这个变深的网络与原来的网络完全等价。从这个角度看，更深的网络最少不会比更浅的网络更差才对。因此，我们要想办法让学习算法能够更好地优化更深的网络，最起码优化出一个不比较浅网络更差的网络。也就是说，我们要想办法让学习算法能够轻松学会恒等映射。</p>
<p>文章提出了一种深度残差学习框架。假设一层网络表示的映射是$H(x)$，则该层的非线性层要拟合另一个表示残差的映射$F(x) := H(x) - x$。换个角度看，就是一层网络的输出由原来的$F(x)$变成了$F(x)+x$，多加上了一个$x$。这样，只要令$F(x)=0$，网络就能轻松描述恒等映射，较深的网络最起码不比较浅的网络更差了。</p>
<p>多个数据集上的实验表明，这种方法不仅容易训练，还能得到更高的精度。在多项任务的CV竞赛中，这种方法都独占鳌头。</p>
<p>看完了引言，我们基本能知道文章在解决怎样的问题，也大致明白了残差学习的原理。接下来，我们来过一过这篇文章的实验，看看这种方法的效果究竟如何。</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>我们主要看几组ImageNet上的实验。为了方便称呼，作者把不使用残差连接的普通网络叫做“平坦(plain)”网络。第一组实验对比了不同深度的平坦网络的训练误差（细线）和验证误差（粗线）。</p>
<p><img src="/2022/08/09/20220807-ResNet/1.jpg" alt></p>
<p>可以看出，更深的网络有更高的训练误差和验证误差。作者排除了梯度问题的影响。这些网络在训练时使用了Batch Normalization (BN)，经调试，它们的梯度值也一切正常。因此，这种问题不是梯度消失导致的。引言中对于退化问题的描述得到了验证。</p>
<p>第二组实验是对残差网络ResNet的实验。我们可以把它的结果和平坦网络的放在一起对比。</p>
<p><img src="/2022/08/09/20220807-ResNet/2.jpg" alt></p>
<p>从图中可以看出，使用残差学习后，更深的网络果然有了更低的训练误差和验证误差。同时，从表中可以看出，残差网络的测试误差也降低了一大截。这说明残差学习很好地解决了前文提到的退化问题，且这种有效性在测试数据上依然能保持。</p>
<p>由于这轮粗读我们没读方法部分，和方法有关的实验结果得跳过。可以直接翻页到和其他模型的对比表格处：</p>
<p><img src="/2022/08/09/20220807-ResNet/3.jpg" alt></p>
<p>ResNet 在各个评价指标上打败了当时的其他网络，确实很厉害。</p>
<p>后面的表格显示，不仅是图像分类，ResNet在检测和分割等任务中也取得了第一名。</p>
<p>一般看到这里，一轮粗读就差不多完成了。从这轮粗读中，我们看懂了残差学习的核心思想，基本上理解了本文的核心创新点。当然，粗读深度学习相关的论文时，还可以扫一眼网络的核心模块和模型结构。精读的时候，我们再仔细理解文章的方法。</p>
<h2 id="精读"><a href="#精读" class="headerlink" title="精读"></a>精读</h2><h3 id="残差公式"><a href="#残差公式" class="headerlink" title="残差公式"></a>残差公式</h3><p>设多个层构成的模块在拟合一个映射$H(x)$，其中第一层的输入是$x$，则我们希望网络的非线性层去拟合另一个残差函数$F(x) := H(x) - x$，或者说整个模块在拟合$F(x) + x$。由于神经网络的多个非线性层能拟合任意复杂的映射，拟合一个残差函数$H(x)-x$也是可以实现的。</p>
<h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><p>有了整体的思路，我们来具体看看每一个带残差的模块是怎么搭建的。具体而言，一个模块可以由输入$x$和参数集合${W_i}$计算得到（为了简化表达，bias被省略了）:</p>
<script type="math/tex; mode=display">
y = F(x, \lbrace W_i \rbrace) +x</script><p>我们主要关心$F(x, {W_i})$是怎么搭建的。文章中给出了一种简单的双层残差块示例，其中，$x$是以短路的形式连接到输出上的：</p>
<script type="math/tex; mode=display">
F = W_2\sigma(W_1x), \sigma = ReLU</script><p><img src="/2022/08/09/20220807-ResNet/4.jpg" alt></p>
<p>注意这里的$y = W_2\sigma(W_1x)$是一个未经激活的结果。整个模块送入下一层的输出是$\sigma(y)$，还要加上激活函数。</p>
<p>残差学习是一个框架，每个残差块可以有更多层。比如本文在实验部分还测试了另一种三层残差块（下图是一个示例，实际上通道数可以任意改变）。</p>
<p><img src="/2022/08/09/20220807-ResNet/5.jpg" alt></p>
<p>多层的残差块都是可行的。但是，单层的残差块$y = W_1x + x$和线性层几乎一样，不能提升网络的有效性。</p>
<p>上述的输入$x$是直接加到激活前的输出上的。这个加法操作有一个前提：模块的输入输出通道数是一样的。在输入输出通道数不同时，可以在短路连接上加一个变换维度的线性运算$W_sx$。原来直接做加法的操作叫做全等映射，这里加上一个线性运算再做加法的操作叫做投影映射。</p>
<script type="math/tex; mode=display">
y = F(x, \lbrace W_i \rbrace) + W_sx</script><p><img src="/2022/08/09/20220807-ResNet/6.jpg" alt></p>
<p>这里的符号标记都是基于全连接层的。这些计算对卷积层来说也一样，把矩阵乘法替换成卷积即可。</p>
<h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p>本文先借鉴VGG的思路，先搭建了一个图像大小逐渐减半，深度逐渐翻倍的平坦网络。之后，在平坦网络连续的3x3卷积层上添加残差连接。下图中的实线代表普通的短路连接，虚线表示需要变换张量形状的短路连接。</p>
<p><img src="/2022/08/09/20220807-ResNet/7.jpg" alt></p>
<p>在虚线表示的残差连接中，图像的大小和深度都发生了改变。对于深度的增加，即可以使用上一节提到的投影映射，也可以直接往多出来的通道数里填0（全等映射）。对于大小的减半，无论是用怎样的深度变化方法，都可以使用步幅为2的操作来实现大小减半。</p>
<blockquote>
<p>在投影时，要进行卷积操作，卷积的步幅为2很好理解。但是，文章没有详细介绍步幅为2的全等映射是怎么做的。直观上理解，就是以步幅2跳着去输入张量里取值。</p>
</blockquote>
<p>文章还提出了层数不同的ResNet架构。对于较深的网络，文章使用了3层残差块。</p>
<p><img src="/2022/08/09/20220807-ResNet/8.jpg" alt></p>
<h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>训练的大部分配置都借鉴自AlexNet。如果你是刚入门图像识别且以后要从事这方面的研究，可以多关注这些细节。</p>
<p><strong>数据处理</strong>：</p>
<ul>
<li>随机缩放图像短边至 [256, 480] 中的均匀随机值</li>
<li>随机翻转</li>
<li>裁剪成 224x224</li>
<li>像素归一化（同AlexNet）</li>
<li>标准颜色增强（同AlexNet）</li>
</ul>
<p><strong>归一化</strong></p>
<ul>
<li>激活前使用BN</li>
<li>参数初始化使用 He Initialization</li>
</ul>
<p><strong>优化配置</strong></p>
<ul>
<li>batch size 256 的 mini-batch 梯度下降</li>
<li>学习率初始化为0.1，发现误差不动了就除以10</li>
<li>迭代$60 \times 10^4$轮</li>
<li>weight decay=0.0001, momentum=0.9</li>
<li>没有dropout</li>
</ul>
<p>此外，为了提高比赛中的精度，在测试时使用了10-crop（对图像裁剪10次，把输入分别输入网络，取结果的平均值）。同时，图像以不同的尺寸输入进了网络，结果取所有运算的平均值。</p>
<h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><p>读懂了方法，我们再来详细读一遍实验部分。</p>
<p>再看一次平坦网络和残差网络的对比。</p>
<p><img src="/2022/08/09/20220807-ResNet/2.jpg" alt></p>
<p>在这份对比实验中，残差网络相对平坦网络没有添加任何参数。当通道数改变时，残差网络用的是0填充的全等映射。这样，平坦网络和残差网络的唯一区别就是是否有短路连接，整个对比实验非常公平。实验的结果证明了残差连接的有效性。</p>
<p>这篇工作还做了另一个比较平坦网络的实验：在CIFAR-10数据集上统计了平坦网络和残差网络的3x3卷积输出的标准差。标准差大，能说明输出的数值较大。</p>
<p><img src="/2022/08/09/20220807-ResNet/9.jpg" alt></p>
<p>从这份对比结果中，我们能看出残差网络的输出标准差小于平坦网络。这符合残差学习的设计初衷：残差块至少是一个不会使性能变差的全等映射，其效果能够在全等映射的基础上优化。也因此，残差网络的输出大小会比平坦网络更靠近0。</p>
<p>看完了和平坦网络的对比，再看一看不同配置的ResNet直接的对比。文章先比较了全等映射和投影映射的短路连接。本文探讨了短路连接的3种配置：A) 全部使用全等连接 B) 有通道数变动的地方才使用投影连接 C) 全部使用投影连接。它们的表现如下：</p>
<p><img src="/2022/08/09/20220807-ResNet/10.jpg" alt></p>
<p>可以看出，投影用得越多，效果越好。作者认为，B相对A更好，是因为通道数变化时0填充的部分没有残差学习（也就是没有做$+x$的操作）；C相对B更好，是因为参数更多了。这些实验结果说明，投影连接并不能解决退化问题，出于性能的考虑，本文没有在其他实验中使用C。</p>
<blockquote>
<p>后来，B是公认的ResNet标配。</p>
</blockquote>
<p>文章还讨论了构筑更深网络的bottleneck结构。如前文所述，50层以上的ResNet在每个残差块里使用了3个1x1, 3x3, 1x3的卷积。这种设计主要是为了缩短训练时间。ResNet-50和ResNet-34有着同样的时间复杂度，却因为深度更大，精度更高。</p>
<p>这篇论文的主要实验就是这些。论文后面还展示了一个比较有趣的实验：在CIFAR-10数据集上训练超过1000层的ResNet。实验结果显示，1000多层的ResNet仍能成功被优化，训练误差也降到了很低，但是测试误差没有110层的网络好。作者认为，这是因为训练数据太少，网络过拟合了。</p>
<p><img src="/2022/08/09/20220807-ResNet/11.jpg" alt></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>ResNet是基于深度学习的计算机视觉中里程碑式的工作。通过阅读这篇论文，我们发现，ResNet使用的残差结构本身并不十分复杂。这篇工作真正出彩之处，是发现了深度神经网络的一个普遍存在的问题，并用精彩的推理设计出了一套能够解决此问题的方案。这种方案确实很有效，基于残差学习的ResNet击败了同时代的所有网络。残差连接被用在了后续几乎所有网络架构中，使用ResNet为backbone也成了其他CV任务较为常用的初始配置。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/08/09/DLS-note-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/09/DLS-note-13/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》笔记（十三）：CNN应用：人脸识别与风格迁移</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-09 14:13:53" itemprop="dateCreated datePublished" datetime="2022-08-09T14:13:53+08:00">2022-08-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在这堂课里，我们要学习两个具体的应用：人脸识别、风格迁移。</p>
<p>相信大家已经在很多地方见识过人脸识别应用了：在火车站，要通过身份证和人脸核实身份；办理业务时，可以用手机完成人脸识别以核实身份；进办公楼时，员工只要刷脸通过就可以打开门禁。通过这节课的学习，我们能够学会如何用CNN完成人脸识别。</p>
<p>神经网络风格迁移是一项有趣的应用。它利用了CNN捕捉图像抽象信息的特点，能够把一幅图像的风格转移到另一幅图像上，从而生成一幅新的图像。这项技术不需要从头训练网络，学完这门课后，我们能快速地用代码实现神经网络风格迁移。</p>
<h1 id="课堂笔记"><a href="#课堂笔记" class="headerlink" title="课堂笔记"></a>课堂笔记</h1><h2 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h2><p>准确来说，在人脸识别(Face Recognition)任务中，会给定一个有$K$个人的数据库。之后，每一次识别都会输入一张图片，输出这张图片是$K$个人中的哪一个，或者没有检测到相关人士。</p>
<p>有一个与这个相关的任务叫做人脸验证(Face Verification)。这个任务稍微简单一些，输入是一张图片和一个标记身份的数据（比如身份证号），要求输出图片中的人是否符合该身份。</p>
<h3 id="单样本学习"><a href="#单样本学习" class="headerlink" title="单样本学习"></a>单样本学习</h3><p>按我们之前学的方法，假如我们在$K$个人的数据库上做识别（分类）任务，应该套用一个CNN模型，并在模型最后接一个$K+1$类的softmax，表示输入图片是K个人中的哪一个，或者都不是。</p>
<p>但是，这样的架构不适合人脸识别任务。以公司的门禁识别为例，这种方法有如下的缺点：</p>
<ol>
<li>每来一个新同事，模型就要重新训练一次。</li>
<li>每个人都得上传大量的个人照片供网络训练。</li>
</ol>
<p>理想情况下，我们希望模型能从一张人脸照片中学会分辨这张人脸，这样每个新同事只需要上传一张照片即可。这叫做单样本学习(One-shot Learning)。</p>
<p>为了完成单样本学习，我们可以从另一个角度来建模人脸识别问题：如果输入的人脸和数据库里某张人脸极为相似，我们就说识别出了这张人脸；否则，就说没有识别到有效的人脸。</p>
<p>这样，人脸识别问题就被转换为了一个求两张图片相似度的问题。我们可以让网络学习一个输入是两张图片，输出是二者相似度的一个映射。</p>
<h3 id="孪生网络"><a href="#孪生网络" class="headerlink" title="孪生网络"></a>孪生网络</h3><p>在完成和相似度有关的问题时，一种常见的做法是使用孪生网络（Siamese Network)。</p>
<p>假设网络的倒数第二层有128个神经元。在普通分类网络中，这128个神经元输出的长度为128的向量会被输入进最后的softmax层。而在孪生网络中，我们要去掉softmax层，并用这个没有sofrmax的网络$f$分别输出两张图片$x^{(1)}, x^{(2)}$对应的128维向量$f(x^{(1)}), f(x^{(2)})$。这样，每张图片有了唯一对应的一个128维向量，这个向量可以看成是该图片的编码(encoding)。而我们又知道，对向量求相似度是很方便的。我们可以利用两张图片的编码求出相似度。</p>
<p><img src="/2022/08/09/DLS-note-13/1.jpg" alt></p>
<p>说起向量的相似度，最容易想到是向量间的距离$||v - u||^2$。因此，我们可以i设法让网络学会这样一种关系：</p>
<ul>
<li>若$x^{(i)}, x^{(j)}$是同一人，则$||f(x^{(i)}) - f(x^{(j)})||^2$很小。</li>
<li>若$x^{(i)}, x^{(j)}$不是同一人，则$||f(x^{(i)}) - f(x^{(j)})||^2$很大。、</li>
</ul>
<p>为了达成这个目标，我们来看看应该用怎样的误差来指导网络的优化方向。</p>
<h3 id="三元组误差"><a href="#三元组误差" class="headerlink" title="三元组误差"></a>三元组误差</h3><p>在让网络学习基于距离的相似度时，一种常用的误差是三元组误差(Triplet Loss)。</p>
<p>在一轮训练中，我们要用到3张图片：一张基准图(anchor)$A$和与其相对的正负样本$P, N$各一张。设$d(A, B)$为图片$A, B$的编码的距离，则我们希望$d(A, P) \leq d(A, N)$。</p>
<blockquote>
<p>左边的人是吴恩达的妻子，这几张图片已经出现了好几次。</p>
</blockquote>
<p><img src="/2022/08/09/DLS-note-13/2.jpg" alt></p>
<p>移个项，即我们希望：</p>
<script type="math/tex; mode=display">
d(A, P) - d(A, N) \leq 0.</script><p>但这个条件太容易满足了，令$f(x)=0$恒成立的话无论怎样的输入都可以使上式左侧为0了。因此，我们要加一点小小的约束：</p>
<script type="math/tex; mode=display">
d(A, P) - d(A, N) + \alpha \leq 0,</script><p>这个$\alpha$是一个较小的数，比如可以令$\alpha=0.2$。为了达成这个目标，我们可以设置以下的误差函数</p>
<script type="math/tex; mode=display">
L(A, P, N)=max(d(A, P) - d(A, N) + \alpha, 0).</script><p>这里取max的直观解释是：只要满足开始那个不等式，让正样本和负样本能够分清楚就行了。至于两类样本要不要分辨得那么分明，我们并不关心。</p>
<p>为了利用这个误差训练网络，我们要在训练集里为同一个人准备多张照片。比如1000个人，每人100张照片，共100000张照片构成训练集。</p>
<p>在提出此设计的FaceNet中，还有一些小细节：为了加大训练难度，让模型效果更好，每次训练时应该选择较难的三元组$A, P, N$。详情请阅读原论文。</p>
<h3 id="人脸验证与二分类问题"><a href="#人脸验证与二分类问题" class="headerlink" title="人脸验证与二分类问题"></a>人脸验证与二分类问题</h3><p>其实，判断两张图片的编码是否相似的问题可以简单地转化成一个二分类问题：把两张图片的编码“拼起来”，输入进一个逻辑回归的单元，让网络输入两张图片是否相似。</p>
<p>这里把两张图片“拼起来”有很多的实现方式。一种简单的方式是求两个编码的绝对值（L1距离）。</p>
<p>另外，在部署时，由于比较的图像的编码是可以预处理的，只需要用神经网络跑一遍输入图片即可。</p>
<h2 id="神经网络风格迁移"><a href="#神经网络风格迁移" class="headerlink" title="神经网络风格迁移"></a>神经网络风格迁移</h2><blockquote>
<p>我之前写了一篇<a href="https://zhouyifan.net/2022/06/01/20220531-styletransfer/">详细介绍神经网络风格迁移的文章</a>，我认为那篇文章比这堂课更好理解，非常推荐大家阅读。因此，这部分笔记我会写得潦草一些。</p>
</blockquote>
<h3 id="什么是神经网络风格迁移？"><a href="#什么是神经网络风格迁移？" class="headerlink" title="什么是神经网络风格迁移？"></a>什么是神经网络风格迁移？</h3><p><img src="/2022/08/09/DLS-note-13/3.jpg" alt></p>
<p>如上图所示，在神经网络风格迁移中，输入一张表示内容的图(C)和一张表示画家风格的图(S)，我们可以借助神经网络生成一幅融合内容与风格的图片(G)。</p>
<p>接下来实现神经网络风格迁移时，我们会关注CNN浅层和深层提取出来的特征。因此，在具体介绍风格迁移的实现之前，我们先来看看CNN各层网络究竟学到了什么。</p>
<h3 id="深度卷积网络学到了什么？"><a href="#深度卷积网络学到了什么？" class="headerlink" title="深度卷积网络学到了什么？"></a>深度卷积网络学到了什么？</h3><p>为了设法可视化神经网络每一层的输出，我们可以考虑把整个训练集喂入网络，找出使某一层某一神经元激活输出最大的几个像素块。</p>
<p>以AlexNet为例，令第一层某几个神经元激活输出最大的像素块是：</p>
<p><img src="/2022/08/09/DLS-note-13/4.jpg" alt></p>
<p>可以看出，浅层的神经网络更关注不同方向轮廓信息。</p>
<p>用同样的方式可视化更深的层，可以看出网络关注的信息越来越抽象。</p>
<p><img src="/2022/08/09/DLS-note-13/5.jpg" alt></p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>和优化一个网络的参数不同，在神经网络风格迁移中，我们要把一幅图像当成可以优化的参数，通过修改图像的像素值来最小化一个损失函数。让我们看看这个损失函数是怎么定义的。</p>
<p>损失函数由两部分构成：生成图像(G)和内容图像(C)的<strong>内容损失</strong>与和风格图像(S)的<strong>风格损失</strong>。二者之间的比例靠比例系数$\alpha, \beta$控制。</p>
<script type="math/tex; mode=display">
J(G) = \alpha J_{content}(C, G) + \beta J_{style}(S, G).</script><p>我们来看一个优化这个损失函数的步骤示例：</p>
<ol>
<li>生成一个随机图像$G: 100 \times 100 \times 3$</li>
<li>使用梯度下降更新$G$，最小化$J(G)$</li>
</ol>
<p><img src="/2022/08/09/DLS-note-13/6.jpg" alt></p>
<h3 id="内容损失"><a href="#内容损失" class="headerlink" title="内容损失"></a>内容损失</h3><p>内容损失的计算方式如下：</p>
<ol>
<li>使用一个预训练的网络（最常用的是VGG）。</li>
<li>选择神经网络中的某一层$l$，这一层相关的数据会用来计算内容损失。$l$越浅，表示越具体的图像；$l$越深，表示越抽象的图像。一般选取适中的$l$。</li>
<li>令$a^{<a href="C">l</a>}, a^{[l][G]}$为图像$C, G$网络第$l$层的激活输出。我们认为，如果这两个值很相似，则两幅图像的内容就很相似。</li>
<li>令$J_{content}(C, G) = ||a^{<a href="C">l</a>} - a^{[l][G]}||^2$</li>
</ol>
<h3 id="风格损失"><a href="#风格损失" class="headerlink" title="风格损失"></a>风格损失</h3><p>看完了内容损失，现在来看风格损失的计算方式。我们刚刚一直在用名词“风格”。这个词放在美术里，可以指画家的绘画风格。而对于神经网络来说，“风格”又是什么意思呢？</p>
<p>和刚刚的内容损失类似，计算风格损失时，我们也要选择CNN的某层$l$，计算这一层卷积激活输出的<strong>通道相关量</strong>。</p>
<p>这里“通道相关量”反映的是图像各个通道间两两的相关关系。这句话可能有点拗口，让我们看一个详细的通道相关量示例。</p>
<p><img src="/2022/08/09/DLS-note-13/7.jpg" alt></p>
<p>假设一个激活输出有5个通道，我们用颜色“红-黄-紫-绿-蓝”称呼它们。每个通道表示的是一类特征，比如红色的通道表示图像有没有竖着的条纹，黄色通道表示图像有没有橙色的色块。我们想计算红黄通道之间的相关量，其实就是计算图像每个像素处红色通道的值和绿色通道的值之间的相关关系，看看它们是会同时出现，还是一个出现了另一个就不出现。（两个数值的相关量就是它们的乘积，具体的数学表示会在后文中给出）</p>
<p>为什么这样的相关关系能够捕捉到风格信息呢？红色通道和黄色通道的相关关系，就是是不是有竖条纹的地方就有橙色色块。这样一种线条和颜色的关联，就可以代表图片的风格。（下图中红色的框和黄色的框分别圈出了两种特征）</p>
<p><img src="/2022/08/09/DLS-note-13/8.jpg" alt></p>
<p>从直觉上认识了风格，现在我们来看一看风格的具体计算方法。在计算两个向量的相关量时，可以直接求两个向量的积。因此，我们要用到一种叫做”风格矩阵“的中间量，它计算了所有通道向量的乘积：</p>
<script type="math/tex; mode=display">
G^{[l]}_{kk'}=\Sigma_i^{H} \Sigma_i^{W} a^{[l]}_{i, j, k} a^{[l]}_{i, j, k'}</script><p>其中，$a^{[l]}_{i, j, k}$是第$i$行$j$列第$k$个通道的激活输出，风格矩阵$G$的形状是$n_c^{[l]} \times n_c^{[l]}$，$G^{[l]}_{kk’}$描述的是第$l$层激活输出中，第$k, k’$个通道间的相关量。</p>
<blockquote>
<p>在数学中这个矩阵叫做gram矩阵。</p>
</blockquote>
<p>有了风格矩阵，就可以算风格损失了。为了简化表示，我们用$G^{[l]}(S), G^{[l]}(G)$分别表示风格图像S和生成图像G的风格矩阵。和刚刚一样，我们选择一层$l$，并计算风格误差：</p>
<script type="math/tex; mode=display">
J^{[l]}_{style}(S, G) = ||G^{[l]}(S)- G^{[l]}(G)||_F^2</script><p>这是一层上的风格误差。其实，我们还可以指定多个层上的风格误差，以使生成图像能够拟合风格图像在多个卷积层上（抽象程度不同）的风格。多个层的风格误差就是各层风格误差之和。</p>
<h2 id="1D和3D上的卷积"><a href="#1D和3D上的卷积" class="headerlink" title="1D和3D上的卷积"></a>1D和3D上的卷积</h2><p>在结束这门课之前，我们来学习最后一项内容——1D和3D数据上的卷积。之前，我们只关注2D的图像数据，当维度不是2时，卷积有怎样的变化呢？</p>
<p>1D数据可以是心电图。和可以用2D卷积捕捉2D图像的边缘一样，我们可以用下图中那个尖尖的1D卷积来捕获高频的数据。1D卷积前后的形状变化规律和2D一样，比如用16个长度为5的卷积卷一个$14 \times 1$的1D图像，可以得到$10 \times 16$的1D图像（第一维是图像长度，第二维是通道数）。 </p>
<p><img src="/2022/08/09/DLS-note-13/9.jpg" alt></p>
<p>3D数据也是类似的道理。用16个$5 \times 5 \times 5$的卷积核卷一个单通道$14 \times 14 \times 14$的单通道图像，可以得到一个$5 \times 5 \times 5 \times 16$的图像。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这节课主要介绍了人脸识别和神经网络风格迁移两项任务，最后顺便介绍了1D和3D上的卷积。</p>
<ul>
<li>人脸识别<ul>
<li>人脸验证与人脸识别任务的定义</li>
<li>如何对单样本学习建模</li>
<li>孪生网络</li>
<li>基于三元组误差和二分类误差的人脸识别网络</li>
</ul>
</li>
<li>神经网络风格迁移<ul>
<li>神经网络风格迁移的输入输出</li>
<li>利用小技巧可视化 CNN 学到的图像信息</li>
<li>生成风格迁移图像时的内容误差与风格误差</li>
</ul>
</li>
</ul>
<p>人脸识别任务依赖于数据集，项目搭建起来比较麻烦。对于这一周的内容，我就不展示其他的代码实战笔记了，欢迎阅读我之前写的<a href="https://zhouyifan.net/2022/06/01/20220531-styletransfer/">神经网络风格迁移的实现</a>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/08/09/DLS-note-12-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/09/DLS-note-12-2/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》代码实战（十二）：目标检测 NMS 的 Python 实现（附算法动图）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-09 14:09:41" itemprop="dateCreated datePublished" datetime="2022-08-09T14:09:41+08:00">2022-08-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在这篇文章中，我将给出一份带运行示例的NMS Python脚本，并对算法和代码进行详细解说。相信大家看完这篇文章后，能够轻松地掌握NMS的底层原理。</p>
<p>如果你对目标检测的基础知识不太熟，欢迎先阅读我的上篇文章：<a href="https://zhouyifan.net/2022/07/26/DLS-note-12/">目标检测简介</a></p>
<p>示例脚本（包括可视化的代码）链接：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/nms">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/nms</a> </p>
<h2 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h2><p>在目标检测算法中，为了尽量不漏掉物体，会输出大量的检测结果（每一条结果由检测概率与检测框组成）。这些检测结果很可能有重复的，即会有多个框标出了同一个物体。下图就是一个例子，算法正确识别出了两辆车，但却有多个检测框标出了同一辆车。</p>
<p><img src="/2022/08/09/DLS-note-12-2/1.jpg" alt></p>
<p>我们需要一个算法来过滤多余的检测框。最常用的算法就是NMS(Non-Maximum Suppresion, 非极大值抑制)。该算法的思路很简单：只保留局部概率最大的检测框，与其重合的其他检测框都会被舍去。</p>
<p>算法的伪代码如下：</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">输入所有检测概率、检测框</span><br><span class="line">当还有检测框没有处理：</span><br><span class="line">    从剩下的框里挑出检测概率最大的框 bbox_a</span><br><span class="line">    遍历所有没有处理的框bbox_i：</span><br><span class="line">        如果 bbox_i != bbox_a 且 bbox_i 与 bbox_a 重合：</span><br><span class="line">            舍去 bbox_i</span><br><span class="line">    把 bbox_a 输出成一个检测结果</span><br></pre></td></tr></table></figure>
<p>当然，这个算法的描述还不够准确：究竟该怎么定义两个检测框是“重合”呢？如果两个检测框有交集就说它们重合是行不通的，因为图片中很可能会有挨得很近的物体，它们的检测框就是相交的。因此，我们一般用IoU（交并比）来描述两个框的重合程度，如果IoU超出某个阈值，就说这两个框是“重合”的。</p>
<p>IoU的计算很简单，算出两个矩形的「交面积」，再用两个矩形面积之和减去「交面积」就可以得到「并面积」，「交面积」比「并面积」就是IoU。</p>
<p><img src="/2022/08/09/DLS-note-12-2/1.gif" alt></p>
<p>在NMS之前，一般还会先做一步预处理：对于预测概率小于某个阈值的检测框，我们不信任它们，会在进行NMS之前就把它们舍去。在代码实现时这部分逻辑常常会放到NMS的函数里。这样，整个NMS的流程是这样的：</p>
<p><img src="/2022/08/09/DLS-note-12-2/2.gif" alt></p>
<p>上述的NMS针对是识别一种物体的任务，在推广到多分类NMS时，只要把输入的检测概率换成有物体的概率乘上概率最大的类别的概率即可。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><blockquote>
<p>理论上这段代码是兼容任何格式的张量的。经测试，NumPy, PyTorch 的张量都可以正确运行。</p>
</blockquote>
<p>先看一下IoU的实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iou</span>(<span class="params">b1: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>], b2: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>]</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">    intersection = box_intersection(b1, b2)</span><br><span class="line">    inter_area = area(intersection)</span><br><span class="line">    union_area = area(b1) + area(b2) - inter_area</span><br><span class="line">    <span class="keyword">return</span> inter_area / union_area</span><br></pre></td></tr></table></figure>
<p>所有的检测框用一个长度为4的元组表示。<code>box_intersection</code>用于计算两个框的相交框，<code>area</code>用于计算框的面积。这段代码和之前描述得一样，先获取相交区域，计算交面积，再计算并面积，最后算除法。</p>
<p><code>box_intersection</code>的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">box_intersection</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        b1: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">        b2: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>]</span>) -&gt; <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>]:</span></span><br><span class="line">    x11, y11, x12, y12 = b1</span><br><span class="line">    x21, y21, x22, y22 = b2</span><br><span class="line"></span><br><span class="line">    xl = <span class="built_in">max</span>(x11, x21)</span><br><span class="line">    xr = <span class="built_in">min</span>(x12, x22)</span><br><span class="line">    yt = <span class="built_in">max</span>(y11, y21)</span><br><span class="line">    yb = <span class="built_in">min</span>(y12, y22)</span><br><span class="line">    <span class="keyword">return</span> (xl, yt, xr, yb)</span><br></pre></td></tr></table></figure>
<p>算相交区域，可以理解成分别求出x, y方向上的相交部分，再把两部分合成一个框。而求直线的相交，就是取最大的左端点和最小的右端点。</p>
<p><code>area</code>的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">area</span>(<span class="params">box: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>]</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">    x1, y1, x2, y2 = box</span><br><span class="line">    width = <span class="built_in">max</span>(x2 - x1, <span class="number">0</span>)</span><br><span class="line">    height = <span class="built_in">max</span>(y2 - y1, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> width * height</span><br></pre></td></tr></table></figure>
<p>如果两个框不相交，<code>x2 - x1</code>和<code>y2 - y1</code>会出现小于0的情况。因此，要保证它们最小为0，再算面积就不会有错了。</p>
<p>有了<code>iou</code>，就可以实现了NMS了。我的NMS函数定义如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nms</span>(<span class="params">predicts: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">        score_thresh: <span class="built_in">float</span> = <span class="number">0.6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        iou_thresh: <span class="built_in">float</span> = <span class="number">0.3</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Non-Maximum Suppression</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        predicts (np.ndarray): Tensor of shape [n, 5]. The second demesion</span></span><br><span class="line"><span class="string">            includes 1 probability and 4 numbers x, y, w, h denoting a bounding</span></span><br><span class="line"><span class="string">            box.</span></span><br><span class="line"><span class="string">        score_thresh (float): The boxes with probability lower than</span></span><br><span class="line"><span class="string">            score_threash will be discarded.</span></span><br><span class="line"><span class="string">        iou_thresh (float): The threshold determining whether two boxes are</span></span><br><span class="line"><span class="string">            &quot;overlapped&quot;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (np.ndarray, List[int]): The filtered predictions and the indices of</span></span><br><span class="line"><span class="string">            remaining boxes.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></p>
<p>NMS算法需要输入检测结果<code>predicts</code>，输出过滤后的检测结果<code>filtered_predicts</code>。此外，NMS算法有两个输入属性<code>score_thresh</code>， <code>iou_thresh</code>，分别表示被选定的检测框最小需要的概率、判断两个框是否重合的IoU阈值。为了方便其他的计算（比如多分类NMS），我还输出了一个索引数组indices，表示被选中的框的索引。这方面的逻辑可以根据自己的项目要求进行优化，没有统一的规定。</p>
<p>NMS的实现和开始的伪代码几乎一模一样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nms</span>(<span class="params">predicts: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">        score_thresh: <span class="built_in">float</span> = <span class="number">0.6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        iou_thresh: <span class="built_in">float</span> = <span class="number">0.3</span></span>):</span></span><br><span class="line">    n_remainder = <span class="built_in">len</span>(predicts)</span><br><span class="line">    vis = [<span class="literal">False</span>] * n_remainder</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Filter predicts with low probability</span></span><br><span class="line">    <span class="keyword">for</span> i, predict <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">        <span class="keyword">if</span> predict[<span class="number">0</span>] &lt; score_thresh:</span><br><span class="line">            vis[i] = <span class="literal">True</span></span><br><span class="line">            n_remainder -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># NMS</span></span><br><span class="line">    output_predicts = []</span><br><span class="line">    output_indices = []</span><br><span class="line">    <span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">        max_pro = -<span class="number">1</span></span><br><span class="line">        max_index = <span class="number">0</span></span><br><span class="line">        <span class="comment"># Find argmax</span></span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[i]:</span><br><span class="line">                <span class="keyword">if</span> max_pro &lt; p[<span class="number">0</span>]:</span><br><span class="line">                    max_index = i</span><br><span class="line">                    max_pro = p[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Append output</span></span><br><span class="line">        max_p = predicts[max_index]</span><br><span class="line">        output_predicts.append(max_p)</span><br><span class="line">        output_indices.append(max_index)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Suppress</span></span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[i] <span class="keyword">and</span> i != max_index:</span><br><span class="line">                <span class="keyword">if</span> iou(p[<span class="number">1</span>:<span class="number">5</span>], max_p[<span class="number">1</span>:<span class="number">5</span>]) &gt; iou_thresh:</span><br><span class="line">                    vis[i] = <span class="literal">True</span></span><br><span class="line">                    n_remainder -= <span class="number">1</span></span><br><span class="line">        vis[max_index] = <span class="literal">True</span></span><br><span class="line">        n_remainder -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_predicts, output_indices</span><br></pre></td></tr></table></figure>
<p>一开始，先声明一些辅助的变量。<code>n_remainder</code>表示还有多少个框没被访问，<code>vis[i]</code>表示第<code>i</code>个框是否被访问。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n_remainder = <span class="built_in">len</span>(predicts)</span><br><span class="line">vis = [<span class="literal">False</span>] * n_remainder</span><br></pre></td></tr></table></figure>
<p>一开始，先过滤那些概率过小的框：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, predict <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">    <span class="keyword">if</span> predict[<span class="number">0</span>] &lt; score_thresh:</span><br><span class="line">        vis[i] = <span class="literal">True</span></span><br><span class="line">        n_remainder -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>之后，正式进入NMS，先准备好输出的列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NMS</span></span><br><span class="line">output_predicts = []</span><br><span class="line">output_indices = []</span><br><span class="line">    <span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br></pre></td></tr></table></figure>
<p>在还有没访问的框时，先找出剩下的框中概率最大的那个框：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">    max_pro = -<span class="number">1</span></span><br><span class="line">    max_index = <span class="number">0</span></span><br><span class="line">    <span class="comment"># Find argmax</span></span><br><span class="line">    <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> vis[i]:</span><br><span class="line">            <span class="keyword">if</span> max_pro &lt; p[<span class="number">0</span>]:</span><br><span class="line">                max_index = i</span><br><span class="line">                max_pro = p[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>之后，抑制掉和概率最大框“重合”的框。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># Find argmax</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    max_p = predicts[max_index]</span><br><span class="line">    <span class="comment"># Suppress</span></span><br><span class="line">    <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> vis[i] <span class="keyword">and</span> i != max_index:</span><br><span class="line">            <span class="keyword">if</span> iou(p[<span class="number">1</span>:<span class="number">5</span>], max_p[<span class="number">1</span>:<span class="number">5</span>]) &gt; iou_thresh:</span><br><span class="line">                vis[i] = <span class="literal">True</span></span><br><span class="line">                n_remainder -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>最后，把这个结果添加进输出，并维护好<code>vis</code>和<code>n_remainder</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># Find argmax</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Suppress</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Append output</span></span><br><span class="line">    output_predicts.append(max_p)</span><br><span class="line">    output_indices.append(max_index)</span><br><span class="line">    vis[max_index] = <span class="literal">True</span></span><br><span class="line">    n_remainder -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里可以把<code>vis[max_index] = True</code>那两行移到抑制操作的前面，这样判断里就不用加<code>i != max_index</code>了。我这样写是为了强调一下，判断重合的时候不需要判断自己。</p>
</blockquote>
<p>实现完了，返回结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> output_predicts, output_indices</span><br></pre></td></tr></table></figure>
<h2 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h2><p>做单元测试的时候，最好是找一份现有的实现用做对照。为了让整个测试过程更贴合实际一些，我用MMDetection的YOLOv3跑了一个NMS前和NMS后的检测框结果，并把NMS前的检测框输入进了自己实现的NMS，比较了两份NMS的输出。以下是具体的测试过程。</p>
<p>照着MMDetection的官方文档，下载好YOLOv3模型，用下面的代码即可对MMDetection的demo图片进行推理并保存结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mmdet.apis <span class="keyword">import</span> inference_detector, init_detector, show_result_pyplot</span><br><span class="line"><span class="keyword">from</span> mmdet.models.detectors <span class="keyword">import</span> BaseDetector</span><br><span class="line"></span><br><span class="line"><span class="comment"># Choose to use a config and initialize the detector</span></span><br><span class="line">config = <span class="string">&#x27;configs/yolo/yolov3_mobilenetv2_320_300e_coco.py&#x27;</span></span><br><span class="line"><span class="comment"># Setup a checkpoint file to load</span></span><br><span class="line">checkpoint = <span class="string">&#x27;checkpoints/yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth&#x27;</span></span><br><span class="line"><span class="comment"># initialize the detector</span></span><br><span class="line">model: BaseDetector = init_detector(config, checkpoint, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"></span><br><span class="line">img = <span class="string">&#x27;demo/demo.jpg&#x27;</span></span><br><span class="line">result = inference_detector(model, img)</span><br><span class="line"></span><br><span class="line">model.show_result(img, result, score_thr=<span class="number">0.3</span>, out_file=<span class="string">&#x27;work_dirs/1.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/08/09/DLS-note-12-2/d_1.jpg" alt></p>
<p>为了得到NMS前的输入，我在<code>mmdet/models/dense_head/YOLOV3Head.get_bboxes</code>里面插入了一段输出结果的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">save_dict = &#123;</span><br><span class="line">    <span class="string">&#x27;bboxes&#x27;</span>: bboxes,</span><br><span class="line">    <span class="string">&#x27;cls_scores&#x27;</span>: scores,</span><br><span class="line">    <span class="string">&#x27;prob&#x27;</span>: objectness</span><br><span class="line">&#125;</span><br><span class="line">torch.save(save_dict, <span class="string">&#x27;work_dirs/bboxes.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">det_bboxes, det_labels = multiclass_nms(</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>得到NMS前的输入是<br><img src="/2022/08/09/DLS-note-12-2/d_2.jpg" alt></p>
<p>把这份数据输入进我们的NMS中，得到的可视化结果如下：</p>
<p><img src="/2022/08/09/DLS-note-12-2/3.jpg" alt></p>
<p>这跟开始那份输出一模一样。看来我们实现的这份NMS完全正确。</p>
<p>如果你对测试过程、可视化、多分类NMS感兴趣，欢迎直接阅读我的项目源码。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我给出了一份NMS的简洁而靠近底层的Python实现，并对NMS算法进行了介绍。通过阅读这篇文章，相信大家已经完全理解了NMS的原理，并且能够用任何一种语言实现NMS。一般的NMS开源实现支持的参数更多，代码会更复杂一些，但它们的核心和我的这份实现是一样的。</p>
<p>这份NMS的实现还有很大的改进空间。比如每轮求概率最大的框时，可以先排好序，或者用优先队列，这样均摊下来每次获取概率最大的框的复杂度是<code>O(logn)</code>。但是后面判断重复的框一定有一个<code>O(n)</code>的计算，这部分的优化并不显著。大家有余力可以参考成熟的CV项目里NMS是怎么高效用C++实现的。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/07/26/DLS-note-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/26/DLS-note-12/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》笔记（十二）：目标检测与语义分割简介 (YOLO, U-Net)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-07-26 21:00:00" itemprop="dateCreated datePublished" datetime="2022-07-26T21:00:00+08:00">2022-07-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这节课中，我们要学习计算机视觉中最重要的任务之一——目标检测任务。我们会先认识目标定位和关键点检测这两个比较简单的任务，慢慢过度到目标检测任务。之后，我们会详细学习目标检测的经典算法YOLO。最后，我们会稍微认识一下语义分割任务及适用于此问题的U-Net架构。</p>
<h1 id="课堂笔记"><a href="#课堂笔记" class="headerlink" title="课堂笔记"></a>课堂笔记</h1><h2 id="目标定位"><a href="#目标定位" class="headerlink" title="目标定位"></a>目标定位</h2><p>在<strong>图像分类问题</strong>中，给定一幅图片，我们只要说出图片里的物体是什么就行了。在这堂课要讨论的任务中，我们还要多做一件事——定位。我们要先用边框圈出图中的物体，再说出框里的物体是什么。这叫做<strong>带定位(localization)的分类问题</strong>。更进一步，如果我们不再是只讨论一个物体，而是要把图片中所有物体都框出来，并标出每一个物体的类别，这就是<strong>目标检测</strong>问题，</p>
<p><img src="/2022/07/26/DLS-note-12/1.jpg" alt></p>
<p>我们对分类任务的神经网络结构已经很熟悉了。那么，带定位的分类该使用怎样的网络呢？实际上，一个边框可以用边框中心和边框宽高这四个量表示。除了softmax出来的分类结果外，我们只要让网络再多输出四个数就行了。如下图所示：</p>
<p><img src="/2022/07/26/DLS-note-12/2.jpg" alt></p>
<p>这里，要统一一下对于边框的定义。我们用$b_x, b_y$表示边框的中心坐标，$b_h, b_w$表示边框的高、宽。</p>
<p>来看一下标签$y$的具体写法。假设一共有四类物体：行人、汽车、摩托车、背景（没有物体）。那么，标签$y$应该用$y=[p_c, b_x, b_y, b_h, b_w, c_1, c_2, c_3]^T$表示。其中，$p_c$表示图中有没有物体。若$p_c=1$，则$c_1, c_2, c_3$分别表示物体属于除背景外的哪一类；若$p_c=0$，则其他值无意义。</p>
<p>这样，在算误差时，也需要分类讨论。若$p_c=1$，则算估计值与标签8个分量两两之间的均方误差；若$p_c=0$，只算$p_c$的均方误差，不用管其他量。</p>
<p><img src="/2022/07/26/DLS-note-12/3.jpg" alt></p>
<p>只要更换一下神经网络的输出格式，我们就能得到一个完成目标定位问题的网络。</p>
<h2 id="关键点检测"><a href="#关键点检测" class="headerlink" title="关键点检测"></a>关键点检测</h2><p>我们刚刚学了用2个点表示一个边框。其实，拓展一下边框检测，就是一个关键点（英文有时叫做”landmark”，是“地标”的意思）检测问题。</p>
<p>比如，在人脸关键点检测中，我们可以定义一堆关键点，分别表示眼睛、鼻子、嘴巴……的位置。我们还是让网络先输出一个数，表示图中有没有人脸；再输出2n个数，表示n个人脸关键点。这样，网络就能学习怎么标出人脸关键点了。</p>
<p><img src="/2022/07/26/DLS-note-12/4.jpg" alt></p>
<p>很多应用都基于人脸关键点检测技术。比如我们检测到了眼睛周围的关键点后，就可以给人“戴上”墨镜。</p>
<p>总之，通过这一节的学习，我们要知道，目标定位中输出2个坐标只是关键点检测的一个特例。只要训练数据按照某种规律标出了关键点，不管这些关键点是表示一个框，还是人脸上各器官的位置，网络都能学习这种规律。</p>
<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><p>有了之前的知识储备，现在我们来正式学习目标检测。目标检测可以用一种叫做“滑动窗口”的技术实现。</p>
<p>假设我们要构建一个汽车的目标检测系统。我们可以先构造一个汽车分类数据集——数据集的x是一些等大的图片，y表示图片里是不是有汽车。如果图片里有汽车，汽车应该占据图片的大部分位置。</p>
<p><img src="/2022/07/26/DLS-note-12/5.jpg" alt></p>
<p>通过学习，网络就能够判断一个框里的物体是不是汽车了。这样，我们可以用一个边框框出图片的一部分，裁剪下来，让网络看看图片的这一部分是不是汽车。只要我们尝试的次数够多，总能找出图中的汽车。</p>
<p><img src="/2022/07/26/DLS-note-12/6.jpg" alt></p>
<p>在遍历边框时，我们是通过“滑动”的方法：遍历边框的大小，选择好大小后把框放到左上角，先往右移，再往下移。所以这种方法叫做“滑动窗口”。</p>
<p>滑动窗口算法有一个缺点：如果我们移动窗口的步伐过小，则运行分类器的次数会很多；如果移动窗口的步伐过大，则算法的精度会受到影响。在深度学习时代之前，分类器都是一些简单的线性函数，能够快速算完，多遍历一些滑动窗口没有问题。而使用了深度CNN后，遍历滑动窗口的代价就很大了。</p>
<p>幸好，滑动窗口也可以通过卷积来生成，而不一定要遍历出来。让我们看下去。</p>
<h2 id="基于卷积的滑动窗口"><a href="#基于卷积的滑动窗口" class="headerlink" title="基于卷积的滑动窗口"></a>基于卷积的滑动窗口</h2><p>滑动窗口其实可以通过执行巧妙的卷积来生成。在那之前，我们先学一门前置技能：怎么把全连接层变成卷积层。</p>
<p>前两周学习CNN时，我们学过，卷积结束后，卷积的输出会被喂入全连接层中。实际上，我们可以用卷积来等价实现全连接层。比如下图中，一个$5 \times 5 \times 16$的体积块想变成一个长度为400的向量，可以通过执行400个$5 \times 5$的卷积来实现。</p>
<p><img src="/2022/07/26/DLS-note-12/7.jpg" alt></p>
<p>知道了这一点后，我们就可以利用卷积来快速实现滑动窗口了。</p>
<p>假设我们按照上一节的算法，先实现了对$14 \times 14$的小图片进行分类的分类器。之后，我们输入了一张$16 \times 16$的大图片。我们遍历滑动窗口，令步幅为2。这样，理论上，有4个合法的滑动窗口，应该执行4次分类器的运算，如下图所示：</p>
<p><img src="/2022/07/26/DLS-note-12/8-2.jpg" alt></p>
<p>可是，仔细一想，在执行4次分类器的过程中，有很多重复的运算。比如，对于4个滑动窗口中间那共有的$12 \times 12$个像素，它们的卷积结果被算了4次。理想情况下，只需要对它们做一次卷积就行了。这该怎么优化呢？</p>
<p>其实，很简单，我们可以利用卷积本身的特性来优化。卷积层只定义了卷积核，而没有规定输入图像的大小。我们可以拿出之前在$14 \times 14$的图像上训练好的卷积层，把它们用在$16 \times 16$的图片的卷积上。经过相同的网络，$16 \times 16$的图片会生成一个$2 \times 2$大小的分类结果，而不是$1 \times 1$的。这$2 \times 2$个分类结果，恰好就是那4个滑动窗口的分类结果。通过这样巧妙地利用卷积操作，我们规避了遍历滑动窗口带来的重复计算。</p>
<p><img src="/2022/07/26/DLS-note-12/8.jpg" alt></p>
<p>不过，这个方法还是有一些缺陷的。在刚才那个例子中，$16 \times 16$的图片其实可以放下9个$14 \times 14$大小的边框。但是，由于分类网络中max pool的存在，我们只能生成4个分类结果，也就是步幅为2的滑动窗口的分类结果。同时，最准确的检测框也不一定是正方形的，而可能是长方形的。为了让生成的滑动窗口更准确一些，我们要用到其他方法。</p>
<h2 id="预测边框"><a href="#预测边框" class="headerlink" title="预测边框"></a>预测边框</h2><p>在这一节，我们要使用YOLO(You Only Look Once)算法解决上一节中碰到的问题。还记得这周课开头学的目标定位问题吗？我们可以把滑动窗口和目标定位结合一下。</p>
<p>给定一幅图像，我们可以把图像分成$3 \times 3$个格子。训练模型前，我们要对训练数据做预处理。根据每个训练样本中物体的中心点所在的格子，我们把物体分配到每一个格子中。也就是说，不管一个物体的边框跨越了几个格子，它的中心点在哪，它就属于哪个格子。比如对于下图的训练样本，右边那辆车就属于橙色的格子。之后，我们给每个格子标上标签$y$。这个标签$y$就是目标定位中那个表示图片中是否有物体、物体的边框、物体的类别的标签向量。对于这个$3 \times 3$的格子，有9个标签向量，整个标签张量的形状是$3 \times 3 \times 8$。</p>
<p><img src="/2022/07/26/DLS-note-12/9.jpg" alt></p>
<p>这样，每一幅图像的输出和标签一样，也是一个$3 \times 3 \times 8$的张量了。输入一幅图片后，我们利用上一节学的卷积滑动窗口，同时预测出每个格子里的物体边框。</p>
<p>另外，这里要详细讨论一下$b_x, b_y, b_h, b_w$的表示方法。由于我们只关心框相对于格子的位置，因此我们可以把规定一个格子的边长为1。这样，就满足$0 \leq b_x, b_y \leq 1$了。不过，由于物体的边框可以超出小框，$b_h, b_w &gt; 1$是很有可能的。</p>
<p>看到这，大家可能会有一些疑问：如果一个格子里有多个物体呢？的确，这个算法无法输出一个格子里的多个物体。一种解决方法是，我们可以把格子分得更细一点，比如$19 \times 19$个格子。这样，可以被检测到物体会多一些。但是，增加格子数又会引入一个新的问题——多个格子检测到了同一个物体。下面的两节里我们会尝试解决这个新的问题。</p>
<blockquote>
<p>吴恩达老师说，YOLO这篇论文很难读懂，他和其他几个资深研究者都花了很大的功夫才读懂这篇论文。</p>
</blockquote>
<h2 id="IoU-交并比"><a href="#IoU-交并比" class="headerlink" title="IoU(交并比)"></a>IoU(交并比)</h2><p>在目标检测中，有一个微妙的问题：框出一个物体的边框有无数个，想精确框出标签的边框是不可能的。怎么判定一个输出结果和标签里的边框“差不多”呢？这就要用到<strong>IoU(Intersection over Union，交并比)</strong> 这个概念。</p>
<p>IoU，顾名思义，二者的交集比上二者的并集，很好理解。比如下图中，网络的输出是紫框，真值是红框。二者的并集是绿色区域，交集是橙色区域。则IoU就是橙色比绿色。</p>
<p><img src="/2022/07/26/DLS-note-12/10.jpg" alt></p>
<p>依照惯例，如果IoU$\geq 0.5$，我们就认为网络的输出是正确的。当然，想更严格一点，0.6,0.7也是可以的。</p>
<blockquote>
<p>IOU 也是 “I owe you(我欠了你的钱)”的缩写，哈哈哈。</p>
</blockquote>
<h2 id="NMS-非极大值抑制"><a href="#NMS-非极大值抑制" class="headerlink" title="NMS(非极大值抑制)"></a>NMS(非极大值抑制)</h2><p>假设在YOLO中，我们用$19 \times 19$个小格来检测物体。可是，由于小格子太多了，算法得到了多个重复的检测框（以及每个框中有物体的概率）。这该怎么办呢？</p>
<p><img src="/2022/07/26/DLS-note-12/11.jpg" alt></p>
<p>NMS(Non-Maximum Suppresion，非极大值抑制)就是解决这个问题的算法。这个算法的名字听起来很奇怪，但大家理解了这个算法的实现后，就知道这个“抑制”是什么意思了。</p>
<blockquote>
<p>讲起算法我就不困了。我会抛弃视频中的讲解思路，用我自己的逻辑讲一遍。讲算法，千万不能一上来就讲算法的步骤，一定要先讲清楚算法的思路。</p>
</blockquote>
<p>在学NMS之前，我们先动动脑，看看在去掉重复的框时，我们期望得到怎样的去重输出结果。</p>
<p>首先，既然是去重，那么就不能出现两个框过度重合的情况。其次，我们希望留下来的框的预测概率尽可能大。</p>
<p>在这两个要求下，我们来看看上面那幅图的输出应该是怎样的。<br>我们一眼就能看出，对于左边那辆车，我们应该保留0.8的框；对于右边那辆车，我们应该保留0.9的框。</p>
<p>为什么我们能“一眼看出”呢？这是因为左边两个框、右边三个框恰好都分别表示了一辆车。我们能够快速地把这些框分成两类。但是，在情况比较复杂时，我们就难以快速找出最好的框了。比如下面这种情况中，两辆车很近，有些框甚至同时标出了两辆车：</p>
<p><img src="/2022/07/26/DLS-note-12/11-2.jpg" alt></p>
<p>为了处理这种复杂的情况，我们必须想出一种万全的算法，以筛选出那些概率比较大的框。</p>
<p>稍微思考一下，其实这样的算法非常简单：找出最大的框，去掉所有和它过度重合的框；在剩下的框中，找出最大的框，去掉所有和它过度重合的框；……。一直重复直到没有未处理的框为止。这就是NMS算法。</p>
<p>还是让我们来看看刚刚那个例子。使用NMS时，我们会先找到0.9这个框，“抑制”掉右边0.6和0.7的框。在剩下的框中，最大的是0.8这个框，它会“抑制”掉左边那个0.7的框。</p>
<p><img src="/2022/07/26/DLS-note-12/11-3.jpg" alt></p>
<p>接下来，让我们来严格描述一下这个算法。假设我们有$19 \times 19=361$个输出结果，每个输出结果是一个长度为5的向量$[p_c, b_x, b_y, b_h, b_w]$，分别表示有物体的概率、边框的中心和高宽（我们先不管检测多个物体的情况。事实上，当推广到多个物体时，只要往这个输出结果里多加几个概率就行了）。我们要用NMS输出应该保留的检测结果。“过度重合”，就是两个框的IoU大于0.5。</p>
<p>首先，先做一步初筛，扔掉概率$p_c$小于0.6的结果。</p>
<p>之后，对于没有遍历的框，重复执行：找出概率最大的框，把它加入输出结果；去掉所有和它IoU大于0.5的框。</p>
<p>这个过程用伪代码表示如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Input and preprocessing</span></span><br><span class="line"><span class="built_in">input</span> predicts of size [<span class="number">19</span>, <span class="number">19</span>, <span class="number">5</span>]</span><br><span class="line">resize predicts to [<span class="number">361</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Filter predicts with low probability</span></span><br><span class="line">filtered_predicts = []</span><br><span class="line"><span class="keyword">for</span> predict <span class="keyword">in</span> predicts:</span><br><span class="line">    <span class="comment"># drop p_c &lt; 0.6</span></span><br><span class="line">    <span class="keyword">if</span> predict[<span class="number">0</span>] &gt;= <span class="number">0.6</span>:  </span><br><span class="line">        filtered_predicts.append(predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># NMS</span></span><br><span class="line">n_remainder  = <span class="built_in">len</span>(filtered_predicts)</span><br><span class="line">vis = [<span class="literal">False</span>] * n_remainder <span class="comment"># False for unvisited item</span></span><br><span class="line">output_predicts = []</span><br><span class="line"><span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">    max_pro = -<span class="number">1</span></span><br><span class="line">    max_index = <span class="number">0</span></span><br><span class="line">    <span class="comment"># Find argmax</span></span><br><span class="line">    <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(filtered_predicts):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> vis[i]:</span><br><span class="line">            <span class="keyword">if</span> max_pro &lt; p[<span class="number">0</span>]:</span><br><span class="line">                max_index = i</span><br><span class="line">                max_pro = p[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Append output</span></span><br><span class="line">    max_p = filtered_predicts[max_index]</span><br><span class="line">    output_predicts.append(max_p)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Suppress</span></span><br><span class="line">    <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(filtered_predicts):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> vis[i] <span class="keyword">and</span> i != max_index:</span><br><span class="line">            <span class="keyword">if</span> get_IoU(p[<span class="number">1</span>:<span class="number">5</span>], max_p[<span class="number">1</span>:<span class="number">5</span>]) &gt; <span class="number">0.5</span>:</span><br><span class="line">                vis[i] = <span class="literal">True</span></span><br><span class="line">                n_remainder -= <span class="number">1</span></span><br><span class="line">    vis[max_index] = <span class="literal">True</span></span><br><span class="line">    n_remainder -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>假设进NMS的框有$N$个。算法里求当前最大框那一步可以用优先队列来优化，这一步复杂度是$O(logN)$。但是“抑制”那一步必须要遍历一遍剩下的框，还是有一个$O(N)$复杂度（我暂时想不出朴素的低复杂度的算法）。因此，不用优先队列优化也差不多。算上外层的循环，整个算法的复杂度是$O(N^2)$。在实际的应用中，送入NMS的结果没那么多，不会超过10000个。而且，随着框被不断过滤，外层循环的次数会减少不少。这个算法的性能瓶颈不在输入数$N$上，而在于求IoU实现的细节上。</p>
</blockquote>
<h2 id="锚框（Anchor-Boxes"><a href="#锚框（Anchor-Boxes" class="headerlink" title="锚框（Anchor Boxes)"></a>锚框（Anchor Boxes)</h2><p>为了让一个格子能够检测到多个物体，YOLO论文还提出了一种叫做锚框(Anchor Boxes)的技术。</p>
<p>假设一个格子里同时包含了两个物体：一个“竖着”的人和一个“横着”的车。那么，我们可以以这个格子的中心点为“锚”，画一个竖的框和横的框，让每个格子可以检测到两个物体。这样，人和车都能被检测了。</p>
<p><img src="/2022/07/26/DLS-note-12/12.jpg" alt></p>
<p>严谨地描述，锚框技术是这样做改进的：</p>
<ul>
<li>之前，每一个格子只能包含<strong>一个</strong>样本。训练数据中每一个标签框会被分配到<strong>它中点所在</strong>的<strong>格子</strong>。</li>
<li>现在，每一个格子能包含<strong>多个</strong>样本。每个格子都会预定义几个不同形状的锚框，有几个锚框，就最多能检测到几个物体。训练数据的每一个标签框会被分配到<strong>和它交并比最大的</strong>的<strong>锚框</strong>。</li>
</ul>
<p>注意，之前的最小单元是格子，现在是锚框，所以说现在每个样本被分配到锚框上而不是格子上。可以看下面这两个样本的例子，第一个例子是两个物体都检测到了，第二个是只有锚框2里有物体。和之前一样，如果有某个锚框里没有物体，则除了$p_c$外全填问号即可。</p>
<p><img src="/2022/07/26/DLS-note-12/13.jpg" alt></p>
<p>锚框技术实际上只是对训练数据做了一个约束，改变了训练数据的格式。检测算法本身没有什么改变。</p>
<h2 id="YOLO-算法总结"><a href="#YOLO-算法总结" class="headerlink" title="YOLO 算法总结"></a>YOLO 算法总结</h2><p>让我们把前几节的内容总结一下，看一下YOLO算法的全貌。</p>
<p>在训练前，我们要对数据做预处理。首先，我们要指定以下超参数：图片切分成多大的格子、每个格子里有多少个锚框。之后，根据这些信息，我们可以得到每一个训练标签张量的形状。比如$3 \times 3 \times 2 \times 8$的一个训练标签，表示图片被切成了$3 \times 3$的格子，每个格子有两个锚框。这是一个三分类问题，对于每一个检测出来的物体，都可以用一个长度为$8$的向量表示。其中，$p_c$表示这个锚框里有没有物体, $(b_x, b_y), (b_h, b_w)$分别表示中心点坐标、框的高宽，$c_1, c_2, c_3$分别表示是否为该类物体。</p>
<p>有了预处理好的训练数据，就可以训练一个CNN了。</p>
<p><img src="/2022/07/26/DLS-note-12/14.jpg" alt></p>
<p>在网络给出了输出后，由于输出的框往往多于标签中的框，还要对输出结果进行筛选。筛选的过程如前文所述，先去掉概率过小的框，再分别对每一类物体的框做NMS。</p>
<blockquote>
<p>课堂上没有介绍loss。loss的组成比较复杂，建议阅读原论文。</p>
</blockquote>
<h2 id="区域提案"><a href="#区域提案" class="headerlink" title="区域提案"></a>区域提案</h2><p>YOLO算法是在一堆固定的框里找物体。实际上，我们还可以用神经网络来找出候选框，再在这些框里详细检测。这种技术就叫做区域提案(region proposal)，相关的网络叫做R-CNN(Region with CNN)。</p>
<p>R-CNN 系列网络有多个改进版本：</p>
<ul>
<li>R-CNN: 使用区域提案，但是每次只对一个区域里的物体做分类。</li>
<li>Fast R-CNN: 使用区域提案，并使用基于卷积的滑动窗口加速各区域里物体的分类。</li>
<li>Faster R-CNN: 前两个算法都是用传统方法提案区域，Faster R-CNN用CNN来提案区域，进一步令算法加速。</li>
</ul>
<p>吴恩达老师认为，虽然区域提案的方法很酷，但把目标检测分两步来完成还是太麻烦了，一步到位的YOLO系列算法已经挺方便了。</p>
<h2 id="基于U-Net的语义分割"><a href="#基于U-Net的语义分割" class="headerlink" title="基于U-Net的语义分割"></a>基于U-Net的语义分割</h2><blockquote>
<p>最早这门课是没有这一节的，估计U-Net的架构太常用了，吴恩达老师把基于U-Net的语义分割加入了这周的课中。</p>
</blockquote>
<p>语义分割也是应用非常广泛的一项CV任务。相较于只把物体框出来的目标检测，语义分割会把每一类物体的每个像素都精确地标出来。如下图的示例所示，输入一张图片，语义分割会把每一类物体准确地用同一种颜色表示。</p>
<p><img src="/2022/07/26/DLS-note-12/15.jpg" alt></p>
<p>具体来说，语义分割的输出是一个单通道图片。图片的数字表示此处像素的类别。</p>
<p><img src="/2022/07/26/DLS-note-12/16.jpg" alt></p>
<p>在分类模型中，图像会越卷越小，最后压平放进全连接层并输出多个类别的分类概率。而在语义分割模型中，由于模型的输出也是一幅图像，在输入图像被卷小了以后，应该还有一个放大的过程。</p>
<p><img src="/2022/07/26/DLS-note-12/17.jpg" alt></p>
<p>目前，我们还没有学过带学习参数的可以放大图像分辨率的结构。下一节介绍的反卷积能够完成这件事。</p>
<h2 id="反卷积"><a href="#反卷积" class="headerlink" title="反卷积"></a>反卷积</h2><p><img src="/2022/07/26/DLS-note-12/1.gif" alt></p>
<p>反卷积和卷积的输入输出大小彻底相反。让我们看看反卷积的形状是怎么计算的。</p>
<p><img src="/2022/07/26/DLS-note-12/18.jpg" alt></p>
<p>如上图所示，反卷积也有卷积核大小、步幅、填充这些参数。不过这些参数都是在输出图像上做的。也就是说，我们会在输出图像上做填充，并且每次在输出图像上一步一步移动。我们把正卷积的输出大小计算公式套到反卷积上的输出上，就能算出反卷积的输入的大小。</p>
<p>在卷积时，我们是把卷积核与图像对应位置的数字乘起来，再求和，算出一个输出值；反卷积则是反了过来，把一个输入值乘到卷积核的每个位置上，再把乘法结果放再输出的对应位置上。一趟反卷积计算如下图所示：</p>
<p><img src="/2022/07/26/DLS-note-12/19.jpg" alt></p>
<p>这里我们只需要知道反卷积可以做上采样就行了，不需要纠结底层实现细节。</p>
<blockquote>
<p>本课对反卷积的介绍甚少。实际上，反卷积可以通过正卷积来实现。我扫了一圈没看到讲解得比较好的相关文章，有兴趣的可以自行查找资料。</p>
</blockquote>
<h2 id="U-Net-架构"><a href="#U-Net-架构" class="headerlink" title="U-Net 架构"></a>U-Net 架构</h2><p>学完了反卷积，可以来看U-Net的结构了。</p>
<p>U-Net除了对图像使用了先缩小再放大的卷积外，还使用了一种跳连（不是ResNet中残差连接的跳连，而是把两份输入拼接在了一起）。这样，在反卷积层中，不仅有来自上一层的输入，还有来自前面相同大小的正卷积的结果。这样做的好处是，后半部分的网络既能获得前一个卷积的抽象、高级（比如类别）的输入，又能获得前半部分网络中具体，低级的特征（比如形状）。这样，后面的层能够更好地生成输出。</p>
<p><img src="/2022/07/26/DLS-note-12/20.jpg" alt></p>
<p>U-Net具体的结构如下：</p>
<p><img src="/2022/07/26/DLS-note-12/21.jpg" alt></p>
<p>这幅图中，做运算的图像张量被表示成了一个二维矩形，矩形的高度是图像的宽高，矩形的宽度是通道数。U-Net的前半部分和常见的CNN一样，缩小图像大小，增大图像通道数。而在后半部分中，每次上采样时，一半的通道来自上一层的输出，另一半的通道来自于网络前半部分。</p>
<p>从图中能看出，U-Net的结构图是一个“U”型，因此它才被叫做U-Net。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我们主要学习了以下内容：</p>
<ul>
<li>任务定义与输出格式<ul>
<li>目标定位</li>
<li>关键点检测</li>
<li>目标检测</li>
<li>语义分割</li>
</ul>
</li>
<li>YOLO<ul>
<li>用卷积实现全连接</li>
<li>用卷积实现滑动窗口</li>
<li>锚框</li>
<li>IoU</li>
<li>NMS</li>
<li>YOLO算法</li>
</ul>
</li>
<li>U-Net<ul>
<li>反卷积</li>
<li>U-Net架构</li>
</ul>
</li>
</ul>
<p>这周的代码实战中，我会详细讲解NMS的实现。时间允许的话，我还会展示一下如何在COCO上训练YOLO。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/07/24/20220717-chinese-internet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/24/20220717-chinese-internet/" class="post-title-link" itemprop="url">从我的公众号被诬告抄袭想到的：中国互联网不配有未来</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-07-24 00:33:06" itemprop="dateCreated datePublished" datetime="2022-07-24T00:33:06+08:00">2022-07-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/" itemprop="url" rel="index"><span itemprop="name">杂谈</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/%E9%9A%8F%E7%AC%94/" itemprop="url" rel="index"><span itemprop="name">随笔</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>今天，刚收到公众号官方的两条通知，说我的文章涉嫌整合他人内容，被暂时取消原创声明功能。</p>
<p><img src="/2022/07/24/20220717-chinese-internet/1.jpg" alt></p>
<p>一开始，我还以为是系统的检测算法出了故障，赶紧低声下气地提交了申诉，说明全网上我的文章都是由我自己完成的。</p>
<p>当我回过头来阅读通知时，愕然发现了“经用户投诉且经平台审核”这几个字，一时间怒火中烧：哦，原来我是被诬告了。</p>
<p>我是两三个月前才在开始在公开媒体上创作的，对一些规则可能不熟。有些长期做自媒体的人或许会劝我：唉，这种事正常啊。只要去申诉，给你恢复就行了。而且你看，就封了两天，之后不就正常了？忍一忍就过去了。你不要玻璃心了。</p>
<p>是啊，从利益的角度来看，这件事不就是让我两天发不了“原创”的文章？两天过后一切损失都抹平了。</p>
<p>才怪呢。</p>
<p>这件事对我真正的影响，是损害了我的名誉。</p>
<p>说实话，你可以说我水平差劲，说我没钱没势，说我狂妄自大。背后说，当面讲，拿着个大喇叭对全国人民喊。我都会不以为然。</p>
<p>问题是，对于我的作品，对于我辛辛苦苦创作出来的受到了客观认可的作品，你不能诋毁它。甚至不是去挖苦文章的内容，而是拿最恶劣的抄袭来指控我。这是对我名声的侮辱，对所有有尊严的创作者的侮辱，也是你们创作平台自己的耻辱。</p>
<p>通知里说“有用户举报”。我不知道是不是真的有人举报。如果是真的，那我也奈何不了那个人。在这件事上，我是弱势的一方。我也不知道是谁干的，也没有受到什么严重的经济损失，没有任何追责的可能。可能别人就是觉得好玩，顺手按了个举报按钮呢？我除了骂一句“此人卑鄙无耻”以外，也做不了什么。</p>
<p>真正有问题的是微信公众号的官方。你们的审核人员心慵意懒，玩忽职守。手握审核大权而不知善用，身着公正之衣而不辩是非。不察之下竟把抄袭之罪强加于光明磊落的原创作者，以至于颠倒是非，污人清白，真是岂有此理！</p>
<p>你以为你们平台做起来靠的是什么？靠的是你们掌握的数以亿计的流量？别开玩笑了。给你们带来价值的，是会下金蛋的鸡。看着满棚的金鸡，几位手持饲料的奴仆倒好像也长出了翅膀，以为自己也能下出金蛋一般，觉得随手杀掉一两只鸡也无所谓。真是可笑至极。</p>
<p>我这里还要好心奉劝一下所有的创作平台，烦请你们给审核人员的评估指标中加一个错审率，加大造成冤情的惩罚。同时，在认定冤情的申诉通过后，把“对不起”三个大字好好地打在私信里。</p>
<p>仔细一想，这事也怪不了公众号平台，整个环境毕竟就是这样的。</p>
<p>每天在平台上发送的内容那么多，审核员能够把每篇文章都过一遍都实属不易，出几个纰漏也是情理之中。这些道理我肯定都懂，也可以理解。</p>
<p>但趁着这口气，我还要发表一下对于中国内容创作平台的看法。</p>
<p>以前，去网上查编程知识的时候，查出来的全是低劣的复制粘贴文章。想要搜个教程，还要跳过那万年不变的前几个网站，去后面几个搜索结果的跟帖中翻出学习资源来。想在网络中找精品资源，可谓是沙里淘金，海底捞针。</p>
<p>现在，我学有小成，想在网上分享一些学习的心得。可是，又关注者寥寥。</p>
<p>是我不会用搜索引擎吗？是我写不出好的文章吗？</p>
<p>我看，是这个互联网的运行机制有问题啊。</p>
<p>在“后来者居上”的论坛中，优秀的帖子还是会被顶起，随后贴上“精品”的标签，供后人赏读。</p>
<p>而在以推荐机制为主的封闭创作平台当道之后，本来就稀有的精品内容便沉入了泥潭之下。只推荐自己喜欢的内容，有谁不乐意呢？坐揽着源源不断的流量，那哪平台不开心呢？这就是大势所趋啊。</p>
<p>平台只知道流量，只知道赚钱。但这也没有办法。很多平台看似规模宏大，实际上，他们还烧着投资人的钱，他们自己还身陷囹圄，入不敷出。因此，他们只能想尽一切办法，赶快扩大规模，赶快收割流量，赶快盈利。然而，哪怕真有一日，他们开始盈利了，也只会在只知道赚钱的道路上转不过弯，忘记了当年平台是怎么火起来的。</p>
<p>公益性地维护一个优质的内容平台。这种看上去吃力不讨好的事情，小平台不会做，大平台也不会做。</p>
<p>按他们这样下去，中国互联网上优质的文章只会越来越少见，看不到更好的未来啊。</p>
<p>质量和金钱，真的就是互斥的关系吗？</p>
<p>我看，只是运营这些平台的人太菜了吧。</p>
<p>一来，他们过于浮躁。在指定最优化目标时，只想到了赚钱，却不知道往里面加一点点的“情怀”。</p>
<p>二来，他们水平低下。但凡掺入了一些不赚钱的因素，就觉得要运营不下去了。</p>
<p>三来，他们目光短浅。以为创造没有利润的精品是在浪费时间，实际上有内涵的事物在多年后能够带来超出金钱的价值。</p>
<p>等我有钱了，我能够把这一切都做好。</p>
<p>我知道，十多年的寒窗苦读，对多数人来讲并不是什么愉快的经历。很多时候，并不是自己没有学好，而是教育的方法有问题。这一问题在大学之后尤为突出。倘若当年能够收获一些优质的知识，也不至于会走那么多的弯路。</p>
<p>等我有钱了，我会设法建立一个吸引优秀创作者的平台，把优质的内容结合并组织起来，把名声打响，让大家都能来这里学习。我不仅要做一些“公益”的事情，我还要赚钱，我要把平台持久地运营下去。我会扶正互联网的创作风气，还互联网一个蓬勃发展的未来。</p>
<p>在这篇文章里，我也只能随口嚷嚷。诸君把这些话当作笑谈即可。不过，在当下，我还是会慢慢地行动着，创作着。</p>
<p>如果未来优秀的中文内容越来越多，说不定不再是我们计算机学生抱着一堆机械工业出版社的黑书，而是美国的教授拿着一本本从中文英化过去的参考书。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/07/24/DLS-note-11-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/24/DLS-note-11-2/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》代码实战（十一）：用 TensorFlow 实现 ResNet 并验证残差连接的有效性</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-07-24 00:31:40" itemprop="dateCreated datePublished" datetime="2022-07-24T00:31:40+08:00">2022-07-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在这篇文章中，我会介绍如何用TensorFlow实现下面4个模型：</p>
<ol>
<li>ResNet-18</li>
<li>ResNet-18 无跳连</li>
<li>ResNet-50</li>
<li>ResNet-50 无跳连</li>
</ol>
<p>实现结束后，我会在一个简单的数据集上训练这4个模型。从实验结果中，我们能直观地看出ResNet中残差连接的作用。</p>
<p>项目链接：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos">https://github.com/SingleZombie/DL-Demos</a></p>
<p>主要代码在<code>dldemos/ResNet/tf_main.py</code>这个文件里。</p>
<h2 id="模型实现"><a href="#模型实现" class="headerlink" title="模型实现"></a>模型实现</h2><h3 id="主要结构"><a href="#主要结构" class="headerlink" title="主要结构"></a>主要结构</h3><p>ResNet中有跳连的结构，直接用<code>tf.keras.Sequenctial</code>串行模型不太方便。因此，我们要自己把模型的各模块连起来，对应的TensorFlow写法是这样的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize input</span></span><br><span class="line"><span class="built_in">input</span> = layers.Input(input_shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get output</span></span><br><span class="line">output = ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build model</span></span><br><span class="line">model = models.Model(inputs=<span class="built_in">input</span>, outputs=output)</span><br><span class="line"><span class="built_in">print</span>(model.summary())</span><br><span class="line"><span class="keyword">return</span> model</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>用<code>layers.Input</code>创建一个输入张量后，就可以对这个张量进行计算，并在最后用<code>tf.keras.models.Model</code>把和该张量相关的计算图搭起来。</p>
<p>接下来，我们看看这个<code>output</code>具体是怎么算出来的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_model</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        input_shape=(<span class="params"><span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span></span>), model_name=<span class="string">&#x27;ResNet18&#x27;</span>, use_shortcut=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="comment"># Initialize input</span></span><br><span class="line">    <span class="built_in">input</span> = layers.Input(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get output</span></span><br><span class="line">    x = layers.Conv2D(<span class="number">64</span>, <span class="number">7</span>, (<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">&#x27;same&#x27;</span>)(<span class="built_in">input</span>)</span><br><span class="line">    x = layers.MaxPool2D((<span class="number">3</span>, <span class="number">3</span>), (<span class="number">2</span>, <span class="number">2</span>))(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> model_name == <span class="string">&#x27;ResNet18&#x27;</span>:</span><br><span class="line">        x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">        x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">        x = convolution_block_2(x, <span class="number">3</span>, <span class="number">128</span>, <span class="number">2</span>, use_shortcut)</span><br><span class="line">        x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">        x = convolution_block_2(x, <span class="number">3</span>, <span class="number">256</span>, <span class="number">2</span>, use_shortcut)</span><br><span class="line">        x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">        x = convolution_block_2(x, <span class="number">3</span>, <span class="number">512</span>, <span class="number">2</span>, use_shortcut)</span><br><span class="line">        x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">    <span class="keyword">elif</span> model_name == <span class="string">&#x27;ResNet50&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">block_group</span>(<span class="params">x, fs1, fs2, count</span>):</span></span><br><span class="line">            x = convolution_block_3(x, <span class="number">3</span>, fs1, fs2, <span class="number">2</span>, use_shortcut)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(count - <span class="number">1</span>):</span><br><span class="line">                x = identity_block_3(x, <span class="number">3</span>, fs1, fs2, use_shortcut)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        x = block_group(x, <span class="number">64</span>, <span class="number">256</span>, <span class="number">3</span>)</span><br><span class="line">        x = block_group(x, <span class="number">128</span>, <span class="number">512</span>, <span class="number">4</span>)</span><br><span class="line">        x = block_group(x, <span class="number">256</span>, <span class="number">1024</span>, <span class="number">6</span>)</span><br><span class="line">        x = block_group(x, <span class="number">512</span>, <span class="number">2048</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">f&#x27;No such model <span class="subst">&#123;model_name&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    x = layers.AveragePooling2D((<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">2</span>))(x)</span><br><span class="line">    x = layers.Flatten()(x)</span><br><span class="line">    output = layers.Dense(<span class="number">1</span>, <span class="string">&#x27;sigmoid&#x27;</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build model</span></span><br><span class="line">    model = models.Model(inputs=<span class="built_in">input</span>, outputs=output)</span><br><span class="line">    <span class="built_in">print</span>(model.summary())</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>构建模型时，我们需要给出输入张量的形状。同时，这个函数用<code>model_name</code>控制模型的结构，<code>use_shortcut</code>控制是否使用跳连。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_model</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        input_shape=(<span class="params"><span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span></span>), model_name=<span class="string">&#x27;ResNet18&#x27;</span>, use_shortcut=<span class="literal">True</span></span>):</span></span><br></pre></td></tr></table></figure>
<p>在ResNet中，主要有两种残差块。</p>
<p><img src="/2022/07/24/DLS-note-11-2/1.jpg" alt></p>
<p>第一种是上图中实线连接的，这种残差块的输入输出形状相同，输入可以直接加到激活函数之前的输出上；第二种是上图中虚线连接的，这种残差块输入输出形状不同，需要用一个1x1卷积调整宽高和通道数。</p>
<p>此外，每种残差块用两种实现方式。</p>
<p><img src="/2022/07/24/DLS-note-11-2/2.jpg" alt></p>
<p>第一种实现方式如上图左半部分所示，这样的残差块由两个通道数相同的3x3卷积构成，只有一个需要决定的通道数；第二种实现方式采用了瓶颈(bottlenect)结构，先用1x1卷积降低了通道数，再进行3x3卷积，共有两个要决定的通道数（第1, 2个卷积和第3个卷积的通道数），如上图右半部分所示。</p>
<p>代码中，我用<code>identity_block_2</code>, <code>identity_block_3</code>分别表示输入输出相同的残差块的两种实现，<code>convolution_block_2</code>, <code>convolution_block_3</code>分别表示输入输出不同的残差块的两种实现。这些代码会在下一小节里给出。</p>
<p>现在，我们来看看该如何用这些模块构成ResNet-18和ResNet-50。首先，我们看一看原论文中这几个ResNet的结构图。</p>
<p><img src="/2022/07/24/DLS-note-11-2/3.jpg" alt></p>
<p>对于这两种架构，它们一开始都要经过一个大卷积层和一个池化层，最后都要做一次平均池化并输入全连接层。不同之处在于中间的卷积层。ResNet-18和ResNet-50使用了实现方式不同且个数不同的卷积层组。</p>
<p>在代码中，开始的大卷积及池化是这样写的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = layers.Conv2D(<span class="number">64</span>, <span class="number">7</span>, (<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">&#x27;same&#x27;</span>)(<span class="built_in">input</span>)</span><br><span class="line">x = layers.MaxPool2D((<span class="number">3</span>, <span class="number">3</span>), (<span class="number">2</span>, <span class="number">2</span>))(x)</span><br></pre></td></tr></table></figure>
<p>ResNet-18的实现是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> model_name == <span class="string">&#x27;ResNet18&#x27;</span>:</span><br><span class="line">    x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">    x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">    x = convolution_block_2(x, <span class="number">3</span>, <span class="number">128</span>, <span class="number">2</span>, use_shortcut)</span><br><span class="line">    x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">    x = convolution_block_2(x, <span class="number">3</span>, <span class="number">256</span>, <span class="number">2</span>, use_shortcut)</span><br><span class="line">    x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">    x = convolution_block_2(x, <span class="number">3</span>, <span class="number">512</span>, <span class="number">2</span>, use_shortcut)</span><br><span class="line">    x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br></pre></td></tr></table></figure>
<p>其中，<code>identity_block_2</code>的参数分别为输入张量、卷积核边长、是否使用短路。<code>convolution_block_2</code>的参数分别为输入张量、卷积核边长、输出通道数、步幅、是否使用短路。</p>
<p>ResNet-50的实现是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> model_name == <span class="string">&#x27;ResNet50&#x27;</span>:</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">block_group</span>(<span class="params">x, fs1, fs2, count</span>):</span></span><br><span class="line">        x = convolution_block_3(x, <span class="number">3</span>, fs1, fs2, <span class="number">2</span>, use_shortcut)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(count - <span class="number">1</span>):</span><br><span class="line">            x = identity_block_3(x, <span class="number">3</span>, fs1, fs2, use_shortcut)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    x = block_group(x, <span class="number">64</span>, <span class="number">256</span>, <span class="number">3</span>)</span><br><span class="line">    x = block_group(x, <span class="number">128</span>, <span class="number">512</span>, <span class="number">4</span>)</span><br><span class="line">    x = block_group(x, <span class="number">256</span>, <span class="number">1024</span>, <span class="number">6</span>)</span><br><span class="line">    x = block_group(x, <span class="number">512</span>, <span class="number">2048</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>其中，<code>identity_block_3</code>的参数分别为输入张量、卷积核边长、中间和输出通道数、是否使用短路。<code>convolution_block_3</code>的参数分别为输入张量、卷积核边长、中间和输出通道数、步幅、是否使用短路。</p>
<p>最后是计算分类输出的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = layers.AveragePooling2D((<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">2</span>))(x)</span><br><span class="line">x = layers.Flatten()(x)</span><br><span class="line">output = layers.Dense(<span class="number">1</span>, <span class="string">&#x27;sigmoid&#x27;</span>)(x)</span><br></pre></td></tr></table></figure>
<h3 id="残差块实现"><a href="#残差块实现" class="headerlink" title="残差块实现"></a>残差块实现</h3><p><img src="/2022/07/24/DLS-note-11-2/4.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block_2</span>(<span class="params">x, f, use_shortcut=<span class="literal">True</span></span>):</span></span><br><span class="line">    _, _, _, C = x.shape</span><br><span class="line">    x_shortcut = x</span><br><span class="line">    x = layers.Conv2D(C, f, padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    x = layers.Conv2D(C, f, padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> use_shortcut:</span><br><span class="line">        x = x + x_shortcut</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><img src="/2022/07/24/DLS-note-11-2/5.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolution_block_2</span>(<span class="params">x, f, filters, s: <span class="built_in">int</span>, use_shortcut=<span class="literal">True</span></span>):</span></span><br><span class="line">    x_shortcut = x</span><br><span class="line">    x = layers.Conv2D(filters, f, strides=(s, s), padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    x = layers.Conv2D(filters, f, padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> use_shortcut:</span><br><span class="line">        x_shortcut = layers.Conv2D(filters, <span class="number">1</span>, strides=(s, s),</span><br><span class="line">                                   padding=<span class="string">&#x27;valid&#x27;</span>)(x_shortcut)</span><br><span class="line">        x_shortcut = layers.BatchNormalization(axis=<span class="number">3</span>)(x_shortcut)</span><br><span class="line">        x = x + x_shortcut</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><img src="/2022/07/24/DLS-note-11-2/6.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block_3</span>(<span class="params">x, f, filters1, filters2, use_shortcut=<span class="literal">True</span></span>):</span></span><br><span class="line">    x_shortcut = x</span><br><span class="line">    x = layers.Conv2D(filters1, <span class="number">1</span>, padding=<span class="string">&#x27;valid&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    x = layers.Conv2D(filters1, f, padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    x = layers.Conv2D(filters2, <span class="number">1</span>, padding=<span class="string">&#x27;valid&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> use_shortcut:</span><br><span class="line">        x = x + x_shortcut</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><img src="/2022/07/24/DLS-note-11-2/7.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolution_block_3</span>(<span class="params">x, f, filters1, filters2, s: <span class="built_in">int</span>, use_shortcut=<span class="literal">True</span></span>):</span></span><br><span class="line">    x_shortcut = x</span><br><span class="line">    x = layers.Conv2D(filters1, <span class="number">1</span>, strides=(s, s), padding=<span class="string">&#x27;valid&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    x = layers.Conv2D(filters1, f, padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    x = layers.Conv2D(filters2, <span class="number">1</span>, padding=<span class="string">&#x27;valid&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> use_shortcut:</span><br><span class="line">        x_shortcut = layers.Conv2D(filters2, <span class="number">1</span>, strides=(s, s),</span><br><span class="line">                                   padding=<span class="string">&#x27;same&#x27;</span>)(x_shortcut)</span><br><span class="line">        x_shortcut = layers.BatchNormalization(axis=<span class="number">3</span>)(x_shortcut)</span><br><span class="line">        x = x + x_shortcut</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这些代码中有一个细节要注意：在<code>convolution_block_3</code>中，<code>stride=2</code>是放在第一个还是第二个卷积层中没有定论。不同框架似乎对此有不同的实现方式。这里是把它放到了第一个1x1卷积里。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>在这个项目中，我已经准备好了数据集预处理的代码。可以轻松地生成数据集并用TensorFlow训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    train_X, train_Y, test_X, test_Y = get_cat_set(</span><br><span class="line">        <span class="string">&#x27;dldemos/LogisticRegression/data/archive/dataset&#x27;</span>,</span><br><span class="line">        train_size=<span class="number">500</span>,</span><br><span class="line">        test_size=<span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(train_X.shape)  <span class="comment"># (m, 224, 224, 3)</span></span><br><span class="line">    <span class="built_in">print</span>(train_Y.shape)  <span class="comment"># (m , 1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#model = init_model()</span></span><br><span class="line">    <span class="comment">#model = init_model(use_shortcut=False)</span></span><br><span class="line">    model = init_model(model_name=<span class="string">&#x27;ResNet50&#x27;</span>)</span><br><span class="line">    <span class="comment"># model = init_model(model_name=&#x27;ResNet50&#x27;, use_shortcut=False)</span></span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">                  loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">                  metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    model.fit(train_X, train_Y, epochs=<span class="number">20</span>, batch_size=<span class="number">16</span>)</span><br><span class="line">    model.evaluate(test_X, test_Y)</span><br></pre></td></tr></table></figure>
<p>为了让训练尽快结束，我只训了20个epoch，且使用的数据集比较小。我在ResNet-18中使用了3000个训练样本，ResNet-50中使用了1000个训练样本。数据的多少不影响对比结果，我们只需要知道模型的训练误差，便足以比较这四个模型了。</p>
<p>以下是我在四个实验中得到的结果。</p>
<p><strong>ResNet-18</strong><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/20</span><br><span class="line">63/63 [==============================] - 75s 1s/step - loss: 1.9463 - accuracy: 0.5485</span><br><span class="line">Epoch 2/20</span><br><span class="line">63/63 [==============================] - 71s 1s/step - loss: 0.9758 - accuracy: 0.5423</span><br><span class="line">Epoch 3/20</span><br><span class="line">63/63 [==============================] - 81s 1s/step - loss: 0.8490 - accuracy: 0.5941</span><br><span class="line">Epoch 4/20</span><br><span class="line">63/63 [==============================] - 73s 1s/step - loss: 0.8309 - accuracy: 0.6188</span><br><span class="line">Epoch 5/20</span><br><span class="line">63/63 [==============================] - 72s 1s/step - loss: 0.7375 - accuracy: 0.6402</span><br><span class="line">Epoch 6/20</span><br><span class="line">63/63 [==============================] - 77s 1s/step - loss: 0.7932 - accuracy: 0.6769</span><br><span class="line">Epoch 7/20</span><br><span class="line">63/63 [==============================] - 78s 1s/step - loss: 0.7782 - accuracy: 0.6713</span><br><span class="line">Epoch 8/20</span><br><span class="line">63/63 [==============================] - 76s 1s/step - loss: 0.6272 - accuracy: 0.7147</span><br><span class="line">Epoch 9/20</span><br><span class="line">63/63 [==============================] - 77s 1s/step - loss: 0.6303 - accuracy: 0.7059</span><br><span class="line">Epoch 10/20</span><br><span class="line">63/63 [==============================] - 74s 1s/step - loss: 0.6250 - accuracy: 0.7108</span><br><span class="line">Epoch 11/20</span><br><span class="line">63/63 [==============================] - 73s 1s/step - loss: 0.6065 - accuracy: 0.7142</span><br><span class="line">Epoch 12/20</span><br><span class="line">63/63 [==============================] - 74s 1s/step - loss: 0.5289 - accuracy: 0.7754</span><br><span class="line">Epoch 13/20</span><br><span class="line">63/63 [==============================] - 73s 1s/step - loss: 0.5005 - accuracy: 0.7506</span><br><span class="line">Epoch 14/20</span><br><span class="line">63/63 [==============================] - 73s 1s/step - loss: 0.3961 - accuracy: 0.8141</span><br><span class="line">Epoch 15/20</span><br><span class="line">63/63 [==============================] - 74s 1s/step - loss: 0.4417 - accuracy: 0.8121</span><br><span class="line">Epoch 16/20</span><br><span class="line">63/63 [==============================] - 74s 1s/step - loss: 0.3761 - accuracy: 0.8136</span><br><span class="line">Epoch 17/20</span><br><span class="line">63/63 [==============================] - 73s 1s/step - loss: 0.2764 - accuracy: 0.8809</span><br><span class="line">Epoch 18/20</span><br><span class="line">63/63 [==============================] - 71s 1s/step - loss: 0.2698 - accuracy: 0.8878</span><br><span class="line">Epoch 19/20</span><br><span class="line">63/63 [==============================] - 72s 1s/step - loss: 0.1483 - accuracy: 0.9457</span><br><span class="line">Epoch 20/20</span><br><span class="line">63/63 [==============================] - 72s 1s/step - loss: 0.2495 - accuracy: 0.9079</span><br></pre></td></tr></table></figure></p>
<p><strong>ResNet-18 无跳连</strong></p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/20</span><br><span class="line">63/63 [==============================] - 63s 963ms/step - loss: 1.4874 - accuracy: 0.5111</span><br><span class="line">Epoch 2/20</span><br><span class="line">63/63 [==============================] - 62s 990ms/step - loss: 0.7654 - accuracy: 0.5386</span><br><span class="line">Epoch 3/20</span><br><span class="line">63/63 [==============================] - 65s 1s/step - loss: 0.6799 - accuracy: 0.6210</span><br><span class="line">Epoch 4/20</span><br><span class="line">63/63 [==============================] - 62s 990ms/step - loss: 0.6891 - accuracy: 0.6086</span><br><span class="line">Epoch 5/20</span><br><span class="line">63/63 [==============================] - 65s 1s/step - loss: 0.7921 - accuracy: 0.5182</span><br><span class="line">Epoch 6/20</span><br><span class="line">63/63 [==============================] - 65s 1s/step - loss: 0.7123 - accuracy: 0.5643</span><br><span class="line">Epoch 7/20</span><br><span class="line">63/63 [==============================] - 64s 1s/step - loss: 0.7071 - accuracy: 0.5173</span><br><span class="line">Epoch 8/20</span><br><span class="line">63/63 [==============================] - 64s 1s/step - loss: 0.6653 - accuracy: 0.6227</span><br><span class="line">Epoch 9/20</span><br><span class="line">63/63 [==============================] - 65s 1s/step - loss: 0.6675 - accuracy: 0.6249</span><br><span class="line">Epoch 10/20</span><br><span class="line">63/63 [==============================] - 64s 1s/step - loss: 0.6959 - accuracy: 0.6130</span><br><span class="line">Epoch 11/20</span><br><span class="line">63/63 [==============================] - 66s 1s/step - loss: 0.6730 - accuracy: 0.6182</span><br><span class="line">Epoch 12/20</span><br><span class="line">63/63 [==============================] - 63s 1s/step - loss: 0.6321 - accuracy: 0.6491</span><br><span class="line">Epoch 13/20</span><br><span class="line">63/63 [==============================] - 63s 992ms/step - loss: 0.6413 - accuracy: 0.6569</span><br><span class="line">Epoch 14/20</span><br><span class="line">63/63 [==============================] - 63s 1s/step - loss: 0.6130 - accuracy: 0.6885</span><br><span class="line">Epoch 15/20</span><br><span class="line">63/63 [==============================] - 62s 988ms/step - loss: 0.6750 - accuracy: 0.6056</span><br><span class="line">Epoch 16/20</span><br><span class="line">63/63 [==============================] - 66s 1s/step - loss: 0.6341 - accuracy: 0.6526</span><br><span class="line">Epoch 17/20</span><br><span class="line">63/63 [==============================] - 68s 1s/step - loss: 0.6384 - accuracy: 0.6676</span><br><span class="line">Epoch 18/20</span><br><span class="line">63/63 [==============================] - 65s 1s/step - loss: 0.5750 - accuracy: 0.6997</span><br><span class="line">Epoch 19/20</span><br><span class="line">63/63 [==============================] - 63s 997ms/step - loss: 0.5932 - accuracy: 0.7094</span><br><span class="line">Epoch 20/20</span><br><span class="line">63/63 [==============================] - 62s 990ms/step - loss: 0.6133 - accuracy: 0.6420</span><br></pre></td></tr></table></figure>
<p><strong>ResNet-50</strong></p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/20</span><br><span class="line">63/63 [==============================] - 72s 1s/step - loss: 3.4660 - accuracy: 0.4970</span><br><span class="line">Epoch 2/20</span><br><span class="line">63/63 [==============================] - 67s 1s/step - loss: 1.3429 - accuracy: 0.5686</span><br><span class="line">Epoch 3/20</span><br><span class="line">63/63 [==============================] - 68s 1s/step - loss: 1.0294 - accuracy: 0.5616</span><br><span class="line">Epoch 4/20</span><br><span class="line">63/63 [==============================] - 68s 1s/step - loss: 0.7920 - accuracy: 0.6186</span><br><span class="line">Epoch 5/20</span><br><span class="line">63/63 [==============================] - 70s 1s/step - loss: 0.6698 - accuracy: 0.6773</span><br><span class="line">Epoch 6/20</span><br><span class="line">63/63 [==============================] - 70s 1s/step - loss: 0.6884 - accuracy: 0.7289</span><br><span class="line">Epoch 7/20</span><br><span class="line">63/63 [==============================] - 70s 1s/step - loss: 0.7144 - accuracy: 0.6399</span><br><span class="line">Epoch 8/20</span><br><span class="line">63/63 [==============================] - 69s 1s/step - loss: 0.7088 - accuracy: 0.6698</span><br><span class="line">Epoch 9/20</span><br><span class="line">63/63 [==============================] - 68s 1s/step - loss: 0.6385 - accuracy: 0.6446</span><br><span class="line">Epoch 10/20</span><br><span class="line">63/63 [==============================] - 69s 1s/step - loss: 0.5389 - accuracy: 0.7417</span><br><span class="line">Epoch 11/20</span><br><span class="line">63/63 [==============================] - 71s 1s/step - loss: 0.4954 - accuracy: 0.7832</span><br><span class="line">Epoch 12/20</span><br><span class="line">63/63 [==============================] - 73s 1s/step - loss: 0.4489 - accuracy: 0.7782</span><br><span class="line">Epoch 13/20</span><br><span class="line">63/63 [==============================] - 69s 1s/step - loss: 0.3987 - accuracy: 0.8257</span><br><span class="line">Epoch 14/20</span><br><span class="line">63/63 [==============================] - 72s 1s/step - loss: 0.3228 - accuracy: 0.8519</span><br><span class="line">Epoch 15/20</span><br><span class="line">63/63 [==============================] - 70s 1s/step - loss: 0.2089 - accuracy: 0.9235</span><br><span class="line">Epoch 16/20</span><br><span class="line">63/63 [==============================] - 69s 1s/step - loss: 0.4766 - accuracy: 0.7756</span><br><span class="line">Epoch 17/20</span><br><span class="line">63/63 [==============================] - 75s 1s/step - loss: 0.2148 - accuracy: 0.9181</span><br><span class="line">Epoch 18/20</span><br><span class="line">63/63 [==============================] - 70s 1s/step - loss: 0.3086 - accuracy: 0.8623</span><br><span class="line">Epoch 19/20</span><br><span class="line">63/63 [==============================] - 69s 1s/step - loss: 0.3544 - accuracy: 0.8732</span><br><span class="line">Epoch 20/20</span><br><span class="line">63/63 [==============================] - 70s 1s/step - loss: 0.0796 - accuracy: 0.9704</span><br></pre></td></tr></table></figure>
<p><strong>ResNet-50 无跳连</strong></p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/20</span><br><span class="line">63/63 [==============================] - 60s 882ms/step - loss: 1.2093 - accuracy: 0.5034</span><br><span class="line">Epoch 2/20</span><br><span class="line">63/63 [==============================] - 56s 892ms/step - loss: 0.8433 - accuracy: 0.4861</span><br><span class="line">Epoch 3/20</span><br><span class="line">63/63 [==============================] - 59s 931ms/step - loss: 0.7512 - accuracy: 0.5235</span><br><span class="line">Epoch 4/20</span><br><span class="line">63/63 [==============================] - 62s 991ms/step - loss: 0.7395 - accuracy: 0.4887</span><br><span class="line">Epoch 5/20</span><br><span class="line">63/63 [==============================] - 62s 990ms/step - loss: 0.7770 - accuracy: 0.5316</span><br><span class="line">Epoch 6/20</span><br><span class="line">63/63 [==============================] - 60s 945ms/step - loss: 0.7408 - accuracy: 0.4947</span><br><span class="line">Epoch 7/20</span><br><span class="line">63/63 [==============================] - 67s 1s/step - loss: 0.7345 - accuracy: 0.5434</span><br><span class="line">Epoch 8/20</span><br><span class="line">63/63 [==============================] - 62s 984ms/step - loss: 0.7214 - accuracy: 0.5605</span><br><span class="line">Epoch 9/20</span><br><span class="line">63/63 [==============================] - 60s 950ms/step - loss: 0.7770 - accuracy: 0.4784</span><br><span class="line">Epoch 10/20</span><br><span class="line">63/63 [==============================] - 60s 956ms/step - loss: 0.7171 - accuracy: 0.5203</span><br><span class="line">Epoch 11/20</span><br><span class="line">63/63 [==============================] - 63s 994ms/step - loss: 0.7045 - accuracy: 0.4921</span><br><span class="line">Epoch 12/20</span><br><span class="line">63/63 [==============================] - 63s 1s/step - loss: 0.6884 - accuracy: 0.5430</span><br><span class="line">Epoch 13/20</span><br><span class="line">63/63 [==============================] - 60s 958ms/step - loss: 0.7333 - accuracy: 0.5278</span><br><span class="line">Epoch 14/20</span><br><span class="line">63/63 [==============================] - 61s 966ms/step - loss: 0.7050 - accuracy: 0.5106</span><br><span class="line">Epoch 15/20</span><br><span class="line">63/63 [==============================] - 59s 943ms/step - loss: 0.6958 - accuracy: 0.5622</span><br><span class="line">Epoch 16/20</span><br><span class="line">63/63 [==============================] - 60s 954ms/step - loss: 0.7398 - accuracy: 0.5172</span><br><span class="line">Epoch 17/20</span><br><span class="line">63/63 [==============================] - 69s 1s/step - loss: 0.7104 - accuracy: 0.5023</span><br><span class="line">Epoch 18/20</span><br><span class="line">63/63 [==============================] - 74s 1s/step - loss: 0.7411 - accuracy: 0.4747</span><br><span class="line">Epoch 19/20</span><br><span class="line">63/63 [==============================] - 67s 1s/step - loss: 0.7056 - accuracy: 0.4706</span><br><span class="line">Epoch 20/20</span><br><span class="line">63/63 [==============================] - 81s 1s/step - loss: 0.7901 - accuracy: 0.4898</span><br></pre></td></tr></table></figure>
<p>对比ResNet-18和ResNet-50，可以看出，ResNet-50的拟合能力确实更强一些。</p>
<p>对比无跳连的ResNet-18和ResNet-50，可以看出，ResNet-50的拟合能力反而逊于ResNet-18。这符合ResNet的初衷，如果不加残差连接的话，过深的网络反而会因为梯度问题而有更高的训练误差。</p>
<p>此外，不同模型的训练速度也值得一讲。在训练数据量减少到原来的1/3后，ResNet-50和ResNet-18的训练速度差不多。ResNet-50看上去比ResNet-18多了很多层，网络中间也使用了通道数很大的卷积，但整体的参数量并没有增大多少，这多亏了能降低运算量的瓶颈结构。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我展示了ResNet-18和ResNet-50的TensorFlow实现。这份代码包括了经典ResNet中两种残差块的两种实现，完整地复现了原论文的模型模块。同时，经实验分析，我验证了ResNet残差连接的有效性。</p>
<p>未来我还会写一篇ResNet的PyTorch实现，并附上论文的详细解读。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/07/24/DLS-note-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/24/DLS-note-11/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》笔记（十一）：深度卷积模型——从示例中学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-07-24 00:31:30" itemprop="dateCreated datePublished" datetime="2022-07-24T00:31:30+08:00">2022-07-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="学习提示"><a href="#学习提示" class="headerlink" title="学习提示"></a>学习提示</h1><p>上周，我们学完了CNN的基础组成模块。而从这周开始，我们要换一种学习方式：我们会认识一些经典的CNN架构，从示例中学习。一方面来说，通过了解他人的网络，阅读他人的代码，我们能够更快地掌握如何整合CNN的基础模块；另一方面，CNN架构往往泛化能力较强，学会了其他任务中成熟的架构，可以把这些架构直接用到我们自己的任务中。</p>
<p>接下来，我们会按照CNN的发展历史，认识许多CNN架构。首先是经典网络：</p>
<ul>
<li>LeNet-5</li>
<li>AlexNet</li>
<li>VGG</li>
</ul>
<p>之后是近年来的一些网络：</p>
<ul>
<li>ResNet</li>
<li>Inception</li>
<li>MobileNet</li>
</ul>
<p>我们不会把这些研究的论文详细过一遍，而只会学习各研究中最精华的部分。学有余力的话，最好能在课后把论文自己过一遍。</p>
<h1 id="课堂笔记"><a href="#课堂笔记" class="headerlink" title="课堂笔记"></a>课堂笔记</h1><h2 id="经典网络"><a href="#经典网络" class="headerlink" title="经典网络"></a>经典网络</h2><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p>LeNet-5是用于手写数字识别（识别0~9的阿拉伯数字）的网络。它的结构如下：</p>
<p><img src="/2022/07/24/DLS-note-11/1.jpg" alt></p>
<p>网络是输入是一张[32, 32, 1]的灰度图像，输入经过4个卷积+池化层，再经过两个全连接层，输出一个0~9的数字。这个网络和我们上周见过的网络十分相似，数据体的宽和高在不断变小，而通道数在不断变多。</p>
<p>这篇工作是1998年发表的，当时的神经网络架构和现在我们学的有不少区别：</p>
<ul>
<li>当时padding还没有得到广泛使用，数据体的分辨率会越降越小。</li>
<li>当时主要使用平均池化，而现在最大池化更常见。</li>
<li>网络只输出一个值，表示识别出来的数字。而现在的多分类任务一般会输出10个值并使用softmax激活函数。</li>
<li>当时激活函数只用sigmoid和tanh，没有人用ReLU。</li>
<li>当时的算力没有现在这么强，原工作在计算每个通道卷积时使用了很多复杂的小技巧。而现在我们直接算就行了。</li>
</ul>
<p>LeNet-5只有6万个参数。随着算力的增长，后来的网络越来越大了。</p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>AlexNet是2012年发表的有关图像分类的CNN结构。它的输入是[227, 227, 3]的图像，输出是一个1000类的分类结果。</p>
<blockquote>
<p>原论文里写的是输入形状是[224, 224, 3]，但实际上这个分辨率是有问题的，按照这个分辨率是算不出后续结果的分辨率的。但现在一些框架对AlexNet的复现中，还是会令输入的分辨率是224。这是因为框架在第一层卷积中加了一个padding的操作，强行让后续数据的分辨率和原论文对上了。 </p>
</blockquote>
<p><img src="/2022/07/24/DLS-note-11/2.jpg" alt></p>
<p>AlexNet和LeNet-5在架构上十分接近。但是，AlexNet做出了以下改进：</p>
<ul>
<li>AlexNet用了更多的参数，一共有约6000万个参数。</li>
<li>使用ReLU作为激活函数。</li>
</ul>
<p>AlexNet还提出了其他一些创新，但与我们要学的知识没有那么多关系：</p>
<ul>
<li>当时算力还是比较紧张，AlexNet用了双GPU训练。论文里写了很多相关的工程细节。</li>
<li>使用了Local Response Normalization这种归一化层。现在几乎没人用这种归一化。</li>
</ul>
<p>AlexNet中的一些技术在今天看来，已经是常识般的存在。而在那个年代，尽管深度学习在语音识别等任务上已经初露锋芒，人们还没有开始重视深度学习这项技术。正是由于AlexNet这一篇工作的出现，计算机视觉的研究者开始关注起了深度学习。甚至在后来，这篇工作的影响力已经远超出了计算机视觉社区。</p>
<h3 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h3><p>VGG-16也是一个图像分类网络。VGG的出发点是：为了简化网络结构，只用3x3等长(same)卷积和2x2最大池化。</p>
<p><img src="/2022/07/24/DLS-note-11/3.jpg" alt></p>
<p>可以看出，VGG也是经过了一系列的卷积和池化层，最后使用全连接层和softmax输出结果。顺带一提，VGG-16里的16表示有16个带参数的层。</p>
<p>VGG非常庞大，有138M(1.38亿)个参数。但是它简洁的结构吸引了很多人的关注。</p>
<p>吴恩达老师鼓励大家去读一读这三篇论文。可以先看AlexNet，再看VGG。LeNet有点难读，可以放到最后去读。</p>
<h2 id="ResNets（基于残差的网络）"><a href="#ResNets（基于残差的网络）" class="headerlink" title="ResNets（基于残差的网络）"></a>ResNets（基于残差的网络）</h2><p>非常非常深的神经网络是很难训练的，这主要是由梯度爆炸/弥散问题导致的。在这一节中，我们要学一种叫做“跳连(skip connection)”的网络模块连接方式。使用跳连，我们能让浅层模块的输出直接对接到深层模块的输入上，进而搭建基于残差的网络，解决梯度爆炸/弥散问题，训练深达100层的网络。</p>
<h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><p>回忆一下，在全连接网络中，假如我们有中间层的输出$a^{[l]}, a^{[l+2]}$，$a^{[l+2]}$是怎么由$a^{[l]}$算出来的呢？我们之前用的公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
z^{[l+1]}&=W^{[l+1]}a^{[l]}+b^{[l+1]} \\
a^{[l+1]}&=g(z^{[l+1]}) \\
z^{[l+2]}&=W^{[l+2]}a^{[l+1]}+b^{[l+2]} \\
a^{[l+2]}&=g(z^{[l+2]}) \\
\end{aligned}</script><p>也就是说，$a^{[l]}$要经过一个线性层、一个激活函数、一个线性层、一个激活函数，才能传递到$a^{[l+2]}$，这条路径非常长：</p>
<p><img src="/2022/07/24/DLS-note-11/4.jpg" alt></p>
<p>而在残差块(Residual block)中，我们使用了一种新的连接方法：</p>
<p><img src="/2022/07/24/DLS-note-11/5.jpg" alt></p>
<p>$a^{[l]}$的值被直接加到了第二个ReLU层之前的线性输出上，这是一种类似电路中短路的连接方法（又称跳连）。这样，浅层的信息能更好地传到深层了。</p>
<p>使用这种方法后，计算公式变更为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
z^{[l+1]}&=W^{[l+1]}a^{[l]}+b^{[l+1]} \\
a^{[l+1]}&=g(z^{[l+1]}) \\
z^{[l+2]}&=W^{[l+2]}a^{[l+1]}+b^{[l+2]} \\
a^{[l+2]}&=g(z^{[l+2]}+a^{[l]}) \\
\end{aligned}</script><p>残差块中还有一个要注意的细节。$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$这个式子能够成立，实际上是默认了$a^{[l+2]}, a^{[l]}$的维度相同。而一旦$a^{[l+2]}$的维度发生了变化，就需要用下面这种方式来调整了。</p>
<p>$a^{[l+2]}=g(z^{[l+2]}+W’a^{[l]})$</p>
<p>我们可以用一个$W’$来完成维度的转换。为了方便理解，我们先让所有$a$都是一维向量，$W’$是矩阵。这样，假设$a^{[l+2]}$的长度是256，$a^{[l]}$的长度是128，则$W’$的形状就是$256 \times 128$。</p>
<p>但实际上，$a$是一个三维的图像张量，三个维度的长度都可能发生变化。因此，对于图像，上式中的$W’$应该表示的是一个卷积操作。通过卷积操作，我们能够减小图像的宽高，调整图像的通道数，使得$a^{[l]}$和$a^{[l+2]}$的维度完全相同。</p>
<h3 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h3><p>在构建残差网络ResNet时，只要把这种残差块一个一个拼接起来即可。或者从另一个角度来看，对于一个“平坦网络”（”plain network”, ResNet论文中用的词，用于表示非残差网络），我们只要把线性层两两打包，添加跳连即可。</p>
<p><img src="/2022/07/24/DLS-note-11/6.jpg" alt></p>
<p>残差块起到了什么作用呢？让我们看看在网络层数变多时，平坦网络和残差网络训练误差的变化趋势：</p>
<p><img src="/2022/07/24/DLS-note-11/7.jpg" alt></p>
<p>理论上来说，层数越深，训练误差应该越低。但在实际中，对平坦网络增加深度，反而会让误差变高。而使用ResNet后，随着深度增加，训练误差起码不会降低了。</p>
<p>正是有这样的特性，我们可以用ResNet架构去训练非常深的网络。</p>
<p>为什么ResNet是有这样的特性呢？我们还是从刚刚那个ResNet的公式里找答案。</p>
<p>假设我们设计好了一个网络，又给它新加了一个残差块，即多加了两个卷积层，那么最后的输出可以写成：</p>
<script type="math/tex; mode=display">
a^{[l+2]}=g(z^{[l+2]}+a^{[l]}),</script><p>即</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{[l+2]}&=g(z^{[l+2]}+a^{[l]}) \\
a^{[l+2]}&=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]}) \\
\end{aligned}</script><p>由于正则化的存在，所有$W$和$b$都倾向于变得更小。极端情况下，$W, b$都变为0了。那么，</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{[l+2]}&=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]}), \\
a^{[l+2]}&=g(a^{[l]}). \\
\end{aligned}</script><p>再不妨设$g=ReLU$。则因为$a^{[l]}$也是ReLU的输出，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{[l+2]}&=g(a^{[l]}), \\
a^{[l+2]}&=a^{[l]}. \\
\end{aligned}</script><p>这其实是一个恒等映射，也就是说，新加的残差块对之前的输出没有任何影响。网络非常容易学习到恒等映射。这样，最起码能够保证较深的网络不比浅的网络差。</p>
<p>准备好了所有基础知识，我们来看看完整的ResNet长什么样。</p>
<p><img src="/2022/07/24/DLS-note-11/7_2.jpg" alt></p>
<p>ResNet有几个参数量不同的版本。这里展示的叫做ResNet-34。完整的网络很长，我们只用关注其中一小部分就行了。</p>
<p>一开始，网络还是用一个大卷积核大步幅的卷积以及一个池化操作快速降低图像的宽度，再把数据传入残差块中。和我们刚刚学的一样，残差块有两种，一种是维度相同可以直接相加的（实线），一种是要调整维度的（虚线）。整个网络就是由这若干个这样的残差块组构成。经过所有残差块后，还是和经典的网络一样，用全连接层输出结果。</p>
<p>这里，我们只学习了残差连接的基本原理。ResNet的论文里还有更多有关网络结构、实验的细节。最好能读一读论文。当然，这周的编程实战里我们也会复现ResNet，以加深对其的理解。</p>
<h2 id="Inception-网络"><a href="#Inception-网络" class="headerlink" title="Inception 网络"></a>Inception 网络</h2><p>我们已经见过不少CNN的示例了。当我们仿照它们设计自己的网络时，或许会感到迷茫：有3x3, 5x5卷积，有池化，该怎么选择每一个模块呢？Inception网络给了一个解决此问题的答案：我全都要。</p>
<p>Inception网络用到了一种特殊的1x1卷积。我们会先学习1x1卷积，再学习Inception网络的知识。</p>
<h3 id="1x1卷积"><a href="#1x1卷积" class="headerlink" title="1x1卷积"></a>1x1卷积</h3><p>用1x1的卷积核去卷一幅图像，似乎是一件很滑稽的事情。假设一幅图像的数字是[1, 2, 3]，卷积核是[2]，那么卷出来的图像就是[2, 4, 6]。这不就是把每个数都做了一次乘法吗？</p>
<p>对于通道数为1的图像，1x1卷积确实没什么大用。而当通道数多起来后，1x1卷积的意义就逐渐显现出来了。思考一下，对多通道的图像做1x1卷积，就是把某像素所有通道的数字各乘一个数，求和，加一个bias，再通过激活函数。这是计算一个输出结果的过程，而如果有多个卷积核，就可以计算出多个结果。（下图中，蓝色的数据体是输入图像，黄色的数据体是1x1的卷积核。两个数据体重合部分的数据会先做乘法，再求和，加bias，经过激活函数。）</p>
<p><img src="/2022/07/24/DLS-note-11/7_3.jpg" alt></p>
<p>这个过程让你想起了什么？没错，正是最早学习的全连接网络。1x1卷积，实际上就是在各通道上做了一次全连接的计算。1x1卷积的输入通道数，就是全连接层上一层神经元的数量；1x1卷积核的数量，就是这一层神经元的数量。</p>
<p>1x1卷积主要用于变换图像的通道数。比如要把一个192通道数的图像变成32通道的，就应该用32个1x1卷积去卷原图像。</p>
<h3 id="Inception块的原理"><a href="#Inception块的原理" class="headerlink" title="Inception块的原理"></a>Inception块的原理</h3><p>在Inception网络中，我们会使用这样一种混合模块：对原数据做1x1, 3x3, 5x5卷积以及最大池化，得到通道数不同的数据体。这些数据体会被拼接起来，作为整个模块的输出。</p>
<p><img src="/2022/07/24/DLS-note-11/7_4.jpg" alt></p>
<p>值得注意的是，这里的池化操作和我们之前见过的不太一样。为了保持输出张量的宽高，这个池化的步幅为1，且使用了等长填充。另外，为了调整池化操作输出的通道数，这条数据处理路线上还有一个用1x1卷积变换通道数的操作。这份图省略了很多这样的细节，下一节我们会见到这幅图的完整版。</p>
<p>在实现这样一种模块时，会碰到计算量过大的问题。比如把上面$28 \times 28 \times 192$的数据体用$5 \times 5$卷积卷成$28 \times 28 \times 32$的数据体，需要多少次乘法计算呢？对每个像素单独考虑，一个通道上的卷积要做$5 \times 5$此乘法，192个通道的卷积要做$192 \times 5 \times 5$次乘法。32个这样的卷积在$28 \times 28$的图片上要做$28 \times 28 \times 32 \times 192 \times 5 \times 5 \approx 120M$次乘法。这个计算量太大了。</p>
<p>为此，我们可以巧妙地先利用1x1卷积减少通道数，再做5x5卷积。这样，计算量就少得多了。</p>
<p><img src="/2022/07/24/DLS-note-11/7_5.jpg" alt></p>
<p>这样一种两头大，中间小的结构被形象地称为瓶颈(bottlenect)。这种结构被广泛用在许多典型网络中。</p>
<h3 id="Inception网络"><a href="#Inception网络" class="headerlink" title="Inception网络"></a>Inception网络</h3><p>有了之前的知识，我们可以看Inception模块的完整结构了。1x1卷积没有什么特别的。为了减少3x3卷积和5x5卷积的计算量，做这两种卷积之前都会用1x1卷积减少通道数。而为了改变池化结果的通道数，池化后接了一个1x1卷积操作。</p>
<p><img src="/2022/07/24/DLS-note-11/7_6.jpg" alt></p>
<p>实际上，理解了Inception块，也就能看懂Inception网络了。如下图所示，红框内的模块都是Inception块。而这个网络还有一些小细节：除了和普通网络一样在网络的最后使用softmax输出结果外，这个网络还根据中间结果也输出了几个结果。当然，这些都是早期网络的设计技巧了。</p>
<p><img src="/2022/07/24/DLS-note-11/7_7.jpg" alt></p>
<h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h2><p>MobileNet，顾名思义，这是一种适用于移动(mobile)设备的神经网络。移动设备的计算资源通常十分紧缺，因此，MobileNet对网络的计算量进行了极致的压缩。</p>
<h3 id="减少卷积运算量"><a href="#减少卷积运算量" class="headerlink" title="减少卷积运算量"></a>减少卷积运算量</h3><p>再回顾一遍，一次卷积操作中主要的计算量如下：</p>
<p><img src="/2022/07/24/DLS-note-11/7_8.jpg" alt></p>
<p>计算量这么大，主要问题出在每一个输出通道都要与每一个输入通道“全连接”上。为此，我们可以考虑让输出通道只由部分的输入通道决定。这样一种卷积的策略叫逐深度可分卷积(Depthwise Separable Convolution)。</p>
<blockquote>
<p>这里的depthwise是“逐深度”的意思，但我觉得“逐通道”这个称呼会更容易理解一点。</p>
</blockquote>
<p>逐深度可分卷积分为两步：逐深度卷积(depthwise convolution)，逐点卷积(pointwise convolution)。逐深度卷积生成新的通道，逐点卷积把各通道的信息关联起来。</p>
<p><img src="/2022/07/24/DLS-note-11/7_9.jpg" alt></p>
<p>之前，要对下图中的三通道图片做卷积，需要3个卷积核分别处理3个通道。而在逐深度卷积中，我们只要1个卷积核。这个卷积核会把输入图像当成三个单通道图像来看待，分别对原图像的各个通道进行卷积，并生成3个单通道图像，最后把3个单通道图像拼回一个三通道图像。也就是说，逐深度卷积只能生成一幅通道数相同的新图像。</p>
<blockquote>
<p>逐深度卷积可以通过设置卷积在编程框架中的<code>groups</code>参数来实现。参见我<a href>讲解卷积的文章</a>。</p>
</blockquote>
<p><img src="/2022/07/24/DLS-note-11/7_10.jpg" alt></p>
<p>下一步，是逐点卷积，也就是1x1卷积。它用来改变图片的通道数。</p>
<p><img src="/2022/07/24/DLS-note-11/7_11.jpg" alt></p>
<p>之前的卷积有2160次乘法，现在只有432+240=672次，计算量确实减少了不少。实际上，优化后计算量占原计算量的比例是：</p>
<script type="math/tex; mode=display">
\frac{1}{n_c'} + \frac{1}{f^2}</script><p>其中$n_c’$是输出通道数，$f$是卷积核边长。一般来说计算量都会少10倍。</p>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>知道了MobileNet的基本思想，我们来看几个不同版本的MobileNet。</p>
<h4 id="MobileNet-v1"><a href="#MobileNet-v1" class="headerlink" title="MobileNet v1"></a>MobileNet v1</h4><p>13个逐深度可分卷积模块，之后接通常的池化、全连接、softmax。</p>
<p><img src="/2022/07/24/DLS-note-11/8.jpg" alt></p>
<h4 id="MobileNet-v2"><a href="#MobileNet-v2" class="headerlink" title="MobileNet v2"></a>MobileNet v2</h4><p>两个改进：</p>
<ol>
<li>残差连接</li>
<li>扩张(expansion)操作</li>
</ol>
<p><img src="/2022/07/24/DLS-note-11/9.jpg" alt></p>
<p>残差连接和ResNet一样。这里我们关注一下第二个改进。</p>
<p>在MobileNet v2中，先做一个扩张维度的1x1卷积，再做逐深度卷积，最后做之前的逐点1x1卷积。由于最后的逐点卷积起到的是减小维度的作用，所以最后一步操作也叫做投影。</p>
<p><img src="/2022/07/24/DLS-note-11/10.jpg" alt></p>
<p>这种架构很好地解决了性能和效果之间的矛盾：在模块之间，数据的通道数只有3，占用内存少；在模块之内，更高通道的数据能拟合更复杂的函数。</p>
<h2 id="EfficientNet"><a href="#EfficientNet" class="headerlink" title="EfficientNet"></a>EfficientNet</h2><p>EfficientNet能根据设备的计算能力，自动调整网络占用的资源。</p>
<p>让我们想想，哪些因素决定了一个网络占用的运算资源？我们很快能想到下面这些因素：</p>
<ul>
<li>图像分辨率</li>
<li>网络深度</li>
<li>特征的长度（即卷积核数量或神经元数量）</li>
</ul>
<p>在EfficientNet中，我们可以在这三个维度上缩放网络，动态改变网络的计算量。EfficientNet的开源实现中，一般会提供各设备下的最优参数。</p>
<h2 id="卷积网络实现细节"><a href="#卷积网络实现细节" class="headerlink" title="卷积网络实现细节"></a>卷积网络实现细节</h2><h3 id="使用开源实现"><a href="#使用开源实现" class="headerlink" title="使用开源实现"></a>使用开源实现</h3><p>由于深度学习项目涉及很多训练上的细节，想复现一个前人的工作是很耗时的。最好的学习方法是找到别人的开源代码，在现有代码的基础上学习。</p>
<p>深度学习的开源代码一般在GitHub上都能找到。如果是想看PyTorch实现，可以直接去GitHub上搜索OpenMMLab。</p>
<h3 id="使用迁移学习"><a href="#使用迁移学习" class="headerlink" title="使用迁移学习"></a>使用迁移学习</h3><p>如第三门课第二周所学，我们可以用迁移学习，导入别人训练好的模型里的权重为初始权重，加速我们自己模型的训练。</p>
<p>还是以多分类任务的迁移学习为例（比如把一个1000分类的分类器迁移到一个猫、狗、其他的三分类模型上）。迁移后，新的网络至少要删除输出层，并按照新的多分类个数，重新初始化一个输出层。之后，根据新任务的数据集大小，冻结网络的部分参数，从导入的权重开始重新训练网络的其他部分：</p>
<p><img src="/2022/07/24/DLS-note-11/11.jpg" alt></p>
<p>当然，可以多删除几个较深的层，也可以多加入几个除了输出层以外的隐藏层。</p>
<h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>由于CV任务总是缺少数据，数据增强是一种常见的提升网络性能的手段。</p>
<p>常见的改变形状的数据增强手段有：</p>
<ul>
<li>镜像</li>
<li>裁剪</li>
<li>旋转</li>
<li>扭曲</li>
</ul>
<p>此外，还可以改变图像的颜色。比如对三个颜色通道都随机加一个偏移量。</p>
<p>数据增强有一些实现上的细节：数据的读取及增强是放在CPU上运行的，训练是放在CPU或GPU上运行的。这两步其实是独立的，可以并行完成。最常见的做法是，在CPU上用多进程（发挥多核的优势）读取数据并进行数据增强，之后把数据搬到GPU上训练。</p>
<h3 id="计算机视觉的现状与相关建议"><a href="#计算机视觉的现状与相关建议" class="headerlink" title="计算机视觉的现状与相关建议"></a>计算机视觉的现状与相关建议</h3><p>一般来说，算法从两个来源获取知识：标注的数据，人工设计的特征。这二者是互补的关系。对于人工智能任务来说，如果有足够的数据，设计一个很简单的网络就行了；而如果数据量不足，则需要去精心设计网络结构。</p>
<p>像语音识别这种任务就数据充足，用简单的网络就行了。而大部分计算机视觉任务都处于数据不足的状态。哪怕计算机视觉中比较基础的图像分类任务，都需要设计结构复杂的网络，更不用说目标检测这些更难的任务了。</p>
<p>如果你想用深度学习模型参加刷精度的比赛，可以使用以下几个小技巧：</p>
<ul>
<li>同时开始训练多个网络，算结果时取它们的平均值。</li>
<li>对图像分类任务，可以把图像随机裁剪一部分并输入网络，多次执行这一步骤并取平均分类结果。</li>
</ul>
<p>也就是说，只是为了提高精度的话，可以想办法对同一份输入执行多次条件不同的推理，并对结果求平均。当然，实际应用中是不可能用性能这么低的方法。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这节课是CNN中最重要的一节课。通过学习一些经典的CNN架构，我们掌握了很多有关搭建CNN的知识。总结一下：</p>
<ul>
<li>早期CNN<ul>
<li>卷积、池化、全连接</li>
<li>边长减小，通道数增加</li>
</ul>
</li>
<li>ResNet<ul>
<li>为什么使用ResNet？</li>
<li>梯度问题是怎么被解决的？</li>
<li>残差块的一般结构</li>
<li>输入输出通道数不同的残差块</li>
<li>了解ResNet的结构(ResNet-18, ResNet-50)</li>
</ul>
</li>
<li>Incpetion 网络<ul>
<li>1x1卷积</li>
<li>用1x1卷积减少计算量</li>
<li>Inception网络的基本模块</li>
</ul>
</li>
<li>MobileNet<ul>
<li>逐深度可分卷积</li>
<li>MobileNet v2中的瓶颈结构</li>
</ul>
</li>
</ul>
<p>这节课介绍的都是比较前沿的CNN架构。在深度学习技术日新月异的今天，最好的学习方式是读论文，尽快一步一步跟上最新的技术。这堂课中提及的比较新的几篇论文，都有很高的阅读价值。</p>
<p>我打算在学完CNN的四周课后，暂时不去学第五门课，而是去阅读这些经典CNN论文并分享一下笔记。</p>
<p>在这周的代码实战里，我会分享一下如何用TensorFlow和PyTorch编写ResNet，同时介绍两种框架的进阶用法。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">Designer, artist, philosopher, researcher.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">153</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">66</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → /atom.xml"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
