<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
<meta property="og:type" content="website">
<meta property="og:title" content="周弈帆的博客">
<meta property="og:url" content="https://zhouyifan.net/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhou Yifan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhouyifan.net/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/05/23/DLS-note-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/23/DLS-note-3/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》笔记+代码实战（三）：“浅度”神经网络</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-23 15:36:34" itemprop="dateCreated datePublished" datetime="2022-05-23T15:36:34+08:00">2022-05-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在上节课中，我们学习了逻辑回归——一种经典的学习算法。我兴致勃勃地用它训练了一个猫狗分类模型，结果只得到了57%这么个惨淡的准确率。正好，这周开始学习如何实现更复杂的模型了。这次，我一定要一雪前耻！</p>
<p>开始学这周的课之前，先回忆一下上周我们学习了什么。</p>
<p>对于一个神经网络，我们要定义它的网络结构（一个数学公式），定义损失函数。根据损失函数和网络结构，我们可以对网络的参数求导，并用梯度下降法最小化损失函数。</p>
<p>也就是说，不管是什么神经网络，都由以下几部分组成：</p>
<ul>
<li><strong>网络结构</strong></li>
<li><strong>损失函数</strong></li>
<li><strong>优化策略</strong></li>
</ul>
<p>而在编程实现神经网络时，我们不仅要用计算机语言定义上面这几项内容，还需要<strong>收集数据</strong>、<strong>预处理数据</strong>。</p>
<p>在这堂课中，我们要学一个更复杂的模型，其知识点逃不出上面这些范围。在之后的学习中我们还会看到，浅层神经网络的<strong>损失函数</strong>和<strong>优化策略</strong>和上节课的逻辑回归几乎是一模一样的。我们要关心的，主要是<strong>网络结构</strong>上的变化。</p>
<p>在学习之前，我们可以先有一个心理准备，知道大概要学到哪些东西。</p>
<h1 id="课堂笔记"><a href="#课堂笔记" class="headerlink" title="课堂笔记"></a>课堂笔记</h1><h2 id="神经网络概述与符号标记"><a href="#神经网络概述与符号标记" class="headerlink" title="神经网络概述与符号标记"></a>神经网络概述与符号标记</h2><p><img src="/2022/05/23/DLS-note-3/0.jpg" alt></p>
<p>上节课我们使用的逻辑回归过于简单，它只能被视为只有一个神经元（计算单元）的神经网络。如上图第一行所示。</p>
<p>一般情况下，神经网络都是由许多神经元组成的。我们把一次性计算的神经元都算作“一层”。比如上图第二行的网络有两层，第一层有3个神经元，第二层有1个神经元。</p>
<p>上节课中，对于一个样本$x$，一层的神经网络是用下面的公式计算的：</p>
<script type="math/tex; mode=display">
\hat{y}=a=\sigma(w^Tx+b)</script><p>而这节课将使用的两层神经网络，也使用类似的公式计算：</p>
<script type="math/tex; mode=display">
\begin{align*}
a^{[1]} & = \sigma(W^{[1]}x+b^{[1]}) \\

\hat{y}=a^{[2]} & = \sigma(W^{[2]}a^{[1]}+b^{[2]})
\end{align*}</script><blockquote>
<p>上节课中，参数$w$是一个列向量。这节课的参数$W$是一个矩阵。我们稍后会见到$W$的全貌。</p>
</blockquote>
<p>这里的方括号上标$[l]$表示第$l$层相关的变量。总结一下，$a_i^{<a href="k">j</a>}$表示第$k$个样本在网络第$j$层中向量的第$i$个分量。</p>
<p>事实上，输入$x$可以看成$a^{[0]}$。</p>
<blockquote>
<p>这里的a是activation（激活）意思，每个$a$都是激活函数的输出。</p>
</blockquote>
<p>为了方便称呼，我们给神经网络的层取了些名字：</p>
<p><img src="/2022/05/23/DLS-note-3/1.jpg" alt></p>
<p>其中，输入叫做“输入层”，最后一个计算层叫做“输出层”，中间其余的层都叫做“隐藏层”。事实上，由于第一个输入层不参与计算，它不会计入网络的总层数，只是为了方便称呼才这么叫。因此，上面这个网络看上去有3层，但叫做“双层神经网络”，或“单隐藏层神经网络”。</p>
<h2 id="单样本多神经元的计算"><a href="#单样本多神经元的计算" class="headerlink" title="单样本多神经元的计算"></a>单样本多神经元的计算</h2><p>让我们先看一下，对于<strong>一个输入样本</strong>$x^{(1)}$，神经网络是怎么计算输出的。</p>
<p><img src="/2022/05/23/DLS-note-3/2.jpg" alt></p>
<p>如图，输入 $x$ 是一个形状为$3 \times 1$的列向量。第一层有三个神经元，第一个神经元的参数是$w_1^{[1]}, b_1^{[1]}$，第二个是$w_2^{[1]}, b_2^{[1]}$，第三个是$w_3^{[1]}, b_3^{[1]}$。</p>
<p>$w_i^{[1]}$的形状是$1 \times 3$，$b_i^{[1]}$是常数。</p>
<p>每个神经元的计算公式和上节课的逻辑回归相同，都是$z_i^{[1]}=w_i^{[1]}x+b_i^{[1]}$，$a_i^{[1]}=\sigma(z_i^{[1]})$（$i \in [1, 2, 3]$)。</p>
<blockquote>
<p>回忆一下，上一节课里$w$的形状是$n_x \times1$，即一个长度为$n_x$的<strong>列向量</strong>，其中$n_x$是输入向量的长度（此处为3）。$b$是一个常数。计算结果时，我们要把$w$转置，计算$w^Tx+b$。这里的$w_i^{[1]}$是一个<strong>行向量</strong>，其形状是$1 \times n_x$，计算时不用转置。计算时直接$w_i^{[1]}x+b_i^{[1]}$就行。</p>
</blockquote>
<p>因为有三个神经元，我们得到三个计算结果$a_1^{[1]}, a_2^{[1]}, a_3^{[1]}$。我们可以把它们合起来当成一个$3 \times 1$的列向量$a^{[1]}$，就像输入$x$一样。</p>
<p>之后，这三个输出作为输入传入第二层的神经元，计算$z^{[2]}=W_1^{[2]}a^{[1]}+b^{[2]}$, $\hat{y}=a^{[2]}=\sigma(z^{[2]})$。这个算式和上周的逻辑回归一模一样。</p>
<p>总结一下，如果某一层有$n$个神经元，那么这一层的输出就是一个长度为$n$的列向量。这个输出会被当作下一层的输入。神经网络的每一层都按同样的方式计算着。</p>
<p>对于单隐层神经网络，隐藏层的参数$W^{[1]}$的形状是$n_1 \times n_x$，其中$n_1$是隐藏层神经元个数，$n_x$是每个输入样本的向量长度。参数$b^{[1]}$的形状是$n_1 \times 1$。输出层参数$W^{[2]}$的形状是$1 \times n_1$，$b^{[2]}$的形状是$1 \times 1$。</p>
<h2 id="多样本多神经元的计算"><a href="#多样本多神经元的计算" class="headerlink" title="多样本多神经元的计算"></a>多样本多神经元的计算</h2><p>和上一节课一样，让我们把一个输入样本拓展到<strong>多个样本</strong>，看看整个计算公式该怎么写。</p>
<p>对于第$i$个输入样本$x^{(i)}$，我们要计算：</p>
<script type="math/tex; mode=display">
\begin{align*}
a^{[1](i)} & =\sigma(W^{[1]}x^{(i)}+b^{[1]}) \\
a^{[2](i)} & =\sigma(W^{[2]}a^{[1](i)}+b^{[2]})
\end{align*}</script><p>直接写的话，我们要写个for循环，把$i$从$0$遍历到$m-1$。</p>
<blockquote>
<p>回忆一下，$m$是样本总数。</p>
</blockquote>
<p>但是，如果把输入打包在一起，形成一个$n_x \times m$的矩阵$X$，那么整个计算过程可以用十分相似的向量化计算公式表示：</p>
<script type="math/tex; mode=display">
\begin{align*}
A^{[1]} & =\sigma(W^{[1]}X+b^{[1]}) \\
A^{[2]} & =\sigma(W^{[2]}A^{[1]}+b^{[2]})
\end{align*}</script><blockquote>
<p>这里的$X$,$A$相当于横向“拉长了”：</p>
<script type="math/tex; mode=display">
X=\left[
  \begin{matrix}
  | & | & & | \\
  x^{(1)} & x^{(2)} & ... & x^{(m)} \\
  | & | & & |
  \end{matrix}
\right] \\ 
\ \\
A^{[l]}=\left[
  \begin{matrix}
  | & | & & | \\
  a^{[l](1)} & a^{[l](2)} & ... & a^{[l](m)} \\
  | & | & & |
  \end{matrix}
\right]</script></blockquote>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>在神经网络中，我们每做完一个线性运算$Z=WX+b$后，都会做一个$\sigma(Z)$的操作。上周我们讲这个$\sigma$（sigmoid函数）是为了把实数的输入映射到$[0, 1]$。这是它在逻辑回归的作用。而在普通的神经网络中，$\sigma$就有别的作用了——<strong>激活</strong>线性输出。$\sigma$其实只是激活输出的<strong>激活函数</strong>的一员，还有很多其他函数都可以用作为激活函数。我们现在暂时不管这个“激活”是什么意思，先认识一下常见的激活函数。</p>
<p><img src="/2022/05/23/DLS-note-3/3.jpg" alt></p>
<blockquote>
<p>画这些函数的代码见后文。</p>
</blockquote>
<p>它们的数学公式如下：</p>
<script type="math/tex; mode=display">
sigmoid(x)=\frac{1}{1+e^{-x}} \\
tanh(x)=\frac{e^x - e^{-x}}{e^x + e^{-x}} \\
relu(x) = \left\{
\begin{aligned}
   x & \ (x \geq 0) \\
   0 & \ (x < 0)
\end{aligned}
\right. \\
leaky\_relu(x) = \left\{
\begin{aligned}
   x & \ (x \geq 0) \\
   kx & \ (x < 0, k < 1)
\end{aligned}
\right.</script><p>其中leaky_relu里的$k$是一个常数，这个常数要小于1。图中的leaky_relu的$k$取了0.1。</p>
<p>现在来介绍一下这些激活函数。</p>
<p>sigmiod，老熟人了，这个函数可以把实数上的输入映射到$(0, 1)$。tanh其实是sigmoid的一个“位移版”（二者的核心都是$e^x$），它可以把实数的输入映射到$(-1, 1)$。</p>
<p>这两个函数有一个问题：当x极大或者极小的时候，函数的梯度几乎为0。从图像上来看，也就是越靠近左边或者右边，函数曲线就越平。梯度过小，会导致梯度下降法每次更新的幅度较小，从而使网络训练速度变慢。</p>
<p>为了解决梯度变小的问题，研究者们又提出了relu函数(rectified linear unit, 线性整流单元)。别看这个名字很高大上，relu函数本身其实很简单：你是正数，就取原来的值；你是负数，就取0。非常的简单直接。把这个函数用作激活函数，梯度总是不会太小，可以有效加快训练速度。</p>
<p>有人觉得relu对负数太“一刀切”了，把relu在负数上的值改成了一个随输入$x$变化的，十分接近0的值。这样一个新的relu函数就叫做leaky relu。（大家应该知道为什么leaky_relu的$k$要小于1了吧）</p>
<blockquote>
<p>写在博客里的题外话：浅谈文章的统一性。为什么这里relu用的是小写呢？按照英文的写法，应该是ReLU才对啊？这里是不是写文章的时候不够严谨啊？其实不是。我们这里其实统一用的是代码写法，即全部单词小写。我们首次介绍relu时，是在上文的图片和公式里。那里面用的是小写的relu。后文其实是对这种描述的一个统一，表示“前文用到的relu”，而不是一般用语中的ReLU。在后面的文章中，我会使用ReLU这个称呼。</p>
<p>如果有严谨的文字工作者，还会质疑道：“你这篇文章里有些单词应该用公式框起来，有些应该用代码框起来，怎么直接用文本表示啊？”这是因为微信公众号对公式的支持很烂，我编辑得累死了，不想动脑去思考到底用公式还是用代码了。要把一个东西写得天衣无缝，需要耗费大量的时间。为了权衡，我抛弃了部分严谨性，换来了写文章的效率。</p>
</blockquote>
<h2 id="如何选择激活函数"><a href="#如何选择激活函数" class="headerlink" title="如何选择激活函数"></a>如何选择激活函数</h2><p>tanh由于其值域比sigmoid大，原理又一模一样，所以tanh在数学上严格优于sigmoid。除非是输出恰好处于$(0, 1)$(比如逻辑回归的输出)，不然宁可用tanh也不要用sigmoid。</p>
<p>现在大家都默认使用relu作为激活函数，偶尔也有使用leaky_relu的。吴恩达老师鼓励大家多多尝试不同的激活函数。</p>
<p>在之前介绍的公式中，我们所有激活函数$g$都默认用的是$g=\sigma$。准确来说，单隐层神经网络公式应该写成下面这种形式：</p>
<script type="math/tex; mode=display">
\begin{align*}
A^{[1]} & =g^{[1]}(W^{[1]}X+b^{[1]}) \\
A^{[2]} & =g^{[2]}(W^{[2]}A^{[1]}+b^{[2]})
\end{align*}</script><p>由于第二层网络的输出落在[0, 1]，我们第二个激活函数还是可以用sigmoid，即$g^{[2]}=\sigma$。</p>
<h2 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h2><p>假设我们有一个两层神经网络：</p>
<script type="math/tex; mode=display">
\hat{y} = g(W_2 \cdot g(W_1x+b_1) + b_2)</script><p>其中激活函数用$g$表示。</p>
<p>假如我们不使用激活函数，即令$g(x)=x$的话，这个神经网络就变成了：</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{y} &= W_2 \cdot (W_1x+b_1) + b_2 \\
&= (W_2W_1)x+(b_1+b_2)
\end{align*}</script><p>我们把$W_2W_1$看成一个新的“$W$”,$(b_1+b_2)$看成一个新的”$b$”,那么这其实是一个单层神经网络。</p>
<p>也就是说，如果我们不用激活函数，那么无论神经网络有多少层，这个神经网络都等价于只有一层。这种神经网络永远只能拟合一个线性函数。</p>
<p>为了让神经网络取拟合一个非线性的，超级复杂的函数，我们必须要使用激活函数。</p>
<h2 id="激活函数的导数（选读）"><a href="#激活函数的导数（选读）" class="headerlink" title="激活函数的导数（选读）"></a>激活函数的导数（选读）</h2><blockquote>
<p>为了让大家重新体验一下高中学数学的感觉，这里求导的步骤推得十分详细。</p>
</blockquote>
<h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><script type="math/tex; mode=display">
\begin{align*}
f(x) &= \frac{1}{1+e^{-x}} \\
f'(x) &=  -(\frac{1}{1+e^{-x}})^2(e^{-x})' \\
&= -(\frac{1}{1+e^{-x}})^2(-e^{-x}) \\
&= \frac{e^{-x}}{(1+e^{-x})^2} \\
&= f(x)(1-f(x))
\end{align*}</script><blockquote>
<p>上篇笔记也吐槽过了，想写出最后一步，需要发动数学家的固有技能：「注意到」。这不怎么学数学的人谁能注意到最后这一步啊。</p>
</blockquote>
<h3 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h3><script type="math/tex; mode=display">
\begin{align*}
f(x) &= \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{e^{2x}-1}{e^{2x}+1}\\
\\
(\frac{x-1}{x+1})' &=\frac{(x+1)-(x-1)}{(x+1)^2}\\
&= \frac{2}{(x+1)^2}\\
\\
f'(x) &=  (\frac{x-1}{x+1})'(e^{2x}) \cdot (e^{2x})' \\
&= \frac{2}{(e^{2x}+1)^2}(2e^{2x}) \\
&= \frac{4e^{2x}}{(e^{2x}+1)^2} \\
&= (1+ f(x))(1-f(x))
\end{align*}</script><blockquote>
<p>回忆一下，$(\frac{u}{v})’=(\frac{u’v-uv’}{v^2})$。</p>
<p>最后这步我依然注意不到。我猜原函数$f(x)$是用$f’(x)=(1+ f(x))(1-f(x))$这个微分方程构造出来的，而不是反过来恰好发现导数能够写得这么简单。</p>
</blockquote>
<h3 id="relu"><a href="#relu" class="headerlink" title="relu"></a>relu</h3><script type="math/tex; mode=display">
f(x) = \left\{
\begin{aligned}
   x & \ (x \geq 0) \\
   0 & \ (x < 0)
\end{aligned}
\right. \\
f'(x) = \left\{
\begin{aligned}
   1 & \ (x > 0) \\
   0 & \ (x < 0)
\end{aligned}
\right. \\</script><blockquote>
<p>这个导求得神清气爽。</p>
</blockquote>
<h3 id="leaky-relu"><a href="#leaky-relu" class="headerlink" title="leaky relu"></a>leaky relu</h3><script type="math/tex; mode=display">
f(x) = \left\{
\begin{aligned}
   x & \ (x \geq 0) \\
   kx & \ (x < 0)
\end{aligned}
\right. \\
f'(x) = \left\{
\begin{aligned}
   1 & \ (x > 0) \\
   k & \ (x < 0)
\end{aligned}
\right. \\</script><blockquote>
<p>学数学的人可能会很在意：relu和leaky relu在0处没有导数啊！碰到0你怎么梯度下降啊？实际上，我们编程的时候，不用管那么多，直接也令0处的导数为1就行（即导数在0处的右极限）。</p>
</blockquote>
<h2 id="对神经网络做梯度下降"><a href="#对神经网络做梯度下降" class="headerlink" title="对神经网络做梯度下降"></a>对神经网络做梯度下降</h2><p>回顾一下，如果只有两个参数$w, b$，应该用下式做梯度下降：</p>
<script type="math/tex; mode=display">
\begin{align*}
w & \gets w - \alpha \frac{dJ}{dw} \\ 
b & \gets b - \alpha \frac{dJ}{db}
\end{align*}</script><blockquote>
<p>回忆一下，$\alpha$是学习率，表示梯度更新的速度，一般取$0.0001$这种很小的值。</p>
</blockquote>
<p>现在，我们有4个参数：$W^{[1]},W^{[2]}, b^{[1]},b^{[2]}$，它们也应该按照同样的规则执行梯度下降：</p>
<script type="math/tex; mode=display">
\begin{align*}
W^{[1]} & \gets W^{[1]} - \alpha \frac{dJ}{dW^{[1]}} \\ 
W^{[2]} & \gets W^{[2]} - \alpha \frac{dJ}{dW^{[2]}} \\ 
b^{[1]} & \gets b^{[1]} - \alpha \frac{dJ}{db^{[1]}} \\
b^{[2]} & \gets b^{[2]} - \alpha \frac{dJ}{db^{[2]}} \\
\end{align*}</script><p>剩下的问题就是怎么求导了。让我们再看一遍神经网络正向传播的公式：</p>
<script type="math/tex; mode=display">
\begin{align*}
Z^{[1]} &= W^{[1]}X+b^{[1]} \\
A^{[1]} &= g^{[1]}(Z^{[1]}) \\
Z^{[2]} &= W^{[2]}X+b^{[2]} \\
A^{[2]} &= g^{[2]}(Z^{[2]})
\end{align*}</script><p>由于我们令$g^{[2]}=\sigma$，所以神经网络第二层（输出层）的导数可以直接套用上周的导数公式：</p>
<script type="math/tex; mode=display">
\begin{align*}
dZ^{[2]} &= A^{[2]}-Y \\
dW^{[2]} &=  \frac{1}{m}dZ^{[2]}A^{[1]T} \\
db^{[2]} &= \frac{1}{m} \Sigma_{i=1}^m dZ^{[2](i)} 
\end{align*}</script><blockquote>
<p><strong>注意！</strong> 上周我们算的是$AdZ^T$，这周是$dZ^{[2]}A^{[1]T}$。这是因为参数$W$转置了一下。上周的$w$是列向量，这周每个神经元的权重$W_i$是行向量。</p>
</blockquote>
<p>之后，我们来看第一层。首先求$dZ^{[1]}$:</p>
<script type="math/tex; mode=display">
\begin{align*}
dZ^{[1]} &= dA^{[1]}\frac{dA^{[1]}}{dZ^{[1]}} \\
&=W^{[2]T}dZ^{[2]} \ast g^{[1]'} (Z^{[1]})
\end{align*}</script><blockquote>
<p>注意，上式中右边第一项$dA^{[1]}$是$\frac{dJ}{dA^{[1]}}$的简写，第二项$\frac{dA^{[1]}}{dZ^{[1]}}$是实实在在的求导。</p>
<p>这里$dA^{[1]}$和$dW^{[2]}$的计算是对称的哟。</p>
</blockquote>
<p>之后的$dW^{[1]}, db^{[1]}$的公式和前面$dW^{[2]}, db^{[2]}$的相同：</p>
<script type="math/tex; mode=display">
\begin{align*}
dW^{[1]} &=  \frac{1}{m}dZ^{[1]}X^{T} \\
db^{[1]} &= \frac{1}{m} \Sigma_{i=1}^m dZ^{[1](i)} 
\end{align*}</script><blockquote>
<p>别忘了，$X=A^{[0]}$。</p>
</blockquote>
<p>这些求导的步骤写成代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dZ2=A2-Y</span><br><span class="line">dW2=np.dot(dZ2, A1.T) / m</span><br><span class="line">db2=np.<span class="built_in">sum</span>(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>) / m</span><br><span class="line">dZ1=np.dot(W2.T, dZ2) * g1_backward(Z1)</span><br><span class="line">dW1=np.dot(dZ1, X.T) / m</span><br><span class="line">db1=np.<span class="built_in">sum</span>(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>) / m</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>再次温馨提示，搞不清楚数学公式的细节没关系，直接拿来用就好了。要学会的是算法的整体思路。</p>
</blockquote>
<p>这段代码有一点需要注意：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db2=np.<span class="built_in">sum</span>(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">db1=np.<span class="built_in">sum</span>(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br>这个<code>keepdims=True</code>是必不可少的。使用<code>np.sum, np.mean</code>这种会导致维度变少的计算时，如果加了<code>keepdims=True</code>,会让变少的那一个维度保持长度1.比如一个[4, 3]的矩阵，我们对第二维做求和，理论上得到的是一个[4]的向量。但如果用了<code>keepdims=True</code>，就会得到一个[4, 1]的矩阵。</p>
<p>保持向量的维度，可以让某些广播运算正确进行。比如我要用[4, 3]的矩阵减去[4]的矩阵就会报错，而减去[4, 1]的矩阵就不会报错。</p>
<h2 id="参数随机初始化"><a href="#参数随机初始化" class="headerlink" title="参数随机初始化"></a>参数随机初始化</h2><p>再次回顾下，梯度下降算法的结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">初始化参数</span><br><span class="line">迭代 k 步：</span><br><span class="line">   算参数的梯度</span><br><span class="line">   用梯度更新参数</span><br></pre></td></tr></table></figure>
<p>对于这节课新学的单隐层神经网络，求导、更新参数的过程我们已经学完了。我们还有一个东西没有详细探究：参数的初始化方式。现在，我们来详细研究一下参数初始化。</p>
<p>在上节课中，我们用一句话就带过了参数初始化方法：令参数全为0就行了。这种初始化方法在这节课还有用吗？让我们来看课堂里提到的一个示例：</p>
<p><img src="/2022/05/23/DLS-note-3/4.jpg" alt></p>
<p>如上图，对于输入长度为2，第一层有2个神经元的网络，其第一层参数$W^{[1]}$为<code>[[0, 0], [0, 0]]</code>。这样算出来的神经元输出$a^{[1]}_1,a^{[1]}_2$是一样的。而更新梯度时，每一个神经元的参数$W^{[1]}_1, W^{[1]}_2$的梯度都只和该神经元的输出有关。这样，每个神经元参数的导数<code>dw</code>都是一模一样的。导数一样，初始化的值也一样，那么每个神经元的参数的值会一直保持相同。这样，不论我们在某一层使用了多少个神经元，都等价于只使用一个神经元。</p>
<p>为了不发生这样的情况，我们需要让每一个神经元的参数$w$都<strong>取不同的值</strong>。这可以通过<strong>随机初始化</strong>实现。只需要使用下面的代码就可以随机初始化$w$:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.randn((h, w)) * 0.01</span><br></pre></td></tr></table></figure>
<p>注意，这里我们给随机出的数乘了个0.01。这是因为出于经验，人们更倾向于使用更小的参数，以计算出更小的结果，防止激活函数（如tanh）在绝对值过大时梯度过小的问题。</p>
<blockquote>
<p>后面的课会详细介绍该如何初始化这些参数，以及初始化参数可以解决哪些问题。</p>
</blockquote>
<p>而$b$和之前一样，直接用0初始化就行了。</p>
<h2 id="知识总结"><a href="#知识总结" class="headerlink" title="知识总结"></a>知识总结</h2><p>在这堂课中，我们正式认识了神经网络的定义。原来，上周的逻辑回归只是一个特殊的神经网络。它只有一个输出层，并且使用sigmoid作激活函数。而这周，我们学习了如何定义一个两层（一个隐藏层、一个输出层）的神经网络，并且知道如何在网络中使用不同的激活函数。</p>
<p>让我们来看一下这节课的知识点：</p>
<ul>
<li>神经网络的定义<ul>
<li>输入层、隐藏层、输出层</li>
<li>每一层每一个神经元相关的参数该怎么表示</li>
</ul>
</li>
<li>神经网络的计算方式<ul>
<li>单样本 -&gt; 多样本</li>
<li>正向传播与反向传播</li>
</ul>
</li>
<li>激活函数<ul>
<li>直观认识激活函数——激活函数属于神经网络计算中的哪一部分？</li>
<li>常见的四种激活函数：sigmoid, tanh, relu, leaky_relu</li>
<li><strong>如何选择激活函数</strong></li>
<li><strong>为什么要使用激活函数</strong></li>
</ul>
</li>
<li>神经网络与逻辑回归的区别——参数初始化问题<ul>
<li><strong>为什么不能用0初始化$W$</strong></li>
<li>随机初始化$W$</li>
<li>可以用0初始化$b$</li>
</ul>
</li>
</ul>
<h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h1><p>这节课的编程作业是搞一个点集分类器。此任务的数据集如下图所示：</p>
<p><img src="/2022/05/23/DLS-note-3/5.jpg" alt></p>
<p>在平面上，已知有一堆红色的点和绿色的点。我们希望任意给定一个点，程序能够判断这个点是红点还是绿点。</p>
<p>让我们人类来分类的话，肯定会认为左边一片花瓣和右上角两片花瓣是绿色的，剩下三片花瓣是红色的（有部分点不满足这个规律，可以认为这些点是噪声，即不正确的数据）。让神经网络来做这个任务，会得到怎样的结果呢？</p>
<p>现在，让我们用这周学的单隐层神经网络，来实现这个分类器。</p>
<p>虽然前面说这周要继续挑战猫狗分类任务，但我估摸着这周的模型可能还是简单了一点。等下周学了再强大一点的模型，我再来复仇。</p>
<p>项目链接:<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/ShallowNetwork">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/ShallowNetwork</a></p>
<h2 id="通用分类器类"><a href="#通用分类器类" class="headerlink" title="通用分类器类"></a>通用分类器类</h2><p>在上节课的编程实战中，我们很暴力地写了“一摊”代码。说实话，有编程洁癖的我是不能接受那种潦草的代码的。如果代码写得太乱，就根本不能复用，根本不可读，根本不能体现编程的逻辑之美。</p>
<p>这周，我将解除封印，释放我30%的编程水平，展示一个比较优雅的通用分类器类该怎么写。我们会先把上周的逻辑回归用继承基类的方式实现一遍，再实现一遍这周的浅层神经网络。</p>
<p>分类器基类的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> abc</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseRegressionModel</span>(<span class="params">metaclass=abc.ABCMeta</span>):</span></span><br><span class="line">    <span class="comment"># Use Cross Entropy as the cost function</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abc.abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, train_mode=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="comment"># if self.train_mode:</span></span><br><span class="line">        <span class="comment">#   forward_train()</span></span><br><span class="line">        <span class="comment"># else:</span></span><br><span class="line">        <span class="comment">#   forward_test()</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abc.abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, Y</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abc.abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span>(<span class="params">self, learning_rate=<span class="number">0.001</span></span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, Y_hat, Y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.mean(-(Y * np.log(Y_hat) + (<span class="number">1</span> - Y) * np.log(<span class="number">1</span> - Y_hat)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">self, X, Y</span>):</span></span><br><span class="line">        Y_hat = self.forward(X, train_mode=<span class="literal">False</span>)</span><br><span class="line">        predicts = np.where(Y_hat &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        score = np.mean(np.where(predicts == Y, <span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;score&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>为了简化代码，我们用<code>BaseRegressionModel</code>表示一个使用交叉熵为损失函数的二分类模型。这样，我们所有的模型都可以共用一套损失函数<code>loss</code>、一套评估方法<code>evaluate</code>。这里损失函数和评估方法的实现都是从上周的代码里复制过来的。</p>
<p>让我们分别看一下其他几个类方法的描述：</p>
<ul>
<li><code>__init__</code>: 模型的参数应该在<code>__init__</code>方法里初始化。</li>
<li><code>forward</code>：正向传播函数。这个函数即可以用于测试，也可以用于训练。如果是用于训练，就要令参数<code>train_mode=True</code>。为什么要区分训练和测试呢？这是因为，正向传播在训练的时候需要额外保存一些数据(缓存)，保存数据是存在开销的。在测试的时候，我们可以不做缓存，以降低程序运行开销。</li>
<li><code>backward</code>：反向传播函数。这个函数用于<code>forward</code>之后的梯度计算。算出来的梯度会缓存起来，供反向传播使用。</li>
<li><code>gradient_descent</code>：用梯度下降更新模型的参数。（一般框架会把优化器和模型分开写。由于我们现在只学了梯度下降这一种优化策略，所以直接把梯度下降当成了模型类的方法）</li>
</ul>
<p>有了这样一个分类器基类后，我们可以用统一的方式训练模型：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">model: BaseRegressionModel,</span></span></span><br><span class="line"><span class="params"><span class="function">                X_train,</span></span></span><br><span class="line"><span class="params"><span class="function">                Y_train,</span></span></span><br><span class="line"><span class="params"><span class="function">                X_test,</span></span></span><br><span class="line"><span class="params"><span class="function">                Y_test,</span></span></span><br><span class="line"><span class="params"><span class="function">                steps=<span class="number">1000</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                learning_rate=<span class="number">0.001</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                print_interval=<span class="number">100</span></span>):</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(steps):</span><br><span class="line">        Y_hat = model.forward(X_train)</span><br><span class="line">        model.backward(Y_train)</span><br><span class="line">        model.gradient_descent(learning_rate)</span><br><span class="line">        <span class="keyword">if</span> step % print_interval == <span class="number">0</span>:</span><br><span class="line">            train_loss = model.loss(Y_hat, Y_train)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Step <span class="subst">&#123;step&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Train loss: <span class="subst">&#123;train_loss&#125;</span>&#x27;</span>)</span><br><span class="line">            model.evaluate(X_test, Y_test)</span><br></pre></td></tr></table></figure><br>有了一个初始化好的模型<code>model</code>后，我们在训练函数<code>train_model</code>里可以直接开始循环训练模型。每次我们先调用<code>model.forward</code>做正向传播，缓存一些数据，再调用<code>model.backward</code>反向传播算梯度，最后调用<code>model.gradient_descent</code>更新模型的参数。每训练一定的步数，我们监控一次模型的训练情况，输出模型的训练loss和测试精度。</p>
<p>看吧，是不是使用了类来实现神经网络后，整个代码清爽而整洁？</p>
<h2 id="工具函数"><a href="#工具函数" class="headerlink" title="工具函数"></a>工具函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.maximum(x, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_de</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.where(x &gt; <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>同样，为了让代码更整洁，我把一些工具函数单独放到了一个文件里。现在，如上面的代码所示，我们的工具函数只有几个损失函数及它们的导数。（用于sigmoid只用于最后一层，我们可以直接用<code>dZ=A-Y</code>跳一个导数计算步骤，所以这里没有写sigmoid的导数）。</p>
<h2 id="复现逻辑回归"><a href="#复现逻辑回归" class="headerlink" title="复现逻辑回归"></a>复现逻辑回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span>(<span class="params">BaseRegressionModel</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_x</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.n_x = n_x</span><br><span class="line">        self.w = np.zeros((n_x, <span class="number">1</span>))</span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, train_mode=<span class="literal">True</span></span>):</span></span><br><span class="line">        Z = np.dot(self.w.T, X) + self.b</span><br><span class="line">        A = sigmoid(Z)  <span class="comment"># hat_Y = A</span></span><br><span class="line">        <span class="keyword">if</span> train_mode:</span><br><span class="line">            self.m_cache = X.shape[<span class="number">1</span>]</span><br><span class="line">            self.X_cache = X</span><br><span class="line">            self.A_cache = A</span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, Y</span>):</span></span><br><span class="line">        d_Z = self.A_cache - Y</span><br><span class="line">        d_w = np.dot(self.X_cache, d_Z.T) / self.m_cache</span><br><span class="line">        d_b = np.mean(d_Z)</span><br><span class="line">        self.d_w_cache = d_w</span><br><span class="line">        self.d_b_cache = d_b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span>(<span class="params">self, learning_rate=<span class="number">0.001</span></span>):</span></span><br><span class="line">        self.w -= learning_rate * self.d_w_cache</span><br><span class="line">        self.b -= learning_rate * self.d_b_cache</span><br></pre></td></tr></table></figure>
<p>逻辑回归是上节课的内容，这里就不讲解，直接贴代码了。大家可以通过这个例子看一看<code>BaseRegressionModel</code>的子类应该怎么写。</p>
<h2 id="实现单隐层神经网络"><a href="#实现单隐层神经网络" class="headerlink" title="实现单隐层神经网络"></a>实现单隐层神经网络</h2><p>有了基类后，我们更加明确代码中哪些地方是要重新写，不能复用以前的代码了。在实现浅层神经网络时，我们要重写<strong>模型初始化</strong>、<strong>正向传播</strong>、<strong>反向传播</strong>、<strong>梯度下降</strong>这几个步骤。</p>
<h3 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h3><p>我们要在<code>__init__</code>里初始化模型的参数。回忆一下这周的单隐层神经网络推理公式：</p>
<script type="math/tex; mode=display">
\begin{align*}
A^{[1]} & =g^{[1]}(W^{[1]}X+b^{[1]}) \\
A^{[2]} & =g^{[2]}(W^{[2]}A^{[1]}+b^{[2]})
\end{align*}</script><p>其中，有四个参数$W^{[1]}, W^{[2]}, b^{[1]}, b^{[2]}$，它们的形状分别是$n_1 \times n_x$, $1 \times n_1$, $n_1 \times 1$, $1 \times 1$。我们需要在这里决定$n_x, n_1$这两个数。</p>
<p>$n_x$由输入向量的长度决定。由于我们是做2维平面点集分类，每一个输入数据就是一个二维的点。因此，在稍后初始化模型时，我们会令$n_x=2$。</p>
<p>$n_1$属于网络的超参数，我们可以调整这个参数的值。</p>
<p>计划好了初始化函数的输入参数后，我们来看看初始化函数的代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_x, n_1</span>):</span></span><br><span class="line">   <span class="built_in">super</span>().__init__()</span><br><span class="line">   self.n_x = n_x</span><br><span class="line">   self.n_1 = n_1</span><br><span class="line">   self.W1 = np.random.randn(n_1, n_x) * <span class="number">0.01</span></span><br><span class="line">   self.b1 = np.zeros((n_1, <span class="number">1</span>))</span><br><span class="line">   self.W2 = np.random.randn(<span class="number">1</span>, n_1) * <span class="number">0.01</span></span><br><span class="line">   self.b2 = np.zeros((<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure></p>
<p>别忘了，前面我们学过，初始化<code>W</code>时要使用随机初始化，且让初始化出来的值比较小。</p>
<h3 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h3><p>我们打算神经网络令第一层的激活函数为relu，第二层的激活函数为sigmoid。因此，模型的正向传播公式如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
A^{[1]} & =relu(W^{[1]}X+b^{[1]}) \\
A^{[2]} & =sigmoid(W^{[2]}A^{[1]}+b^{[2]})
\end{align*}</script><p>用代码表示如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, train_mode=<span class="literal">True</span></span>):</span></span><br><span class="line">   Z1 = np.dot(self.W1, X) + self.b1</span><br><span class="line">   A1 = relu(Z1)</span><br><span class="line">   Z2 = np.dot(self.W2, A1) + self.b2</span><br><span class="line">   A2 = sigmoid(Z2)</span><br><span class="line">   <span class="keyword">if</span> train_mode:</span><br><span class="line">      self.m_cache = X.shape[<span class="number">1</span>]</span><br><span class="line">      self.X_cache = X</span><br><span class="line">      self.Z1_cache = Z1</span><br><span class="line">      self.A1_cache = A1</span><br><span class="line">      self.A2_cache = A2</span><br><span class="line">   <span class="keyword">return</span> A2</span><br></pre></td></tr></table></figure></p>
<p>其中<code>train_mode</code>里的内容是我们待会儿要在反向传播用到的数据，这里需要先缓存起来。</p>
<blockquote>
<p>事实上，我是边写反向传播函数，边写这里<code>if train_mode:</code>里面的缓存数据的。编程不一定要按照顺序写。</p>
</blockquote>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>翻译一下这些公式：</p>
<script type="math/tex; mode=display">
\begin{align*}
dZ^{[2]} &= A^{[2]}-Y \\
dW^{[2]} &=  \frac{1}{m}dZ^{[2]}A^{[1]T} \\
db^{[2]} &= \frac{1}{m} \Sigma_{i=1}^m dZ^{[2](i)} \\
dZ^{[1]} &=W^{[2]T}dZ^{[2]} \ast g^{[1]'} (Z^{[1]}) \\
dW^{[1]} &=  \frac{1}{m}dZ^{[1]}X^{T} \\
db^{[1]} &= \frac{1}{m} \Sigma_{i=1}^m dZ^{[1](i)} 
\end{align*}</script><p>用代码写就是这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, Y</span>):</span></span><br><span class="line">   dZ2 = self.A2_cache - Y</span><br><span class="line">   dW2 = np.dot(dZ2, self.A1_cache.T) / self.m_cache</span><br><span class="line">   db2 = np.<span class="built_in">sum</span>(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>) / self.m_cache</span><br><span class="line">   dA1 = np.dot(self.W2.T, dZ2)</span><br><span class="line"></span><br><span class="line">   dZ1 = dA1 * relu_de(self.Z1_cache)</span><br><span class="line">   dW1 = np.dot(dZ1, self.X_cache.T) / self.m_cache</span><br><span class="line">   db1 = np.<span class="built_in">sum</span>(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>) / self.m_cache</span><br><span class="line"></span><br><span class="line">   self.dW2_cache = dW2</span><br><span class="line">   self.dW1_cache = dW1</span><br><span class="line">   self.db2_cache = db2</span><br><span class="line">   self.db1_cache = db1</span><br></pre></td></tr></table></figure>
<p>算完梯度后，我们要把它们缓存起来，用于之后的梯度下降。</p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span>(<span class="params">self, learning_rate=<span class="number">0.001</span></span>):</span></span><br><span class="line">   self.W1 -= learning_rate * self.dW1_cache</span><br><span class="line">   self.b1 -= learning_rate * self.db1_cache</span><br><span class="line">   self.W2 -= learning_rate * self.dW2_cache</span><br><span class="line">   self.b2 -= learning_rate * self.db2_cache</span><br></pre></td></tr></table></figure>
<p>梯度已经算好了，梯度下降就没什么好讲的了。</p>
<h2 id="挑战点集分类问题"><a href="#挑战点集分类问题" class="headerlink" title="挑战点集分类问题"></a>挑战点集分类问题</h2><h3 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h3><blockquote>
<p>这里我已经提前实现好了生成数据集的函数。本文的附录里会介绍这些函数的细节。</p>
</blockquote>
<p>使用项目里的 <code>generate_point_set</code> 函数可以生成一个平面点集分类数据集：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x, y, label = generate_point_set()</span><br><span class="line"><span class="comment"># x: [240]</span></span><br><span class="line"><span class="comment"># y: [240]</span></span><br><span class="line"><span class="comment"># label: [240]</span></span><br></pre></td></tr></table></figure></p>
<p>其中，<code>x[i]</code>是第i个点的横坐标，<code>y[i]</code>是第i个点的纵坐标，<code>label[i]</code>是第i个点的标签。标签为0表示是红色的点，标签为1表示是绿色的点。</p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>得到了原始数据后，我们要把数据处理成矩阵X和Y，其中X的形状是<code>[2, m]</code>，Y的形状是<code>[1, m]</code>，其中<code>m</code>是样本大小。之后，我们还需要把原始数据拆分成训练集和测试集。</p>
<p>第一步生成矩阵的代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = np.stack((x, y), axis=<span class="number">1</span>)</span><br><span class="line">Y = np.expand_dims(label, axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># X: [240, 2]</span></span><br><span class="line"><span class="comment"># Y: [240, 1]</span></span><br></pre></td></tr></table></figure></p>
<p>大家应该能猜出<code>stack</code>和<code>expand_dims</code>是什么意思。<code>stack</code>能把两个张量堆起来，比如这里把表示x,y坐标的一维向量合成起来，变成一个向量（长度为2）的向量（长度为240）。<code>expand_dims</code>就是凭空给张量加一个长度为1的维度，比如这里给<code>Y</code>添加了<code>axis=1</code>上的维度。</p>
<p>第二步划分数据集的方法如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">indices = np.random.permutation(X.shape[<span class="number">0</span>])</span><br><span class="line">X_train = X[indices[<span class="number">0</span>:<span class="number">200</span>], :].T</span><br><span class="line">Y_train = Y[indices[<span class="number">0</span>:<span class="number">200</span>], :].T</span><br><span class="line">X_test = X[indices[<span class="number">200</span>:], :].T</span><br><span class="line">Y_test = Y[indices[<span class="number">200</span>:], :].T</span><br><span class="line"><span class="comment"># X_train: [2, 200]</span></span><br><span class="line"><span class="comment"># Y_train: [1, 200]</span></span><br><span class="line"><span class="comment"># X_test: [2, 40]</span></span><br><span class="line"><span class="comment"># Y_test: [1, 40]</span></span><br></pre></td></tr></table></figure><br>注意，我们划分数据集的时候最好要随机划分。我这里使用<code>np.random.permutation</code>生成了一个排列，把这个排列作为下标来打乱数据集。</p>
<p>大家看不懂这段代码的话，可以想象这样一个例子：老师想抽10个人去值日，于是，他把班上同学的学号打乱，在打乱后的学号列表中，把前10个学号的同学叫了出来。代码里<code>indices</code>就是用随机排列生成的一个“打乱过的学号”，根据这个随机索引值，我们把前200个索引的数据当成训练集，200号索引之后的数据当成测试集。</p>
<p>经过这些处理，数据就符合课堂上讲过的形状要求了。</p>
<h3 id="使用模型"><a href="#使用模型" class="headerlink" title="使用模型"></a>使用模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">n_x = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">model1 = LogisticRegression(n_x)</span><br><span class="line">model2 = ShallowNetwork(n_x, <span class="number">2</span>)</span><br><span class="line">model3 = ShallowNetwork(n_x, <span class="number">4</span>)</span><br><span class="line">model4 = ShallowNetwork(n_x, <span class="number">10</span>)</span><br><span class="line">train_model(model1, X_train, Y_train, X_test, Y_test, <span class="number">500</span>, <span class="number">0.0001</span>, <span class="number">50</span>)</span><br><span class="line">train_model(model2, X_train, Y_train, X_test, Y_test, <span class="number">2000</span>, <span class="number">0.01</span>, <span class="number">100</span>)</span><br><span class="line">train_model(model3, X_train, Y_train, X_test, Y_test, <span class="number">2000</span>, <span class="number">0.01</span>, <span class="number">100</span>)</span><br><span class="line">train_model(model4, X_train, Y_train, X_test, Y_test, <span class="number">2000</span>, <span class="number">0.01</span>, <span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>由于我们前面已经定义好了模型，使用模型的过程就很惬意了。这里直接初始化我们自己编写的类，再用训练函数训练模型即可。</p>
<p>为了比较不同的模型，从感性上认识不同模型间的区别，在示例代码中我训练了4个模型。第一个模型是逻辑回归，后三个模型分别是隐藏层有2、4、10个神经元的单隐藏层神经网络。</p>
<p>模仿这堂课的编程作业，我也贴心地实现了模型可视化函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">visualize_X = generate_plot_set()</span><br><span class="line">plot_result = model4.forward(visualize_X, train_mode=<span class="literal">False</span>)</span><br><span class="line">visualize(X, Y, plot_result)</span><br></pre></td></tr></table></figure>
<p>只要运行上面这些代码，大家就可以看到模型具体是怎么分类2维平面上所有点的。让我们在下一节里看看这些函数的运行效果。</p>
<h3 id="实验报告"><a href="#实验报告" class="headerlink" title="实验报告"></a>实验报告</h3><p>好了，最好玩的地方来了。让我们有请四位选手，看看他们在二维点分类任务上表现如何。</p>
<p>首先是逻辑回归：</p>
<p><img src="/2022/05/23/DLS-note-3/6.jpg" alt></p>
<p>逻辑回归选手也太菜了吧！他只能模拟一条直线。这条直线虽然把下面两片红色花瓣包进去了，但忽略了左上角的花瓣。太弱了，太弱了！</p>
<p><img src="/2022/05/23/DLS-note-3/7.jpg" alt></p>
<p>隐藏层只有2个神经元的选手也菜得不行，和逻辑回归一起可谓是“卧龙凤雏”啊！</p>
<p><img src="/2022/05/23/DLS-note-3/8.jpg" alt></p>
<p><img src="/2022/05/23/DLS-note-3/9.jpg" alt></p>
<p>4-神经元选手似乎在尝试做出一些改变！好像有一次的运行结果还挺不错！但怎么我感觉他的发挥不是很稳定啊？他是在瞎蒙吧？</p>
<p>好，那我们最后上场的是4号选手10-神经元网络。4号选手可谓是受到万众的期待啊。据说，他有着“二维点分类小丸子”的称号，让我们来看一看他的表现:</p>
<p><img src="/2022/05/23/DLS-note-3/10.jpg" alt></p>
<p><img src="/2022/05/23/DLS-note-3/11.jpg" alt></p>
<p>只见4号网络手起刀落，刀刀见血。不论是怎么运行程序，他都能精准无误地把点集正确分类。我宣布，他就是本届点集分类大赛的冠军！让我们祝贺他！</p>
<p>程序里很多超参数是可调的，数据集也是可以随意修改的。欢迎大家去使用<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/ShallowNetwork">本课的代码</a>，比较一下不同的神经网络。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过这节课的编程练习后，大家应该掌握以下编程技能：</p>
<ul>
<li>编写单隐层神经网络的正向传播</li>
<li>编写单隐层神经网络的反向传播</li>
<li>正确初始化神经网络的参数</li>
<li>常见激活函数及其导数的实现</li>
</ul>
<p>此外，通过浏览我的项目，大家应该能够提前学到以下技能：</p>
<ul>
<li>在神经网络中使用<strong>缓存</strong>的方法保存数据</li>
</ul>
<p>当然，我相信我的项目里还展示了许多编程技术。这些技能严格来说不在本课程的要求范围内，大家可以自行体悟。</p>
<h2 id="附赠内容：如何画激活函数"><a href="#附赠内容：如何画激活函数" class="headerlink" title="附赠内容：如何画激活函数"></a>附赠内容：如何画激活函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.maximum(x, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leaky_relu</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.maximum(x, <span class="number">0.1</span> * x)</span><br></pre></td></tr></table></figure>
<p>先定义好激活函数的公式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line">y1 = sigmoid(x)</span><br><span class="line">y2 = tanh(x)</span><br><span class="line">y3 = relu(x)</span><br><span class="line">y4 = leaky_relu(x)</span><br></pre></td></tr></table></figure>
<p>画函数，其实就是生成函数上的一堆点，再把相邻的点用直线两两连接起来。为了生成函数上的点，我们先用<code>np.linspace(-3, 3, 100)</code>生成100个位于[-3, 3]上的x坐标值，用这些x坐标值算出每个函数的y坐标值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.axvline(x=<span class="number">0</span>, color=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.axhline(y=<span class="number">0</span>, color=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.plot(x, y1)</span><br><span class="line">plt.title(<span class="string">&#x27;sigmoid&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>之后就是调用API了。这里只展示一下sigmoid函数是怎么画出来的。<code>plt.subplot(a, b, c)</code>表示你要在一个a <em>x</em> b的网格里的第c个格子里画图。 <code>plt.axvline(x=0, color=&#39;k&#39;) plt.axhline(y=0, color=&#39;k&#39;)</code>用于生成x,y轴，<code>plt.plot(x, y1)</code>用于画函数曲线，<code>plt.title(&#39;sigmoid&#39;)</code>用于给图像写标题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>用类似的方法画完所有函数后，调用<code>plt.show()</code>把图片显示出来就大功告成了。</p>
<p>这段代码的链接：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/blob/master/dldemos/ShallowNetwork/plot_activation_func.py">https://github.com/SingleZombie/DL-Demos/blob/master/dldemos/ShallowNetwork/plot_activation_func.py</a></p>
<p>学API本身没有任何技术含量，知道API能做什么，有需求的时候去查API用法即可。</p>
<h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>我很早之前就在计划如何构建我的个人IP。没想到，从上周日开始，我不知不觉地开始认真地在公开渠道上发文章了。</p>
<p>发完文章后，我其实抱有很大的期待，希望能有很多人来读我的文章（哪怕是早已养成了不以他人的评价来评价自己的我，也不能免俗）。很可惜，文章似乎并不是很受欢迎。</p>
<p>还好我有着强大的自信心，心态一点也不受影响。首先，我自己有着强大的鉴别能力，在我自己来看，我的文章水准不低；其次，我的部署教程经 OpenMMLab 发表，受到了不少赞誉，客观上证明我当前的写作水平很强。文章不受欢迎，肯定另有原因。</p>
<p>首先，是我现在没有曝光度。这是当然的，毕竟我之前一点名气也没有，平台并不会去推荐你的文章，能够接触到你文章的人本来就少。另外，我的文章十分冗长，用我自己的话来讲，“根本不是给人来看的”（本来写文章的目的就是为了总结我自己的学习心得，提升我的学习效果）。虽然认真读起来，其实还可以，但几乎没有人有足够的动力去把我这些文章认真读完。</p>
<p>这两个问题，我都会去想办法解决。曝光度的问题我已经想好了办法，在这里就不提了。而第二点，文章可读性这点，对现在的我来说非常好解决。说实话，我不是写不出大家很愿意去读的文章，而是不愿写。如果你想去迎合他人的体验，那你肯定要付出额外的心血。我现在的主业是学习，不是搞自媒体，我之前比较高傲，懒得去把文章写得更加适合大部分人群阅读。但是，现在，我生气了，我认真了，我很不服气。我不是做不好，而是没有去做。我一旦出手，必定是一鸣惊人。</p>
<p>从这周开始，我的博客只发笔记原稿。发到其他平台上时，我会做一定的修改，使之阅读体验更好。</p>
<p>最可怕的是，我还是不会花大量的时间去讨好读者，我还是会保证我的学习工作不受影响。我会拿出我的真实实力，真正的人性洞察能力，真正的时间分配能力，真正的权衡利弊的能力，以最高效率生产出质量优秀的文章。以我这些精心写作的博客原稿为基础，我有自信生产出大量有趣、有深度的文章。我靠这些文字火不起来，可以理解，因为认真愿意去学深度学习的人，没有那么多。但是，我有足够的信心，我认为我的文章一定会受到很多人的好评。</p>
<p>另外，我刚刚是承认我仅凭这些深度学习教程文章是火不起来的。但我并没有承认我的个人IP火不起来。究竟我之后还会干出哪些大事？我这里不讲，且看历史是怎么发展的。</p>
<hr>
<p>嘿嘿嘿，为了准备之后的编程实况解说，我这一课的编程是一边在录制一边编的。结果我发挥超神，3小时左右就把这一课代码写完了，其中实现逻辑回归和通用分类器框架花了40分钟，实现神经网络花了20分钟，剩下时间都在捣鼓Numpy API，在可视化网络的输出结果。可以说我的编程水平相比普通人已经登峰造极了。但我还会继续精进我的编程技术，直至出神入化，神鬼莫及的境界。</p>
<p>顺带一提，第一次编写一个程序的直播是没有节目效果的。你大部分时间都会花在思考上，你脑子里想的东西是无法即时传递给观众的。哪怕是搞节目效果能力这么强的我，录出来的视频也不太好看。要做编程教学视频，必须要提前写一遍代码，第二次重新编同一段程序的时候，才有可能游刃有余地解说。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/05/10/DLS-note-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/10/DLS-note-2/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》笔记+代码实战（三）：简单的神经网络——逻辑回归</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-10 17:16:34" itemprop="dateCreated datePublished" datetime="2022-05-10T17:16:34+08:00">2022-05-10</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这堂课要学习的是逻辑回归——一种求解二分类任务的算法。同时，这堂课会补充实现逻辑回归必备的数学知识、编程知识。学完这堂课后，同学们应该能够用Python实现一个简单的小猫辨别器。</p>
<h1 id="学习提示"><a href="#学习提示" class="headerlink" title="学习提示"></a>学习提示</h1><p><img src="/2022/05/10/DLS-note-2/preface.png" alt></p>
<p>如上图所示，深度学习和编程，本来就是相对独立的两块知识。</p>
<p>深度学习本身的知识包括数学原理和实验经验这两部分。深度学习最早来自于数学中的优化问题。随着其结构的复杂化，很多时候我们解释不清为什么某个模型性能更高，只能通过重复实验来验证模型的有效性。因此，深度学习很多情况下变成了一门“实验科学”。</p>
<p>深度学习中，只有少量和编程有关系的知识，比如向量化计算、自动求导器等。得益于活跃的开源社区，只要熟悉了这些少量的编程技巧，人人都可以完成简单的深度学习项目。但是，真正想要搭建一个实用的深度学习项目，需要完成大量“底层”的编程工作，要求开发者有着广泛的编程经验。</p>
<p>通过上吴恩达老师的课，我们应该能比较好地掌握深度学习的数学原理，并且了解深度学习中少量的编程知识。而广泛的编程经验、修改模型的经验，这些都是只上这门课学不到的。</p>
<p>获取修改模型的经验这项任务过于复杂，不太可能短期学会，几乎可以作为研究生的课题了。而相对而言，编程的经验就很好获得了。</p>
<p>我的系列笔记会补充很多编程实战项目，希望读者能够通过完成类似的编程项目，在学习课内知识之余，提升广义上的编程能力。比如在这周的课程里，我们会用课堂里学到的逻辑回归从头搭建一个分类器。</p>
<h1 id="课堂笔记"><a href="#课堂笔记" class="headerlink" title="课堂笔记"></a>课堂笔记</h1><h2 id="本节课的目标"><a href="#本节课的目标" class="headerlink" title="本节课的目标"></a>本节课的目标</h2><p>在这节课里，我们要完成一个二分类任务。所谓二分类任务，就是给一个问题，然后给出一个“是”或“否”的回答。比如给出一张照片，问照片里是否有一只猫。</p>
<p>这节课中，我们用到的方法是逻辑回归。逻辑回归可以看成是一个非常简单的神经网络。</p>
<h3 id="符号标记"><a href="#符号标记" class="headerlink" title="符号标记"></a>符号标记</h3><p>从这节课开始，我们会用到一套统一的符号标记：</p>
<p>$(x, y)$ 是一个训练样本。其中，$x$ 是一个长度为 $n_x$ 的一维向量，即 $x \in \mathcal{R}^{n_x}$。$y$ 是一个实数，取0或1，即$y \in {0, 1}$。取0表示问题的的答案为“否”，取1表示问题的答案为“是”。</p>
<blockquote>
<p>这套课默认读者对统计机器学习有基本的认识，似乎没有过多介绍训练集是什么。在有监督统计机器学习中，会给出<strong>训练数据</strong>。训练数据中的每一条<strong>训练样本</strong>包含一个“问题”和“问题的答案”。神经网络根据输入的问题给出一个自己的解答，再和正确的答案对比，通过这样一个“学习”的过程来优化解答能力。</p>
<p>对计算机知识有所了解的人会知道，在计算机中，颜色主要是通过RGB（红绿蓝）三种颜色通道表示。每一种通道一般用长度8位的整数表示，即用一个0~255的数表示某颜色在红、绿、蓝上的深浅程度。这样，一个颜色就可以用一个长度为3的向量表示。一幅图像，其实就是许多颜色的集合，即许多长度为3的向量的集合。颜色通道，再算上某颜色所在像素的位置$(x, y)$，图像就可以看成一个3维张量$I \in \mathcal{R}^{H \times W \times 3}$，其中$H$是图像高度，$W$是图像宽度，$3$是图像的通道数。在把图像输入逻辑回归时，我们会把图像“拉直”成一个一维向量。这个向量就是前面提到的网络输入$x$，其中$x$的长度$n_x$满足$n_x = H \times W \times 3$。这里的“拉直”操作就是把张量里的数据按照顺序一个一个填入新的一维向量中。</p>
<p>其实向量就是一维的，但我还是很喜欢强调它是“一维”的。这是因为在计算机中所有数据都可以看成是数组（甚至C++的数组就叫<code>vector</code>)。二维数组不过是一维数组的数组，三位数组不过是二维数组的数组。在数学中，为了方便称呼，把一维数组叫“向量”，二维数组叫“矩阵”，三维及以上数组叫“张量”。其实在我看来它们之间只是一个维度的差别而已，叫“三维向量”、“一维张量”这种不是那么严谨的称呼也没什么问题。</p>
</blockquote>
<p>实际上，我们有很多个训练样本。样本总数记为$m$。第$i$个训练样本叫做$(x^{(i)}, y^{(i)})$。在后面使用其他标记时，也会使用上标$(i)$表示第$i$个训练样本得到的计算结果。</p>
<p>所有输入数据的集合构成一个矩阵（其中每个输入样本用<strong>列向量</strong>的形式表示，这是为了方便计算机的计算）：</p>
<script type="math/tex; mode=display">
X=\left[
  \begin{matrix}
  | & | & & | \\
  x^{(1)} & x^{(2)} & ... & x^{(m)} \\
  | & | & & |
  \end{matrix}
\right]
,X \in \mathcal{R}^{n_x \times m}</script><p>同理，所有真值也构成集合 $Y$:</p>
<script type="math/tex; mode=display">
Y=\left[
  \begin{matrix}
  y^{(1)} & y^{(2)} & ... & y^{(m)} 
  \end{matrix}
\right]
,Y \in \mathcal{R}^{m}</script><p>由于每个样本$y^{(i)}$是一个实数，所以集合$Y$是一个向量。</p>
<h2 id="逻辑回归的公式描述"><a href="#逻辑回归的公式描述" class="headerlink" title="逻辑回归的公式描述"></a>逻辑回归的公式描述</h2><p>逻辑回归是一个学习算法，用于对真值只有0或1的“逻辑”问题进行建模。给定输入$x$,逻辑回归输出一个$\hat{y}$。这个$\hat{y}$是对真值$y$的一个估计，准确来说，它描述的是$y=1$的概率，即$\hat{y}=P(y=1 \ | \ x)$</p>
<p>逻辑回归会使用一个带参数的函数计算$\hat{y}$。这里的参数包括$w \in \mathcal{R}^{n_x}, b \in \mathcal{R}$。</p>
<p>说起用于拟合的函数，最容易想到的是线性函数$w^Tx+b$（即做点乘再加$b$： $w^Tx+b = (\Sigma_{i=1}^{n_x}w_ix_i)+b $）。但线性函数的值域是$(- \infty,+\infty) $（即全体实数$\mathcal{R}$），概率的取值是$[0, 1]$。我们还需要一个定义域为$\mathcal{R}$，值域为$[0, 1]$，把线性函数映射到$[0, 1]$上的一个函数。</p>
<p>逻辑回归中，使用的映射函数是sigmoid函数$\sigma$,它的定义为：</p>
<script type="math/tex; mode=display">\sigma(z)=\frac{1}{1 + e^{-z}}</script><p>这个函数可以有效地完成映射，它的函数图像长这个样子：</p>
<p><img src="/2022/05/10/DLS-note-2/0.png" alt></p>
<blockquote>
<p>这里不用计较为什么使用这个函数，只需要知道这个函数的趋势：$x$越小，$\sigma (x)$越靠近0；$x$越大，$\sigma (x)$越靠近1。</p>
</blockquote>
<p>也就是说，最终的逻辑回归公式长这个样子：$\hat{y} = \sigma(w^Tx+b)$。</p>
<h2 id="逻辑回归的损失函数（Cost-Function）"><a href="#逻辑回归的损失函数（Cost-Function）" class="headerlink" title="逻辑回归的损失函数（Cost Function）"></a>逻辑回归的损失函数（Cost Function）</h2><p>所有的机器学习问题本质上是一个优化问题，一般我们会定义一个<strong>损失函数（Cost Function）</strong>，再通过优化参数来最小化这个损失函数。</p>
<p>回顾一下我们的任务目标：我们定义了逻辑回归公式$\hat{y} = \sigma(w^Tx+b)$，我们希望$\hat{y}$尽可能和$y$相近。这里的“相近”，就是我们的优化目标。损失函数，可以看成是$y, \hat{y}$间的“距离”。</p>
<p>逻辑回归中，定义了每个输出和真值的<strong>误差函数（Loss Function）</strong>，这个误差函数叫<strong>交叉熵</strong></p>
<script type="math/tex; mode=display">L(\hat{y}, y)=-(y \ log\hat{y} + (1-y) \ log(1-\hat{y}))</script><p>不使用另一种常见的误差函数<strong>均方误差</strong>的原因是，交叉熵较均方误差梯度更加平滑，更容易在之后的优化中找到全局最优解。</p>
<p><strong>误差函数</strong>是定义在每个样本上的，而<strong>损失函数</strong>是定义在整个样本上的，表示所有样本误差的“总和”。这个“总和”其实就是平均值，即损失函数$J(w, b)$为:</p>
<script type="math/tex; mode=display">J(w, b)=\frac{1}{m}\Sigma_{i=1}^{m}-(y^{(i)} \ log\hat{y}^{(i)} + (1-y^{(i)}) \ log(1-\hat{y}^{(i)}))</script><h2 id="优化算法——梯度下降"><a href="#优化算法——梯度下降" class="headerlink" title="优化算法——梯度下降"></a>优化算法——梯度下降</h2><p>有了优化目标，接下来的问题就是如何用优化算法求出最优值。这里使用的是<strong>梯度下降（Gradient Descent）</strong> 法。梯度下降的思想很符合直觉：如果要让函数值更小，就应该让函数的输入沿着函数值下降最快的方向（梯度的方向）移动。</p>
<p>以课件中的一元函数为例：</p>
<p><img src="/2022/05/10/DLS-note-2/1.png" alt></p>
<p>一元函数的梯度值就是导数值，方向只有正和负两个方向。我们要根据每个点的导数，让每个点向左或向右“运动”，以使函数值更小。</p>
<p>从图像里可以看出，如果是参数最开始在A点，则往右走函数值才会变少；反之，对于B点，则应该往左移动。</p>
<p>每个点都应该向最小值“一小步一小步”地移动，直至抵达最低点。为什么要“一小步”移动呢？可以想象，如果一次移动的“步伐”过大，改变参数不仅不会让优化函数变小，甚至会让待优化函数变大。比如从A点开始，同样是往右移动，如果“步伐”过大，A点就会迈过最低点的红点，甚至跑到B点的上面。那么这样下去，待优化函数会越来越大，优化就失败了。</p>
<p>为了让优化能顺利进行，梯度下降法使用<strong>学习率（Learning Rate)</strong> 来控制参数优化的“步伐”，即用如下方法更新损失函数$J(w)$的参数：</p>
<script type="math/tex; mode=display">
Repeat: \\
w \gets w - \alpha \frac{dJ}{dw}</script><p>这里的 $\alpha$ 就是学习率，它控制了每次梯度更新的幅度。</p>
<p>其实这里还有两个问题：参数$w$该如何初始化；该执行梯度下降多少次。在这个问题中初始化对结果影响不大，可以简单地令$w=0$。而优化的次数没有硬性的需求，先执行若干次，根据误差是否收敛再决定是否继续优化即可。</p>
<h2 id="前置知识补充"><a href="#前置知识补充" class="headerlink" title="前置知识补充"></a>前置知识补充</h2><p>到这里，逻辑回归的知识已经讲完了。让我们梳理一下：</p>
<p>在逻辑回归问题中，我们有输入样本集$X$和其对应的期望输出$Y$，我们希望找到拟合函数$\hat{Y}=w^TX+b$，使得$\hat{Y}$和$Y$尽可能接近，即让损失函数$J(w, b)=mean(-(Ylog\hat{Y}+(1-Y)log(1-\hat{Y})))$尽可能小。</p>
<blockquote>
<p>这里的$X,Y,\hat{Y}$表示的是全体样本。稍后我们会讨论如何用公式表示全体样本的计算。</p>
</blockquote>
<p>我们可以用$0$来初始化所有待优化参数$w, b$，并执行梯度下降</p>
<script type="math/tex; mode=display">
\begin{align*}
w & \gets w - \alpha \frac{dJ}{dw} \\ 
b & \gets b - \alpha \frac{dJ}{db}
\end{align*}</script><p>若干次后得到一个较优的拟合函数。</p>
<p>为了让大家成功用代码实现逻辑回归，这门课贴心地给大家补充了数学知识和编程知识。</p>
<blockquote>
<p>在我的笔记中，补充编程知识的记录会潦草一些。</p>
</blockquote>
<h3 id="求导"><a href="#求导" class="headerlink" title="求导"></a>求导</h3><blockquote>
<p>这部分对中国学生来说十分简单，因为求导公式是高中教材的内容。</p>
</blockquote>
<p>导数即函数每时每刻的变化率，比如位移对时间的导数就是速度。以常见函数为例，对于直线$y=kx$，函数的变化率时时刻刻都是$k$；对于二次函数$y=x^2$，$x$处的导数是$2x$。</p>
<h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p>其实，所有复杂的数学运算都可以拆成计算图表示法。</p>
<p><img src="/2022/05/10/DLS-note-2/2.png" alt></p>
<blockquote>
<p>计算<strong>图</strong>中的”图”其实是一个计算机概念，表示由节点和边组成的集合。不熟悉的话，当成日常用语里的图来理解也无妨。</p>
</blockquote>
<p>比如上图中，哪怕是简单的运算$2a+b$，也可以拆成两步：先算$2 \times a$，再算$(2a) + b$。</p>
<p>这里的“步”指原子运算，即最简单的运算。原子运算可以是加减乘除，也可以是求指数、求对数。复杂的运算，只是对简单运算的组合、嵌套。</p>
<p>明明简简单单可以用一行公式表示的事，要费很大的功夫画一张计算图呢？这是因为，对函数求导满足“链式法则”，借助计算图，可以更方便地用链式法则算出所有参数的导数。比如在上图中要求$f$对$a$的导数，使用链式法则的话，可以通过先求$f$对$c$的导数，再求$c$对$a$的导数得到。</p>
<h3 id="利用计算图对逻辑回归求导"><a href="#利用计算图对逻辑回归求导" class="headerlink" title="利用计算图对逻辑回归求导"></a>利用计算图对逻辑回归求导</h3><p>逻辑回归有计算图：<br><img src="/2022/05/10/DLS-note-2/3.png" alt></p>
<p>现在利用链式法则从右向左求导：</p>
<script type="math/tex; mode=display">
\begin{align*}
z & =  w_1x_1 + w_2x_2 +b \\
a & =  \frac{1}{1+e^{-z}} \\
L & = -(yloga+(1-y)log(1-a)) \\

\frac{dL}{da} & =  -(\frac{y}{a}-\frac{1-y}{1-a}) \\
\frac{da}{dz} & =  \frac{e^{-z}}{(1+e^{-z})^2} = a(1-a)\\
\frac{dL}{dz} & = \frac{dL}{da} \frac{da}{dz} \\
&= -(\frac{y}{a}-\frac{1-y}{1-a}) \times a(1-a) \\
&= -(y(1-a)-(1-y)a) \\
&= -(y-ya-a+ya) \\
&= a-y \\
\frac{dL}{dw_i} &= \frac{dL}{dz}\frac{dz}{dw_i}=(a-y)x_i \\
\frac{dL}{db} &= \frac{dL}{dz}\frac{dz}{db}=(a-y)
\end{align*}</script><blockquote>
<p>这些运算里最难“注意到”的是$\frac{e^{-z}}{(1+e^{-z})^2} = a(1-a)$。</p>
<p>在学计算机科学的知识时，可以适当忽略一些数学证明，把算好的公式直接拿来用，比如这里的$\frac{dL}{dz}=a-y$。</p>
</blockquote>
<p>$\frac{dL}{dw_i}, \frac{dL}{db}$就是我们要的梯度了，用它们去更新原来的参数即可。值得一提的是，这里的梯度是对一个样本而言。对于全部$m$个样本来说，本轮的梯度应该是所有样本的梯度的平均值。后面我们会学习如何对所有样本求导。</p>
<h3 id="Python-向量化计算"><a href="#Python-向量化计算" class="headerlink" title="Python 向量化计算"></a>Python 向量化计算</h3><p>在刚刚的一轮迭代中，我们要用到两次循环：</p>
<ol>
<li>对$m$个样本循环处理</li>
<li>对$n_x$个权重$w_i$与对应的$x_i$相乘</li>
</ol>
<p>直接拿 Python 写这些 for 循环，程序会跑得很慢的，这里最好使用向量化计算。在这一节里我们补充一下 Python 基础知识，下一节介绍怎么用它们实现逻辑回归的向量化实现。</p>
<blockquote>
<p>课程中提到向量化的好处是可以用<strong>SIMD</strong>（单指令多数据流）优化，这个概念可以理解成计算机会同时对16个或32个数做计算。如果输入的数据是向量的话，相比一个一个做for循环，一次算16,32个数的计算速度会更快。</p>
<p>但实际上，除了无法使用SIMD以外，Python的低效也是拖慢速度的原因之一。哪怕是不用SIMD，单纯地用C++的for循环实现向量化计算，都能比用Python的循环快上很多。</p>
</blockquote>
<p>Python 的 numpy 库提供了向量化计算的接口。比如以下是向量化的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.zeros((<span class="number">10</span>)) <span class="comment"># 新建长度为10的向量，值为0</span></span><br><span class="line">b = np.ones((<span class="number">10</span>)) <span class="comment"># 新建长度为10的向量，值为1</span></span><br><span class="line">a = a + b <span class="comment"># 10个数同时做加法</span></span><br><span class="line">a = np.exp(a) <span class="comment"># 对10个数都做指数运算</span></span><br></pre></td></tr></table></figure>
<p>numpy 允许一种叫做“广播”的操作，这种操作能够完成不同形状数据间的运算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.ones(<span class="number">10</span>) <span class="comment"># a的形状:[10]</span></span><br><span class="line">k = np.array([<span class="number">3</span>]) <span class="comment"># 用列表[3]新建张量，k的形状:[1]</span></span><br><span class="line">a = k * a <span class="comment"># 广播</span></span><br></pre></td></tr></table></figure>
<p>这里k的shape为<code>[1]</code>，a的shape为<code>[10]</code>。用k乘a，实际上就是令<code>a[i] = k[0] * a[i]</code>。也就是说，<code>k[0]</code>“广播”到了<code>a</code>的每一个元素上。</p>
<p>有一种快速理解广播的方法：可以认为k的形状从<code>[1]</code>变成了<code>[10]</code>，再让k和a逐个元素做乘法。</p>
<p>同理，如果用一个<code>a[x, y]</code>的矩阵加一个<code>b[x, 1]</code>的矩阵，实际上是做了下面的运算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x):</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(y):</span><br><span class="line">    a[i, j] = a[i, j] + b[i, <span class="number">0</span>] </span><br></pre></td></tr></table></figure></p>
<p>用刚刚介绍的方法来理解，可以认为<code>b</code>从<code>[x, 1]</code>扩充成了<code>[x, y]</code>，再和<code>a</code>做逐个元素的加法运算。</p>
<h2 id="向量化计算前向和反向传播"><a href="#向量化计算前向和反向传播" class="headerlink" title="向量化计算前向和反向传播"></a>向量化计算前向和反向传播</h2><p>现在，有了求导的基础知识和向量化计算的基础知识，让我们来写一下如何用矩阵表示逻辑回归中的运算，并用Python代码描述这些计算过程。</p>
<p>单样本的正向传播：</p>
<script type="math/tex; mode=display">
\hat{y} = a=\sigma(w^Tx+b)</script><p>推广到多样本：</p>
<script type="math/tex; mode=display">
\hat{Y} = A=\sigma(w^TX+b)</script><p>这里的$X, A, \hat{Y}$是把原来单样本的列向量$x_i, \hat{y}_i$横向堆叠起来形成的矩阵，即:</p>
<script type="math/tex; mode=display">
[\hat{y_1}, ..., \hat{y_m}] = \sigma([w^Tx_1+b, ..., w^Tx_m+b])</script><p>单样本反向传播：</p>
<script type="math/tex; mode=display">
\begin{align*}
dz &= a-y \\
dw_i &= dz \cdot x_i \\
dw &= \left[
  \begin{matrix}
  dw_1 \\
  ... \\
  dw_{n_x} 
  \end{matrix}
\right] =
\left[
  \begin{matrix}
  dz \cdot x_1\\
  ... \\
  dz \cdot x_{n_x}
  \end{matrix}
\right]=dz \ast x\\
db &= dz 
\end{align*}</script><blockquote>
<p>$dz$ 是 $\frac{dJ}{dz}$的简写，其他变量同理。编程时也按同样的方式命名。</p>
<p>所有的$\ast$都表示逐元素乘法。比如$[1, 2, 3] \ast [1, 2, 3]=[1, 4, 9]$。$\ast$满足前面提到的广播，比如$[2] \ast [1, 2, 3]=[2, 4, 6]$。 </p>
</blockquote>
<p>多样本反向传播：</p>
<script type="math/tex; mode=display">
\begin{align*}
dZ &= A-Y \\
dw_i &=  X_i dZ^T = dz^{(1)}x_i^{(1)} +  ... +  dz^{(m)}x_i^{(m)} \\
dw &=  \frac{1}{m} \left[
  \begin{matrix}
  dw_1 \\
  ... \\
  dw_{n_x} 
  \end{matrix}
\right]=\frac{1}{m}\left[
  \begin{matrix}
  dz^{(1)}x_1^{(1)} + &...& + dz^{(m)}x_1^{(m)}\\
  ... &...& ...\\
  dz^{(1)}x_{n_x}^{(1)} + &...& +dz^{(m)}x_{n_x}^{(m)} 
  \end{matrix}
\right]\\
&= \frac{1}{m}XdZ^T\\
db &= \frac{1}{m} \Sigma_{i=1}^m dZ^{(i)} 
\end{align*}</script><p>用代码描述多样本前向传播和反向传播就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T, x)+b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dZ = A-Y</span><br><span class="line">dw = np.dot(X, dZ.T) / m</span><br><span class="line">db = np.mean(dZ)</span><br><span class="line"><span class="comment"># db=np.sum(dZ) / m</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>np.dot</code>实现了求向量内积或矩阵乘法，<code>np.sum</code>实现了求和，<code>np.mean</code>实现了求均值。</p>
</blockquote>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这堂课的主要知识点有：</p>
<ul>
<li>什么是二分类问题。</li>
<li>如何对建立逻辑回归模型。<ul>
<li>Sigmoid 函数 $\sigma(z)=\frac{1}{1 + e^{-z}}$</li>
</ul>
</li>
<li>误差函数与损失函数<ul>
<li>逻辑回归的误差函数：$L(\hat{y}, y)=-(y \ log\hat{y} + (1-y) \ log(1-\hat{y}))$</li>
</ul>
</li>
<li>用梯度下降算法优化损失函数</li>
<li>计算图的概念及如何利用计算图算梯度</li>
</ul>
<p>学完这堂课后，应该掌握的编程技能有：</p>
<ul>
<li>了解numpy基本知识<ul>
<li>resize</li>
<li>.T</li>
<li>exp</li>
<li>dot</li>
<li>mean, sum</li>
</ul>
</li>
<li>用numpy做向量化计算</li>
<li>实现逻辑回归<ul>
<li>对输入数据做reshape的预处理</li>
<li>用向量化计算算$\hat{y}$及参数的梯度</li>
<li>迭代优化损失函数</li>
</ul>
</li>
</ul>
<h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h1><p>这节课有两个编程作业:第一个作业要求使用numpy实现对张量的一些操作，第二个作业要求用逻辑回归实现一个分类器。这些编程作业是在python的notebook上编写的。每道题给出了代码框架，只要写关键的几行代码就行。对我来说，编程体验极差。作为编程最强王者，怎能受此“嗟来之码”的屈辱？我决定从零开始，自己收集数据，并用numpy实现逻辑回归。</p>
<blockquote>
<p>其实我不分享作业代码的真正原因是：Coursera不允许公开展示作业代码。在之后的笔记中，我也会分享如何用自己的代码实现每堂课的编程目标。</p>
</blockquote>
<p>这篇笔记用到的代码已在GitHub上开源：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/LogisticRegression">https://github.com/SingleZombie/DL-Demos/tree/master/LogisticRegression</a> 。下文展示的代码和原本的代码有略微的出入，建议大家对着源代码阅读后文。</p>
<h2 id="程序设计"><a href="#程序设计" class="headerlink" title="程序设计"></a>程序设计</h2><p>不管写什么程序，都要先想好整体的架构，再开始动手写代码。</p>
<p>深度学习项目的架构比较固定。一般一个深度学习项目由以下几部分组成：</p>
<ul>
<li>数据预处理</li>
<li>定义网络结构</li>
<li>定义损失函数</li>
<li>定义优化策略</li>
<li>用训练pipeline串联起网络、损失函数、优化策略</li>
<li>测试模型精度</li>
</ul>
<p>当然，实现深度学习项目比一般的编程项目多一个步骤：除了写代码外，完成深度学习项目还需要收集数据。</p>
<p>接下来，我将按照<strong>数据收集</strong>、<strong>数据处理</strong>、<strong>网络结构</strong>、<strong>损失函数</strong>、<strong>训练</strong>、<strong>测试</strong>这几部分介绍这个项目。之后的笔记也会以这个形式介绍编程项目。</p>
<h2 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h2><p>说起最经典的二分类任务，大家都会想起小猫分类（或许跟吴恩达老师的课比较流行有关）。在这个项目中，我也顺应潮流，选择了一个猫狗数据集（<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/fusicfenta/cat-and-dog?resource=download）。">https://www.kaggle.com/datasets/fusicfenta/cat-and-dog?resource=download）。</a></p>
<p>在此数据集中，数据是按以下结构存储的：</p>
<p><img src="/2022/05/10/DLS-note-2/4.png" alt></p>
<p>在二分类任务中，数据的标签为0或1（表示是否是小猫）。而此数据集只是把猫、狗的图片分别放到了不同的文件夹里，这意味着我们待会儿要手动给这些数据打上0或1的标签。</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>由于训练集和测试集的目录结构相同，我们先写一个读数据集的函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">input_shape=(<span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span>(<span class="params"><span class="built_in">dir</span>, data_num</span>):</span></span><br><span class="line">    cat_images = glob(osp.join(<span class="built_in">dir</span>, <span class="string">&#x27;cats&#x27;</span>, <span class="string">&#x27;*.jpg&#x27;</span>))</span><br><span class="line">    dog_images = glob(osp.join(<span class="built_in">dir</span>, <span class="string">&#x27;dogs&#x27;</span>, <span class="string">&#x27;*.jpg&#x27;</span>))</span><br><span class="line">    cat_tensor = []</span><br><span class="line">    dog_tensor = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(cat_images):</span><br><span class="line">        <span class="keyword">if</span> idx &gt;= data_num:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        i = cv2.imread(image) / <span class="number">255</span></span><br><span class="line">        i = cv2.resize(i, input_shape)</span><br><span class="line">        cat_tensor.append(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(dog_images):</span><br><span class="line">        <span class="keyword">if</span> idx &gt;= data_num:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        i = cv2.imread(image) / <span class="number">255</span></span><br><span class="line">        i = cv2.resize(i, input_shape)</span><br><span class="line">        dog_tensor.append(i)</span><br><span class="line"></span><br><span class="line">    X = cat_tensor + dog_tensor</span><br><span class="line">    Y = [<span class="number">1</span>] * <span class="built_in">len</span>(cat_tensor) + [<span class="number">0</span>] * <span class="built_in">len</span>(dog_tensor)</span><br><span class="line">    X_Y = <span class="built_in">list</span>(<span class="built_in">zip</span>(X, Y))</span><br><span class="line">    shuffle(X_Y)</span><br><span class="line">    X, Y = <span class="built_in">zip</span>(*X_Y)</span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure><br>函数先是用<code>glob</code>读出文件夹下所有猫狗的图片路径，再按文件路径依次把文件读入。接着，函数为数据生成了0或1的标签。最后，函数把数据打乱，并返回数据。让我们来看看这段代码里有哪些要注意的地方。</p>
<p>在具体介绍代码之前，要说明一下我在这个数据集上做的两个特殊处理：</p>
<ol>
<li>这个函数有一个参数<code>data_num</code>，表示我们要读取<code>data_num</code>张猫+<code>data_num</code>张狗的数据。原数据集有上千张图片，直接读进内存肯定会把内存塞爆。为了实现上的方便，我加了一个控制数据数量的参数。在这个项目中，我只用了800张图片做训练集。</li>
<li>原图片是很大的，为了节约内存，我把所有图片都变成了input_shape=(224, 224)的大小。</li>
</ol>
<p>接下来，我们再了解一下数据处理中的一些知识。在读数据的时候，把数据<strong>归一化</strong>（令数据分布在(-1, 1)这个区间内）十分关键。如果不这样做的话，loss里的$loge^{z}$会趋近$log0$，梯度的收敛速度会极慢，训练会难以进行。这是这节课上没有讲的内容，但是它在实战中非常关键。</p>
<blockquote>
<p>这个时候输出loss的话，会得到一个Python无法表示的数字：<code>nan</code>。在训练中如果看到loss是<code>nan</code>，多半就是数据没有归一化的原因。这个是一个非常常见的bug，一定要记得做数据归一化！</p>
<p>第三节课里讲了激活函数的收敛速度问题。</p>
</blockquote>
<p>现在来详细看代码。</p>
<p>下面的代码用于从文件系统中读取所有图片文件，并把文件的绝对路径保存进一个list。如果大家有疑问，可以自行搜索<code>glob</code>函数的用法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat_images = glob(osp.join(<span class="built_in">dir</span>, <span class="string">&#x27;cats&#x27;</span>, <span class="string">&#x27;*.jpg&#x27;</span>))</span><br><span class="line">dog_images = glob(osp.join(<span class="built_in">dir</span>, <span class="string">&#x27;dogs&#x27;</span>, <span class="string">&#x27;*.jpg&#x27;</span>))</span><br></pre></td></tr></table></figure></p>
<p>在之后的两段for循环中，我们通过设定循环次数来控制读取的图片数。在循环里，我们先读入文件，再归一化文件，最后把图片resize到(224, 224)。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(cat_images):</span><br><span class="line">    <span class="keyword">if</span> idx &gt;= data_num:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    i = cv2.imread(image) / <span class="number">255</span></span><br><span class="line">    i = cv2.resize(i, input_shape)</span><br><span class="line">    cat_tensor.append(i)</span><br></pre></td></tr></table></figure><br>在这段代码里，归一化是靠</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">i = cv2.imread(image) / <span class="number">255</span></span><br></pre></td></tr></table></figure>
<p>实现的。</p>
<blockquote>
<p>这里我们知道输入是图像，颜色通道最大值是255，所以才这样归一化。在很多问题中，我们并不知道数据的边界是多少，这个时候只能用普通的归一化方法了。一种简单的归一化方法是把每个输入向量的模设为1。后面的课程里会详细介绍归一化方法。</p>
</blockquote>
<p>读完数据后，我们用以下代码生成了训练输入和对应的标签：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = cat_tensor + dog_tensor</span><br><span class="line">Y = [<span class="number">1</span>] * <span class="built_in">len</span>(cat_tensor) + [<span class="number">0</span>] * <span class="built_in">len</span>(dog_tensor)</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>Python里，<code>[1] * 10</code>可以把列表<code>[1]</code>复制10次。</p>
</blockquote>
<p>现在，我们的数据是“[猫，猫，猫……狗，狗，狗]”这样整整齐齐地排列着，没有打乱。由于我们是一次性拿整个训练集去训练，训练数据不打乱倒也没事。但为了兼容之后其他训练策略，这里我还是习惯性地把数据打乱了：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_Y = <span class="built_in">list</span>(<span class="built_in">zip</span>(X, Y))</span><br><span class="line">shuffle(X_Y)</span><br><span class="line">X, Y = <span class="built_in">zip</span>(*X_Y)</span><br></pre></td></tr></table></figure><br>使用这三行“魔法Python”可以打乱<code>list</code>对中的数据。</p>
<p>有了读一个文件夹的函数<code>load_dataset</code>，用下面的代码就可以读训练集和测试集：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span>(<span class="params"><span class="built_in">dir</span>=<span class="string">&#x27;data/archive/dataset&#x27;</span>, input_shape=(<span class="params"><span class="number">224</span>, <span class="number">224</span></span>)</span>):</span></span><br><span class="line">    train_X, train_Y = load_dataset(osp.join(<span class="built_in">dir</span>, <span class="string">&#x27;training_set&#x27;</span>), <span class="number">400</span>)</span><br><span class="line">    test_X, test_Y = load_dataset(osp.join(<span class="built_in">dir</span>, <span class="string">&#x27;test_set&#x27;</span>), <span class="number">100</span>)</span><br><span class="line">    <span class="keyword">return</span> train_X, train_Y, test_X, test_Y</span><br></pre></td></tr></table></figure><br>这里训练集有400+400=800张图片，测试集有100+100=200张图片。如果大家发现内存还是占用太多的话，可以改小这两个数字。</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>在这个项目中，我们使用的是逻辑回归算法。它可以看成是只有一个神经元的神经网络。如之前的课堂笔记所述，我们网络的公式是：</p>
<script type="math/tex; mode=display">
\hat{y} = \sigma(w^Tx+b)</script><p>这里我们要实现两个函数：</p>
<ol>
<li>resize_input：由于图片张量的形状是[h, w, c]（高、宽、颜色通道），而网络的输入是一个列向量，我们要把图片张量resize一下。</li>
<li>sigmoid: 我们要用<code>numpy</code>函数组合出一个<code>sigmoid</code>函数。</li>
</ol>
<p>熟悉了<code>numpy</code>的API后，实现这两个函数还是很容易的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resize_input</span>(<span class="params">a: np.ndarray</span>):</span></span><br><span class="line">    h, w, c = a.shape</span><br><span class="line">    a.resize((h * w * c))</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure></p>
<p>这里我代码实现上写得有点“脏”，调用<code>resize_input</code>做数据预处理是放在<code>main</code>函数里的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y, test_X, test_Y = generate_data()</span><br><span class="line"></span><br><span class="line">train_X = [resize_input(x) <span class="keyword">for</span> x <span class="keyword">in</span> train_X]</span><br><span class="line">test_X = [resize_input(x) <span class="keyword">for</span> x <span class="keyword">in</span> test_X]</span><br><span class="line">train_X = np.array(train_X).T</span><br><span class="line">train_Y = np.array(train_Y)</span><br><span class="line">train_Y = train_Y.reshape((<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">test_X = np.array(test_X).T</span><br><span class="line">test_Y = np.array(test_Y)</span><br><span class="line">test_Y = test_Y.reshape((<span class="number">1</span>, -<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>array = array.reshape(a, b)</code> 等价于 <code>array.resize(a, b)</code>。但是，<code>reshape</code>的某一维可以写成<code>-1</code>，表示这一维的大小让程序自己用除法算出来。比如总共有<code>a * b</code>个元素，调用<code>reshape(-1, a)</code>，<code>-1</code>的那一维会变成<code>b</code>。</p>
</blockquote>
<p>经过这些预处理代码，X的shape会变成[$n_x$, $m$]，Y的shape会变成[$1$, $m$]，和课堂里讲的内容一致。</p>
<p>有了sigmoid函数和正确shape的输入，我们可以写出网络的推理函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">w, b, X</span>):</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(np.dot(w.T, X) + b)</span><br></pre></td></tr></table></figure>
<h2 id="损失函数与梯度下降"><a href="#损失函数与梯度下降" class="headerlink" title="损失函数与梯度下降"></a>损失函数与梯度下降</h2><p>如前面的笔记所述，损失函数可以用下面的方法计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(-(y * np.log(y_hat) + (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - y_hat)))</span><br></pre></td></tr></table></figure>
<p>我们定义损失函数，实际上为了求得每个参数的梯度。在求梯度时，其实用不到损失函数本身，只需要知道每个参数对于损失函数的导数。在这个项目中，损失函数只用于输出，以监控当前的训练进度。</p>
<p>而在梯度下降中，我们不需要用到损失函数，只需要算出每个参数的梯度并执行梯度下降：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">w, b, X, Y, lr</span>):</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    Z = np.dot(w.T, X) + b</span><br><span class="line">    A = sigmoid(Z)</span><br><span class="line">    d_Z = A - Y</span><br><span class="line">    d_w = np.dot(X, d_Z.T) / m</span><br><span class="line">    d_b = np.mean(d_Z)</span><br><span class="line">    <span class="keyword">return</span> w - lr * d_w, b - lr * d_b</span><br></pre></td></tr></table></figure>
<p>在这段代码中，我们根据前面算好的公式，算出了<code>w, b</code>的梯度并对<code>w, b</code>进行更新。</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">train_X, train_Y, step=<span class="number">1000</span>, learning_rate=<span class="number">0.00001</span></span>):</span></span><br><span class="line">    w, b = init_weights()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;learning rate: <span class="subst">&#123;learning_rate&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(step):</span><br><span class="line">        w, b = train_step(w, b, train_X, train_Y, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出当前训练进度</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            y_hat = predict(w, b, train_X)</span><br><span class="line">            ls = loss(y_hat, train_Y)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;step <span class="subst">&#123;i&#125;</span> loss: <span class="subst">&#123;ls&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure>
<p>有了刚刚的梯度下降函数<code>train_step</code>，训练实现起来就很方便了。我们只需要设置一个训练总次数<code>step</code>，再调用<code>train_step</code>更新参数即可。</p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>在深度学习中，我们要用一个网络从来没有见过的数据集做测试，以验证网络能否泛化到一般的数据上。这里我们直接使用数据集中的<code>test_set</code>，用下面的代码计算分类任务的准确率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">w, b, test_X, test_Y</span>):</span></span><br><span class="line">    y_hat = predict(w, b, test_X)</span><br><span class="line">    predicts = np.where(y_hat &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    score = np.mean(np.where(predicts == test_Y, <span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;score&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>这里的<code>np.where</code>没有在课堂里讲过，这里补充介绍一下。<code>predicts=np.where(y_hat &gt; 0.5, 1, 0)</code>这一行，等价于下面的循环：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建一个和y_hat一样形状的ndarray</span></span><br><span class="line">predicts = np.zeros(y_hat.shape)</span><br><span class="line"><span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(y_hat):</span><br><span class="line">  <span class="keyword">if</span> v &gt; <span class="number">0.5</span>:</span><br><span class="line">    predicts[i] = <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    predicts[i] = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>也就是说，我们对<code>y_hat</code>做了逐元素的判断<code>v &gt; 0.5?</code>，如果判断成立，则赋值<code>1</code>，否则赋值<code>0</code>。这就好像是一个老师在批改学生的作业，如果对了，就给1分，否则给0分。</p>
<p><code>y_hat &gt; 0.5</code>是有实际意义的：在二分类问题中，如果网络输出图片是小猫的概率大于0.5，我们就认为图片就是小猫的图片；否则，我们认为不是。</p>
<p>之后，我们用另一个<code>(np.where(predicts == test_Y, 1, 0)</code>来“批改作业”：如果预测值和真值一样，则打1分，否则打0分。</p>
<p>最后，我们用<code>score = np.mean(...)</code>算出每道题分数的平均值，来给整个网络的表现打一个总分。</p>
<p>这里要注意一下，整个项目中我们用了两个方式来评价网络：我们监控了<code>loss</code>,因为<code>loss</code>反映了网络在<strong>训练集</strong>上的表现；我们计算了网络在测试集上的准确度，因为准确度反映了网络在<strong>一般数据</strong>上的表现。之后的课堂里应该也会讲到如何使用这些指标来进一步优化网络，这里会算它们就行了。</p>
<h2 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h2><p>嘿嘿，想不到吧，除了之前计划的章节外，这里还多了一个趣味性比较强的调参章节。</p>
<h3 id="使用错误代码得到的结果，千万不要学我"><a href="#使用错误代码得到的结果，千万不要学我" class="headerlink" title="使用错误代码得到的结果，千万不要学我"></a>使用错误代码得到的结果，千万不要学我</h3><p>搞深度学习，最好玩的地方就是调参数了。通过优化网络的超参数，我们能看到网络的性能在不断变好，准确率在不断变高。这个感觉就和考试分数越来越高，玩游戏刷的伤害越来越高给人带来的成就感一样。</p>
<p>在这个网络中，可以调的参数只有一个学习率。通过玩这个参数，我们能够更直观地认识学习率对梯度下降的影响。</p>
<p>这里我分享一下我的调参结果：</p>
<p>如果学习率&gt;=0.0003，网络更新的步伐过大，从而导致梯度不收敛，训练失败。</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">learning rate: 0.0003</span><br><span class="line">step 0 loss: 0.6918513655136874</span><br><span class="line">step 10 loss: 0.9047000002073068</span><br><span class="line">step 20 loss: 0.9751763789675365</span><br></pre></td></tr></table></figure>
<p>学习率==0.0002的话，网络差不多能以最快的速度收敛。</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">learning rate: 0.0002</span><br><span class="line">step 0 loss: 0.692168431534233</span><br><span class="line">step 10 loss: 0.684254876013497</span><br><span class="line">step 20 loss: 0.6780829877162996</span><br></pre></td></tr></table></figure>
<p>学习率==0.0001,甚至==0.00003也能训练，但是训练速度会变慢。</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">learning rate: 0.0001</span><br><span class="line">step 0 loss: 0.6926003513589579</span><br><span class="line">step 10 loss: 0.6883167092427446</span><br><span class="line">step 20 loss: 0.684621635180076</span><br></pre></td></tr></table></figure>
<p>这里判断网络的收敛速度时，要用到的指标是<strong>损失函数</strong>。我的代码里默认每10次训练输出一次损失函数的值。</p>
<blockquote>
<p>一般大家不会区别误差和损失函数，会把损失函数叫成 loss。</p>
</blockquote>
<p>为了节约时间，一开始我只训练了1000步，最后准确率只有0.57左右。哪怕我令输出全部为1，从期望上都能得到0.5的准确率。这个结果确实不尽如人意。</p>
<p>我自己亲手设计的模型，结果怎么能这么差呢？肯定是训练得不够。我一怒之下，加了个零，让程序跑10000步训练。看着loss不断降低，从0.69，到0.4，再到0.3，最后在0.24的小数点第3位之后变动，我的心情也越来越激动：能不能再低点，能不能再创新低？那感觉就像股市开盘看到自己买的股票高开，不断祈祷庄家快点买入一样。</p>
<p>在电脑前，盯着不断更新的控制台快一小时后，loss定格在了0.2385，我总算等到了10000步训练结束的那一刻。模型即将完成测试，准确率即将揭晓。<br>我定睛一看——准确率居然还只有0.575!</p>
<p>这肯定不是我代码的问题，一定是逻辑回归这个模型太烂了！希望在之后的课程中，我们能够用更复杂的模型跑出更好的结果。</p>
<p>欢迎大家也去下载这个demo(<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/LogisticRegression)，一起调一调参数~">https://github.com/SingleZombie/DL-Demos/tree/master/LogisticRegression)，一起调一调参数~</a></p>
<h3 id="修好bug后的结果"><a href="#修好bug后的结果" class="headerlink" title="修好bug后的结果"></a>修好bug后的结果</h3><p>第一次写的代码竟然把梯度全部算错了，这太离谱了，我也不知道当时写代码的时候脑子里进了多少水。修好bug后，我又跑了一次训练。</p>
<p>首先，按照上次的经验，学习率0.0002，跑1000步，就得到了0.59的准确率。这效果差的也太多吧！</p>
<p>接下来训练10000步，我又满怀期待地盯着控制台，看着梯度降到了0.2395。</p>
<p>精度测出来了——好家伙，又是0.575。</p>
<p>行吧，起码文章的内容不用大改了。逻辑回归太菜了，和代码确实没什么关系。</p>
<p>其实，这段写bug经历对我来说是很赚的。我学到了：在梯度算得有问题的情况下，网络可以正常训练，甚至loss还会正常降低。但是，网络的正确率肯定会更低。一定要尊重数学规律，老老实实地按照数学推导的结果写公式。如果没有写bug，我反而学不到这么多东西，反而很亏。</p>
<h1 id="吐槽"><a href="#吐槽" class="headerlink" title="吐槽"></a>吐槽</h1><p>把这篇文章刚发到博客上的时候，这篇文章有一堆错误：$W,w$不分，损失函数乱写……。写这种教学文章一定要严谨，尤其是涉及了数学运算的。很多时候程序有bug，根本看不出来。希望我能引以为戒，学踏实了，把文章检查了几遍了，再把文章发出来。</p>
<p>突然又发现一个bug：reshape不是inplace运算。我写得也太潦草了吧！</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/04/23/DLS-note-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/23/DLS-note-1/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》笔记+代码实战（一）：深度学习入门</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-23 21:49:02" itemprop="dateCreated datePublished" datetime="2022-04-23T21:49:02+08:00">2022-04-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>最近在学吴恩达的《深度学习专项》（Deep Learning Specialization)。为了让学习更有效率（顺便有一些博文上的产出），我准备写一些学习笔记。笔记的内容比较简单，没有什么原创性的内容，主要是对课堂的知识进行梳理（这些文章的标题虽然叫”笔记“，但根据我之前的分类，这些文章由于原创性较低，被划分在”知识记录“里）。如果读者也在学这门课的话，可以对照我总结出来的知识，查缺补漏。之后几节课有课后作业，我会在笔记里讲解我的编程思路，给读者一些编程上的启发。</p>
<p>文章中的正文主要是对课堂内容的总结。引用里的内容和每篇笔记的总结是我个人的观点或评论。</p>
<h2 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络"></a>什么是神经网络</h2><p>我们把一个有输入有输出的计算单元叫做“神经元”。神经元可以简单地理解成一个线性函数。比如要预测房价和房屋面积的关系，我们可以近似地用一个线性函数去拟合。这个函数就是一个神经元。</p>
<blockquote>
<p>事实上，一个神经元不仅包含一个线性函数，还包括一个激活函数。这里提到了激活函数 ReLU 的概念，其具体内容应该会在后面介绍。</p>
</blockquote>
<p>神经元的堆叠，构成了神经网络，如下图所示。</p>
<p><img src="/2022/04/23/DLS-note-1/1.jpg" alt></p>
<p>在用一个神经元来表示房价和房屋面积的关系时，神经元的输入是房屋面积，输出是房价。而用多层神经元时，每个神经元的意义可能都不一样。比如中间的神经元可能会根据输入的邮政编码、地址特征，输出一个表示房屋地段的中间特征。在神经网络中，这些特征都是<strong>自动生成</strong>的（意味着我们只需要管理神经网络的输入和输出，而不用指定中间的特征，也不用理解它们究竟有没有实际意义）。</p>
<blockquote>
<p>以前的一些机器学习要手动设置特征。而神经网络这种自动生成特征的性质，是其成功的原因之一。</p>
</blockquote>
<h2 id="用神经网络做监督学习"><a href="#用神经网络做监督学习" class="headerlink" title="用神经网络做监督学习"></a>用神经网络做监督学习</h2><blockquote>
<p>要理解监督学习，其实应该要对比无监督学习。本节实际上是介绍了监督学习的几个例子。</p>
</blockquote>
<p>常见的神经网络有三类：</p>
<ol>
<li>标准神经网络（即全连接网络）可以用于房屋分类、广告分类问题。（这些问题一般输入是<strong>结构化</strong>的）</li>
<li>卷积神经网络（CNN）一般用于图像相关的问题，比如图片猫狗分类，自动驾驶中识别其他车辆的位置。</li>
<li>循环神经网络（RNN）一般用于处理有时序的序列数据，比如和声音、文字有关的应用都需要RNN。</li>
</ol>
<p>结构化数据，就是所有其数据项都是人能理解的（房子的面积、价格）。对比来看，无结构化的数据的具体含义是无法直接解释的，比如图像每一个像素值、声音某时刻的频率和响度、某一个文字/单词。</p>
<h2 id="为什么最近深度学习“起飞”了？"><a href="#为什么最近深度学习“起飞”了？" class="headerlink" title="为什么最近深度学习“起飞”了？"></a>为什么最近深度学习“起飞”了？</h2><p><img src="/2022/04/23/DLS-note-1/2.jpg" alt></p>
<p>这张图足以解释深度学习腾飞的原因。随着数据量的增加，所有方法都有性能的上限。而对于神经网络来说，结构越复杂的神经网络，其性能上限越高。复杂的神经网络（深度学习方法）在海量数据不断产生的今天更具优势。</p>
<p>光有大量的数据，没有使用数据的方法是不够的。总结来看，深度学习在近几年得到发展的原因有下：</p>
<ul>
<li>互联网的发展使得数字数据大量增长。</li>
<li>GPU等计算设备使得处理数据的硬件变强。</li>
<li>深度学习的算法不断更新迭代，从软件层面上加快了数据处理。（比如激活函数的改进，从sigmoid到ReLU）</li>
</ul>
<p>深度学习本质上还是以实验为主。计算能力上来了，研究人员做实验做得快了，各种各样的深度学习的应用也就出来了。各种应用又鼓舞着更多人参与深度学习研究。也就是说，是计算能力的提升使得近年来深度学习进入了良好的正反馈循环中。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>第一周的课没有什么深奥的内容，主要是给对深度学习不太熟悉的同学们介绍了下背景知识。</p>
<p>在我看来，这周的课需要记住的东西有：</p>
<ul>
<li>神经元有输入和输出的计算单元。神经元堆叠成了神经网络。</li>
<li>大致有三种不同类型的神经网络，适用于不同的任务。</li>
<li>神经网络的性能随其规模和数据量而增长。</li>
<li>计算效率的提高使深度学习近期得到飞速发展。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/03/18/20220315-custom-op/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/18/20220315-custom-op/" class="post-title-link" itemprop="url">PyTorch Custom OP （自定义算子） 教程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-18 15:28:41" itemprop="dateCreated datePublished" datetime="2022-03-18T15:28:41+08:00">2022-03-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="PyTorch-自定义算子教程：两种方法实现加法算子（附LibTorch-Windows环境配置教程）"><a href="#PyTorch-自定义算子教程：两种方法实现加法算子（附LibTorch-Windows环境配置教程）" class="headerlink" title="PyTorch 自定义算子教程：两种方法实现加法算子（附LibTorch Windows环境配置教程）"></a>PyTorch 自定义算子教程：两种方法实现加法算子（附LibTorch Windows环境配置教程）</h1><p>我们都知道，PyTorch做卷积等底层运算时，都是用C++实现的。有时，我们再怎么去调用PyTorch定义好的算子，也无法满足我们的需求。这时，我们就要考虑用C++自定义一个PyTorch的算子了。</p>
<p>PyTorch提供了两种添加C++算子的方法：编译动态库并嵌入<code>TorchScript</code><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html">[1]</a>、用PyTorch的C++拓展接口<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/cpp_extension.html">[2]</a>。前者适合导入独立的C++项目，后者需要用PyTorch的API设置编译信息，只适合小型C++项目，更适合于把新算子共享给他人的情况。由于我还没有用过torch的C++接口，这里先用第一种方法写一套独立的算子实现示例，跑通整个流程，再基于同一份代码，用第二种方法实现一次，以全方位地介绍PyTorch自定义算子的方法。</p>
<p><strong>前置准备：</strong></p>
<ul>
<li>装好了CMake</li>
<li>装好了PyTorch</li>
<li>装好了OpenCV</li>
<li>看得懂C++、Python</li>
</ul>
<p><strong>知识点预览：</strong></p>
<ul>
<li>如何配置LibTorch</li>
<li>第一个Torch C++程序</li>
<li>如何自己写简单的CMake</li>
<li>如何用Visual Studio写CMake项目</li>
<li>如何编译使用简单的动态库</li>
<li>如何用两种方法实现PyTorch自定义算子</li>
<li>如何用setuptools自动编译C++源代码</li>
</ul>
<p>（以上是我写这篇文章之前还不会的东西。）</p>
<ul>
<li>如何用PyTest做单元测试</li>
</ul>
<p><strong>参考教程</strong>：</p>
<p>[1] 添加<code>TorchScript</code>拓展 <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html">https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html</a></p>
<p>[2] PyTorch的C++拓展 <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/cpp_extension.html">https://pytorch.org/tutorials/advanced/cpp_extension.html</a></p>
<p>[3] 安装LibTorch <a target="_blank" rel="noopener" href="https://pytorch.org/cppdocs/installing.html">https://pytorch.org/cppdocs/installing.html</a></p>
<p>[4] VS CMake <a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/cpp/build/cmake-projects-in-visual-studio?view=msvc-170&amp;viewFallbackFrom=vs-2019">https://docs.microsoft.com/zh-cn/cpp/build/cmake-projects-in-visual-studio?view=msvc-170&amp;viewFallbackFrom=vs-2019</a></p>
<h2 id="配置-LibTorch-开发环境"><a href="#配置-LibTorch-开发环境" class="headerlink" title="配置 LibTorch 开发环境"></a>配置 LibTorch 开发环境</h2><p>我们这个项目是使用CMake开发的，理论上任何平台都能使用。我是在Windows上测试的，理论上Windows上碰到的毛病会多一些，Linux上可能直接用就没问题了。</p>
<p>对于我们这个CMake项目来说，成功添加路径，使得<code>find_package（Torch)</code>（找到Torch的CMake配置）不报错就算配置环境成功。当然，貌似由于Torch依赖于OpenCV，找OpenCV包也得成功才行。</p>
<p>参考教程是<a target="_blank" rel="noopener" href="https://pytorch.org/cppdocs/installing.html">[3]</a>，但对于像我一样什么都不懂的新手来说，由于CMake有些东西要配置，这篇官方教程还不太够用。</p>
<h3 id="下载-LibTorch"><a href="#下载-LibTorch" class="headerlink" title="下载 LibTorch"></a>下载 LibTorch</h3><p>想用PyTorch的C++相关内容的话，要先去下载LibTorch库。</p>
<p>在获取PyTorch的Python版本下载命令处，可以找到LibTorch的安装链接：</p>
<p><img src="/2022/03/18/20220315-custom-op/1.jpg" alt></p>
<p>和装PyTorch Python版的时候类似，选好自己的版本，之后点击某个链接下载就行。第一个链接是Release版，第二个是Debug版。由于我是编程高手，不要调试，所以直接选择了Release版。建议大家去下Debug版方便随时调试。</p>
<h3 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a>添加环境变量</h3><p>下一步要把LibTorch的动态库所在目录加入环境变量中，以使程序运行时能够找得到依赖的动态库（编译是没问题的）。</p>
<p>把<code>xxxxxxxx\libtorch\lib</code>这个目录添加进环境变量即可。</p>
<p>如果是在Windows上，添加环境变量时有一个细节要注意：</p>
<p><img src="/2022/03/18/20220315-custom-op/2.jpg" alt></p>
<p>相信90%的人装PyTorch前都是把Cuda装好了的。在添加LibTorch的动态库目录时一定要注意，要把这个路径移到Cuda路径的上面。详细原因见<strong>FAQ</strong>。</p>
<h3 id="Hello-LibTorch"><a href="#Hello-LibTorch" class="headerlink" title="Hello LibTorch"></a>Hello LibTorch</h3><p>接下来我们要用一个能调试CMake程序的环境来完成第一个C++ LibTorch程序。</p>
<p>创建一个崭新的文件夹，在里面添加一个<code>CMakeLists.txt</code>:</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.1</span> FATAL_ERROR)</span><br><span class="line"><span class="keyword">project</span>(equi_conv)</span><br><span class="line"></span><br><span class="line"><span class="keyword">find_package</span>(Torch REQUIRED)</span><br><span class="line"><span class="keyword">find_package</span>(OpenCV REQUIRED)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(equi_conv op.cpp)</span><br><span class="line"><span class="keyword">target_compile_features</span>(equi_conv PRIVATE cxx_std_14)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(equi_conv <span class="string">&quot;$&#123;TORCH_LIBRARIES&#125;&quot;</span>)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(equi_conv opencv_core opencv_imgproc)</span><br></pre></td></tr></table></figure>
<p>里面的<code>equi_conv</code>可以换成你喜欢的项目名。我使用的项目名是<code>equi_conv</code>，这个名称会在后面多次出现。理论上我显示<code>equi_conv</code>的地方显示的应该是你自己的项目名。</p>
<p><strong>注意！</strong>一般情况下CMake是找不到Torch和OpenCV的，要手动设置CMake Configure附加命令中的<code>Torch_DIR</code>和<code>OpenCV_DIR</code>这两个参数，比如我的附加命令是</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-D Torch_DIR=&quot;D:/Download/libtorch-win-shared-with-deps-1.11.0+cu113/libtorch/share/cmake/Torch&quot; -D OpenCV_DIR=&quot;D:/OpenCV/opencv/build&quot;</span><br></pre></td></tr></table></figure>
<p>。<code>Torch_DIR</code>是<code>&quot;xxxxxxxx/libtorch/share/cmake/Torch</code>,<code>OpenCV_DIR</code>大约是<code>xxxxxxxx/opencv/build</code>。每个人的具体路径可能不一样，只要记住，这两个路径里都得是包含了<code>.cmake</code>文件的。根据编程环境的不同，设置这两个CMake参数的位置也不同，详见后文。</p>
<p>官方教程[3]给了一种很骚的提供路径的方法：<code>-DCMAKE_PREFIX_PATH=&quot;$(python -c &#39;import torch.utils; print(torch.utils.cmake_prefix_path)&#39;)&quot;</code>。这个命令是调用Python脚本以添加PyTorch默认的CMake搜索目录。但是这个命令有一些问题：1) 当前命令行环境里不一定能正确调用Python及访问torch库（比如PyTorch是用conda装的，而当前环境不是对应的conda环境）；2) 我们下载的libtorch似乎难以对得上PyTorch包里默认的libtorch路径。这行命令似乎仅适用于处于正确Python环境下，把libtorch装到了<code>/libtorch</code>目录下的Linux系统。为了命令的兼容性，我们不用这么骚的操作，老老实实自己设置LibTorch目录和OpenCV目录。</p>
<p>再写一个叫<code>op.cpp</code>的C++源文件。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	torch::Tensor tensor = torch::<span class="built_in">rand</span>(&#123; <span class="number">2</span>, <span class="number">3</span> &#125;);</span><br><span class="line">	std::cout &lt;&lt; tensor &lt;&lt; std::endl;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果你是高手，可以不去配环境，直接手敲CMake命令。但为了方便，接下来我们还是准备调试运行这个程序。配置CMake调试环境有很多方法，这里先给一个Windows上Visual Studio的方案<a target="_blank" rel="noopener" href="https://docs.microsoft.com/zh-cn/cpp/build/cmake-projects-in-visual-studio?view=msvc-170&amp;viewFallbackFrom=vs-2019">[4]</a>：</p>
<p>准备好上面那个<code>CMakeLists.txt</code>后，用VS打开这个CMake文件（相信大家的VS都是2017版本以上的，旧版本是没有CMake的功能的~）：</p>
<p><img src="/2022/03/18/20220315-custom-op/3.jpg" alt></p>
<p>如果文件没写错VS会自动配置(Configure)CMake。在工具栏中可以手动中断或开始CMake的配置。</p>
<p><img src="/2022/03/18/20220315-custom-op/4.jpg" alt></p>
<p>还可以点击上面的“{PROJECT_NAME}的CMake设置”来设置CMake命令中要用的参数（比如<code>-D</code>参数）</p>
<p><img src="/2022/03/18/20220315-custom-op/5.jpg" alt></p>
<p>注意，一开始CMake只有Debug版的配置，可以点左上角的加号手动加一个Release版的配置。</p>
<p>同时，如图中所示，<code>xxx_DIR</code>应该卸载CMake命令参数里面。</p>
<p>配置好后去上面的工具栏点击”生成-全部生成”就可以把程序编译好了。接下来按熟悉的F5就可以运行程序了。</p>
<p>再介绍一个VSCode的CMake编程环境，这个基本上是全平台通用的。不过同样，我还是在Windows上测试的，以Windows上的配置为主。</p>
<p>通过搜索”Windows CMake VSCode cl 配置”等关键词，我搜索到了一篇<a target="_blank" rel="noopener" href="https://blog.csdn.net/Nichlson/article/details/113763551">很好的教程</a>，我是照着这篇教程配的环境。如果是Linux的话，换一下编译器应该就能拿过来用了。</p>
<p>为了添加<code>-D</code>等配置参数，可以用<code>ctrl+,</code>打开设置，修改工作区设置里的CMake Configure命令：</p>
<p><img src="/2022/03/18/20220315-custom-op/6.jpg" alt></p>
<p>如果一切正常，程序会输出随机张量的内容。</p>
<p><img src="/2022/03/18/20220315-custom-op/7.jpg" alt></p>
<h2 id="LibTorch-A-B"><a href="#LibTorch-A-B" class="headerlink" title="LibTorch A+B"></a>LibTorch A+B</h2><h3 id="C-侧"><a href="#C-侧" class="headerlink" title="C++ 侧"></a>C++ 侧</h3><p>修改<code>op.cpp</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">my_add</span><span class="params">(torch::Tensor t1, torch::Tensor t2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="built_in">assert</span>(t1.<span class="built_in">size</span>(<span class="number">0</span>) == t2.<span class="built_in">size</span>(<span class="number">0</span>));</span><br><span class="line">	<span class="built_in">assert</span>(t1.<span class="built_in">size</span>(<span class="number">1</span>) == t2.<span class="built_in">size</span>(<span class="number">1</span>));</span><br><span class="line">	<span class="function">cv::Mat <span class="title">m1</span><span class="params">(t1.size(<span class="number">0</span>), t1.size(<span class="number">1</span>), CV_32FC3, t1.data_ptr&lt;<span class="keyword">float</span>&gt;())</span></span>;</span><br><span class="line">	<span class="function">cv::Mat <span class="title">m2</span><span class="params">(t1.size(<span class="number">0</span>), t1.size(<span class="number">1</span>), CV_32FC3, t2.data_ptr&lt;<span class="keyword">float</span>&gt;())</span></span>;</span><br><span class="line">	</span><br><span class="line">	cv::Mat res = m1 + m2;</span><br><span class="line"></span><br><span class="line">	torch::Tensor output = torch::<span class="built_in">from_blob</span>(res.ptr&lt;<span class="keyword">float</span>&gt;(), &#123; t1.<span class="built_in">size</span>(<span class="number">0</span>), t1.<span class="built_in">size</span>(<span class="number">1</span>), <span class="number">3</span>&#125;);</span><br><span class="line">	<span class="keyword">return</span> output.<span class="built_in">clone</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TORCH_LIBRARY</span>(my_ops, m)</span><br><span class="line">&#123;</span><br><span class="line">	m.<span class="built_in">def</span>(<span class="string">&quot;my_add&quot;</span>, my_add);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们要实现一个新PyTorch算子<code>my_add</code>，该实现函数先把两个PyTorch Tensor转换成OpenCV Mat，用Mat做加法，再把Mat转回Tensor。整个代码非常易懂，哪怕对LibTorch和OpenCV的语法不熟，也基本猜得出每行代码的作用。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br></pre></td></tr></table></figure>
<p>一开始，先包含LibTorch、OpenCV的头文件。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">torch::Tensor <span class="title">my_add</span><span class="params">(torch::Tensor t1, torch::Tensor t2)</span></span></span><br></pre></td></tr></table></figure>
<p>我们要实现的是一个PyTorch的加法，因此实现函数中所有的张量类型都是<code>torch::Tensor</code>。加法输入是两个量，输出是一个量，因此最后的函数头要这样写。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">assert</span>(t1.<span class="built_in">size</span>(<span class="number">0</span>) == t2.<span class="built_in">size</span>(<span class="number">0</span>));</span><br><span class="line"><span class="built_in">assert</span>(t1.<span class="built_in">size</span>(<span class="number">1</span>) == t2.<span class="built_in">size</span>(<span class="number">1</span>));</span><br></pre></td></tr></table></figure>
<p>做为严谨的程序员，我们要对输入的Tensor做一定的检查（实际上这两个检查还不够，由于我们默认输入图像的通道是3，还应该检查一下通道数。但这样检查下去可能会没完没了了，这里仅仅是提醒大家要养成良好的编程习惯）（其实是我写了两行就懒得写下去了）。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cv::Mat <span class="title">m1</span><span class="params">(t1.size(<span class="number">0</span>), t1.size(<span class="number">1</span>), CV_32FC3, t1.data_ptr&lt;<span class="keyword">float</span>&gt;())</span></span>;</span><br><span class="line"><span class="function">cv::Mat <span class="title">m2</span><span class="params">(t1.size(<span class="number">0</span>), t1.size(<span class="number">1</span>), CV_32FC3, t2.data_ptr&lt;<span class="keyword">float</span>&gt;())</span></span>;</span><br></pre></td></tr></table></figure>
<p>这两行是用Tensor构造Mat。从这两行代码中，可以学到两点：1）可以通过<code>tensor.data_ptr&lt;float&gt;</code>来获取Tensor存储数据的指针;2）不同框架下的数据结构互转时一般是传指针，再传shape。</p>
<p>OpenCV这里有一点点特殊。OpenCV的Mat是二维的，要维护一个H-W-C（高-宽-通道）的数据，需要传一个基础数据类型<code>CV_32FC3</code>，即3通道浮点数。</p>
<p>从代码中可以猜出来，<code>tensor.size(i)</code>可以获取Tensor第i维的长度。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv::Mat res = m1 + m2;</span><br></pre></td></tr></table></figure>
<p>不用猜都知道这是调用了Mat的加法。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch::Tensor output = torch::<span class="built_in">from_blob</span>(res.ptr&lt;<span class="keyword">float</span>&gt;(), &#123; t1.<span class="built_in">size</span>(<span class="number">0</span>), t1.<span class="built_in">size</span>(<span class="number">1</span>), <span class="number">3</span>&#125;);</span><br></pre></td></tr></table></figure>
<p>这一行是Mat转Tensor，同样是传了数据指针和张量形状。</p>
<p>这里第二个参数是个叫<code>at::IntArrayRef</code>的类型的。这个类型会用在Tensor的shape上。该类型的最简单的初始化方式就是用大括号把值框进去，就像Python里用方括号或圆括号传List和Set一样。</p>
<p>相比生成OpenCV Mat，这里没有传数据类型。原因如前文所述，应该是由于OpenCV的数据类型里包含了维度信息，所以OpenCV的Mat构造时要额外传这个信息。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> output.<span class="built_in">clone</span>();</span><br></pre></td></tr></table></figure>
<p>最后返回的是<code>tensor.clone()</code>。官方教程里说，用指针创建Tensor时会复用原来的指针，而不会新申请内存。函数结束后，Mat里的资源会释放，等于说这个用Mat创建出的Tensor也失效了。因此要<code>clone()</code>一下，让数据在函数结束后依然存在。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TORCH_LIBRARY</span>(my_ops, m)</span><br><span class="line">&#123;</span><br><span class="line">	m.<span class="built_in">def</span>(<span class="string">&quot;my_add&quot;</span>, my_add);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后调用API把C++函数绑定到Python上，现在可以不用追究这些代码的具体原理，只要知道这样写Python就可以访问到<code>my_add</code>了。</p>
<p>这里可以改动的内容其实有两处：算子的域<code>my_ops</code>，算子名/函数名<code>my_add</code>。前面那个<code>my_ops</code>在PyTorch的某些地方会用到，这里我们先不管，随便取一个名字即可。</p>
<p>现在我们要编译的是一个包含一个函数的动态库，而不是一个包含<code>main</code>的应用程序了。因此，我们要修改一下<code>CMakeLists.txt</code>中的编译选项：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.1</span> FATAL_ERROR)</span><br><span class="line"><span class="keyword">project</span>(equi_conv)</span><br><span class="line"></span><br><span class="line"><span class="keyword">find_package</span>(Torch REQUIRED)</span><br><span class="line"><span class="keyword">find_package</span>(OpenCV REQUIRED)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_library</span>(equi_conv SHARED op.cpp)</span><br><span class="line"><span class="keyword">target_compile_features</span>(equi_conv PRIVATE cxx_std_14)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(equi_conv <span class="string">&quot;$&#123;TORCH_LIBRARIES&#125;&quot;</span>)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(equi_conv opencv_core opencv_imgproc)</span><br></pre></td></tr></table></figure>
<p>其实就改了一行:<code>add_library(equi_conv SHARED op.cpp)</code>，这样可以把编译目标变成一个动态库。</p>
<p>代码没错的话，重新Configure和Generate后动态库就编译好了。</p>
<h3 id="Python-侧"><a href="#Python-侧" class="headerlink" title="Python 侧"></a>Python 侧</h3><p>我们写一个单元测试Python脚本来测试一下我们的算子能否在PyTorch里成功运行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">lib_path = <span class="string">r&quot;D:\Repo\equi_conv\EquiConv\out\build\x64-Release\equi_conv.dll&quot;</span></span><br><span class="line">torch.ops.load_library(lib_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_add</span>():</span></span><br><span class="line">    a = torch.rand([<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>])</span><br><span class="line">    b = torch.rand([<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>])</span><br><span class="line">    c = torch.ops.my_ops.my_add(a, b)</span><br><span class="line">    d = a + b</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(c, d)</span><br></pre></td></tr></table></figure>
<p>再一次，为了体现我们编程时的严谨性，我们使用pytest来测试这个脚本。<code>pip install pytest</code>就可以轻松安装好这个Python单元测试工具。但如果你实在太懒了，不想下pytest，就得在后面补一行<code>test_add()</code>手动调用一下这个函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lib_path = <span class="string">r&quot;D:\Repo\equi_conv\EquiConv\out\build\x64-Release\equi_conv.dll&quot;</span></span><br><span class="line">torch.ops.load_library(lib_path)</span><br></pre></td></tr></table></figure>
<p><code>import torch</code>就不说了。这两行代码是调用PyTorch的API来读取我们刚刚编译出来的动态库。我们这里只需要把动态库路径改成自己的就好，别的都不用改。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_add</span>():</span></span><br><span class="line">    a = torch.rand([<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>])</span><br><span class="line">    b = torch.rand([<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>])</span><br><span class="line">    c = torch.ops.my_ops.my_add(a, b)</span><br><span class="line">    d = a + b</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(c, d)</span><br></pre></td></tr></table></figure>
<p>后面这些代码就是实际单元测试的代码里，代码非常简单：生成两个随机tensor，比较一下我们的加法和PyTorch自己的加法是否结果一致。</p>
<p>值得注意的是，用<code>torch.ops.my_ops.my_add</code>可以调用我们刚刚那个C++函数。前面的<code>torch.ops</code>都是写死的，后面的<code>my_add</code>是我们自己定义的函数名。而<code>my_ops</code>，则是我们刚刚调API时填的“算子域”了。算子域在注册Python符号表的时候还会用到，这里不用管那么多，把算子域理解成一个命名空间，一个防止算子命名冲突的东西即可。</p>
<p><code>torch.allclose</code>可以简单地理解为一个要求两个Tensor所有值都<strong>几乎</strong>相等的比较函数。</p>
<p>在该文件夹下运行命令<code>pytest</code>，屏幕上显示绿色的<code>1 passed xxxxxxxxxx</code>即说明单元测试成功运行。</p>
<p>至此，我们算是成功在Python里调用了一个C++写的算子。只需要写上torch.ops.my_ops.my_add`，我们就能够在任何地方（比如模型的forward函数）调用我们的算子。聪明的人看到这里，已经学会随心所欲地在PyTorch里嵌入自己的高效率的C++算子了。</p>
<p>配好环境，搭好框架后，我们自己实现算子倒是非常舒服。问题是，如果我们要把这些算子给别人使用的话，要么是给别人源代码，让别人自己配置LibTorch编译环境；要么是把所有<code>Torch版本数 * Cuda版本数 * 操作系统数</code>这么多个动态库给预编译出来。</p>
<p>要是能抛掉LibTorch，让有PyTorch和Cuda环境的用户自己编译源代码，似乎一个平衡开发者体验和用户体验的选择。所以，这里再介绍之前讲过的第二种添加算子的方法：直接在PyTorch里添加C++拓展。</p>
<h2 id="PyTorch-Extension-A-B"><a href="#PyTorch-Extension-A-B" class="headerlink" title="PyTorch Extension A+B"></a>PyTorch Extension A+B</h2><p>用Python的setuptools也可以编译一些C++项目但，由于其头文件目录、依赖的库目录这些编译选项需要手动设置，setuptools仅适用于编译比较简单的C++项目。</p>
<p>在同文件夹中，编写以下的<code>setup.py</code>文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> cpp_extension</span><br><span class="line"></span><br><span class="line">include_dirs = [<span class="string">r&#x27;D:\OpenCV\opencv\build\include&#x27;</span>]</span><br><span class="line">library_dirs = [<span class="string">r&#x27;D:\OpenCV\opencv\build\x64\vc15\lib&#x27;</span>]</span><br><span class="line">libraries = [<span class="string">r&#x27;opencv_world452&#x27;</span>]</span><br><span class="line"></span><br><span class="line">setup(name=<span class="string">&#x27;my_add&#x27;</span>,</span><br><span class="line">      ext_modules=[</span><br><span class="line">          cpp_extension.CppExtension(<span class="string">&#x27;my_ops&#x27;</span>, [<span class="string">&#x27;op2.cpp&#x27;</span>],</span><br><span class="line">                                     include_dirs=include_dirs,</span><br><span class="line">                                     library_dirs=library_dirs,</span><br><span class="line">                                     libraries=libraries)</span><br><span class="line">      ],</span><br><span class="line">      cmdclass=&#123;<span class="string">&#x27;build_ext&#x27;</span>: cpp_extension.BuildExtension&#125;)</span><br></pre></td></tr></table></figure>
<p>在这个源文件中，要改的就是以下三个路径（代码块中显示的是我的路径）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">include_dirs = [<span class="string">r&#x27;D:\OpenCV\opencv\build\include&#x27;</span>]</span><br><span class="line">library_dirs = [<span class="string">r&#x27;D:\OpenCV\opencv\build\x64\vc15\lib&#x27;</span>]</span><br><span class="line">libraries = [<span class="string">r&#x27;opencv_world452&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>这三个路径用于配置OpenCV的编译选项，分别表示OpenCV的包含目录（头文件目录）、静态库目录、静态库名。用Visual Studio导入过第三方库的，肯定对这三个选项不陌生。</p>
<blockquote>
<p>如果是在 Linux 上，前两个路径大概是”/usr/local/include/opencv2”, “/usr/local/lib” 。最后的库名填写<code>opencv_core</code>即可。</p>
</blockquote>
<p>至于PyTorch相关的编译选项，我们不需要手动设置。这是因为我们用了PyTorch封装的添加C++拓展接口，PyTorch有关的路径已经被填好了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">setup(name=<span class="string">&#x27;my_add&#x27;</span>,</span><br><span class="line">      ext_modules=[</span><br><span class="line">          cpp_extension.CppExtension(<span class="string">&#x27;my_ops&#x27;</span>, [<span class="string">&#x27;op2.cpp&#x27;</span>],</span><br><span class="line">                                     include_dirs=include_dirs,</span><br><span class="line">                                     library_dirs=library_dirs,</span><br><span class="line">                                     libraries=libraries)</span><br><span class="line">      ],</span><br><span class="line">      cmdclass=&#123;<span class="string">&#x27;build_ext&#x27;</span>: cpp_extension.BuildExtension&#125;)</span><br></pre></td></tr></table></figure>
<p>在调用<code>setup</code>时，<code>name</code>是整个项目的名字，可以随便取。<code>my_ops</code>和刚刚一样，是命名空间的名字，我们还是保持<code>my_ops</code>这个名字。<code>op2.cpp</code>就是要编译的源文件了，这里我们待会再讨论。剩下的参数这些传进去就行了。</p>
<p>我们再在<code>op.cpp</code>的基础上新建一个新的C++源文件<code>op2.cpp</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">my_add</span><span class="params">(torch::Tensor t1, torch::Tensor t2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">assert</span>(t1.<span class="built_in">size</span>(<span class="number">0</span>) == t2.<span class="built_in">size</span>(<span class="number">0</span>));</span><br><span class="line">  <span class="built_in">assert</span>(t1.<span class="built_in">size</span>(<span class="number">1</span>) == t2.<span class="built_in">size</span>(<span class="number">1</span>));</span><br><span class="line">  <span class="function">cv::Mat <span class="title">m1</span><span class="params">(t1.size(<span class="number">0</span>), t1.size(<span class="number">1</span>), CV_32FC3, t1.data_ptr&lt;<span class="keyword">float</span>&gt;())</span></span>;</span><br><span class="line">  <span class="function">cv::Mat <span class="title">m2</span><span class="params">(t1.size(<span class="number">0</span>), t1.size(<span class="number">1</span>), CV_32FC3, t2.data_ptr&lt;<span class="keyword">float</span>&gt;())</span></span>;</span><br><span class="line"></span><br><span class="line">  cv::Mat res = m1 + m2;</span><br><span class="line"></span><br><span class="line">  torch::Tensor output = torch::<span class="built_in">from_blob</span>(res.ptr&lt;<span class="keyword">float</span>&gt;(), &#123;t1.<span class="built_in">size</span>(<span class="number">0</span>), t1.<span class="built_in">size</span>(<span class="number">1</span>), <span class="number">3</span>&#125;);</span><br><span class="line">  <span class="keyword">return</span> output.<span class="built_in">clone</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(my_ops, m)</span><br><span class="line">&#123;</span><br><span class="line">  m.<span class="built_in">def</span>(<span class="string">&quot;my_add&quot;</span>, my_add);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其实修改的就是这一行<code>TORCH_LIBRARY(my_ops, m)-&gt;PYBIND11_MODULE(my_ops, m)</code>，没有调用TorchScript的绑定接口，而是直接用Pybind绑定了C++函数。</p>
<p>接下来，在当前文件夹下运行命令<code>python setup.py install</code>即可编译刚刚的C++源文件了。成功的话大概会有<code>Finished processing dependencies for my-add==0.0.0</code>这样的提示。</p>
<p>编译结束后，我们在原来<code>test_add.py</code>的基础上添加一些单元测试，看看用这种新方法编译完C++拓展后怎么调用C++函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_add2</span>():</span></span><br><span class="line">    <span class="keyword">import</span> my_ops</span><br><span class="line">    a = torch.rand([<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>])</span><br><span class="line">    b = torch.rand([<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>])</span><br><span class="line">    c = my_ops.my_add(a, b)</span><br><span class="line">    d = a + b</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(c, d)</span><br></pre></td></tr></table></figure>
<p>由于我们刚刚编译了一个命名空间为<code>my_ops</code>的包，我们可以用<code>import my_ops</code>导入这个刚刚编译好的库了。现在调用C++函数的方法变成了<code>my_ops.my_add</code>，其他地方都没有变化。</p>
<p>运行<code>pytest test_add.py::test_add2</code>可以单独测试这一个函数。当然懒的话直接<code>pytest</code>可以把刚刚那个测试和这个测试一起做一遍。单元测试通过就说明我们成功运行了C++拓展。</p>
<p>事实上，这种安装方式还是不够友好。由于我们用到了OpenCV，OpenCV的库路径还是要手动设置。这种安装方式只有在除PyTorch本身外不需要任何第三方库时比较友好。不然的话要么让用户自己手动设置路径，要么在代码库里引用别的开源库，再一个一个重写路径。大型项目还是用CMake等编译系统来编译比较友好。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我介绍了两种在PyTorch里调用C++新算子的方法。只要看懂了这篇文章，就算是彻底打通了PyTorch与C++的桥梁，以后写代码可以专注于C++算子的实现及PyTorch对算子的封装，剩下的绑定算子的内容直接套这个模板就行。</p>
<p>两种算子实现方法的区别主要在于编译选项的设置上和用户在编译算子的体验上。应根据项目的实际情况选择一种方案。</p>
<p>这篇文章强行调用OpenCV实现了Tensor加法，看上去是多此一举，实际上这是为了展示如何在添加自定义算子时使用第三方库。但为了简化他人编译的过程，实际实现算子时最好只用原本的PyTorch API。</p>
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><h3 id="运行LibTorch的示例程序，无法定位程序输入点-xxxxx-于动态链接库-xxxxx"><a href="#运行LibTorch的示例程序，无法定位程序输入点-xxxxx-于动态链接库-xxxxx" class="headerlink" title="运行LibTorch的示例程序，无法定位程序输入点 xxxxx 于动态链接库 xxxxx"></a>运行LibTorch的示例程序，无法定位程序输入点 xxxxx 于动态链接库 xxxxx</h3><p>这个问题找了我老半天，就找到2~3个相关的答案，全是治标不治本的方法。</p>
<p>有人说，是动态库路径的问题。我测试了一下，直接运行编译好的程序会报错，但是<strong>把程序放到LibTorch的动态库目录下就不会报错</strong>。我已经隐隐约约地感觉到，不是动态库找不到，而是<strong>动态库路径的优先顺序</strong>出了问题。</p>
<p>果不其然，最后我在<a target="_blank" rel="noopener" href="https://www.icode9.com/content-3-1005434.html">这篇文章</a>里找到了问题的真正原因：Cuda的动态库和LibTorch的冲突了（PyTorch和Cuda要背大锅）。那篇文章中暴力删掉了Cuda的动态库，但是温柔的我们绝对不要这样做。按照前面章节的内容，调整LibTorch与Cuda的路径优先级即可。</p>
<p><img src="/2022/03/18/20220315-custom-op/bug1.jpg" alt></p>
<p>貌似官方教程提到了类似的错误。这里再提供一种可能的解决问题的思路（反正我没试）。</p>
<h3 id="WinError-126-找不到指定的模块"><a href="#WinError-126-找不到指定的模块" class="headerlink" title="[WinError 126] 找不到指定的模块"></a>[WinError 126] 找不到指定的模块</h3><p>这个问题说明Python的PyTorch库版本和下载的LibTorch C++版本不一致。用<code>pip show torch</code>查看当前的PyTorch版本，去重新下载对应的LibTorch即可。</p>
<h3 id="OSError-xxx-Undefined-symbol-Linux"><a href="#OSError-xxx-Undefined-symbol-Linux" class="headerlink" title="OSError: xxx Undefined symbol (Linux)"></a>OSError: xxx Undefined symbol (Linux)</h3><p>要把 LibTorch 的动态库加入 LD_LIBRARY_PATH 里。</p>
<h1 id="有关博客“学习”分类下子类别的说明"><a href="#有关博客“学习”分类下子类别的说明" class="headerlink" title="有关博客“学习”分类下子类别的说明"></a>有关博客“学习”分类下子类别的说明</h1><p>貌似之前说明过一次，这里再整理一遍。</p>
<ul>
<li>工具用法指南：几乎没有技术含量的，把下载安装过程的踩坑过程原封不动地讲一遍。</li>
<li>知识记录：对现有成体系知识的描述，尤其会写教科书、公开课上的知识，较少我个人的见解。</li>
<li>知识整理：对某一工具、知识、技术的说明，主要以我个人的见解、整理为主。</li>
</ul>
<p>另外，“学习”类别和“记录”类别挺容易混淆的。这里我再做个规定：“记录”以具体的任务为导向，比如先有要写的作业、要看的论文、要做的项目、规划好的旅游计划，再对这些事情进行描述。而“学习”中包含的文章，更多是一种主观的，以学到东西为目的而写的文章。如果我看了一篇论文，只写论文的内容的话，会分到“记录”里；如果我想调研一个主题的文章，会把调研结果放到“学习-笔记”里；如果我看了很多论文，有了原创性非常强的一篇描述知识的文章，会放到“知识分享”（未来的“创作-知识”）里。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/03/08/Code-Optimization-Fun/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/08/Code-Optimization-Fun/" class="post-title-link" itemprop="url">拆掉循环竟然让代码性能大幅提升？ ~ 有趣的高性能计算大作业</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-08 22:29:29" itemprop="dateCreated datePublished" datetime="2022-03-08T22:29:29+08:00">2022-03-08</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/%E4%BD%9C%E4%B8%9A-%E7%BB%83%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">作业/练习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>由于这个学期没有返校，我变懒了不少。明明有不少可以写的东西，却没有去写。等学期结束了有时间了我会好好补写一些博客。</p>
<p>这几天，我在赶一个高性能计算的大作业。题目要求优化一段代码，使程序的运行时间尽可能短。大作业本来是一个令人烦躁忧虑，头皮发麻的事物，但在deadline的紧逼之下，我仿佛按下了大脑的启动按钮，火力全开地应对起这个大作业来。于是，我的漫长的编程时间就开始了。——看到这里，如何你对编程不是非常了解，可能完全没有读下去的兴致。但我保证我会用外行人也能看懂的方式，来描述我这次有趣的写大作业经历。</p>
<p>再具体地讲一下我的大作业要求。我的大作业题目是代码优化，跑代码花费的时间是评价成绩的唯一指标。当然，代码的正确性不能受到影响，你不能让程序刚进去就关掉。代码跑得越快，分数越高。如果代码速度是原来的2倍，则可以拿到60分及格。如果达到原来的2.5，3，3.5，4倍，则可以分别拿到80，90， 95， 100分。成绩评判标准唯一且清晰。</p>
<p>我是作业截止的最后4天开始看这个大作业的。看完这个评分标准，我心里先是一乐：“哈哈，终于有一个评分透明的大作业了，写论文什么的成绩太容易受老师主观评价影响了。”接着，我又发现事情有些不太对：“既然老师敢给这么高的分数，说明代码想优化到3倍或者4倍是很困难的。我的时间这么少，恐怕只能拿个80分吧。”我也没再想下去，反正代码能优化多少就优化多少，也没有那种先定目标再开始干活的必要。</p>
<p>我的编程任务正式开始了。准确来说，我做的不是编程，是合理而优雅地修改老师给的代码，让这个代码运行速度更快一些，而不改变程序本身的意思。要打比方的话，我是一个拿着手术刀的医生，我需要精准地切掉病变部位，还病人一个更好的身体。比较幸运的是，我的工作可以反复调试，代码出了问题可以重新修改，而不需要担心造成什么破坏性的后果。</p>
<p>代码的功能是计算化学反应的一些参数。这段代码产生的程序不会产生任何花哨的网页、按钮，只会在默默地运行数十秒中后，冷冰冰地在控制台上输出一行数学计算的结果。运行这段代码就好像把一个优等生关进一个房间，让他把数学卷子全做出来再离开一样。只不过程序在输入的参数完全相同的情况下，每次都会执行一模一样的操作，最终得到完全相同，完全正确的结果。</p>
<p>这段代码可谓是又臭又长。丝毫没有注释，一个文件里的代码（代码分布在多个文件中）全是看不懂的化学常量，另一个文件的代码全是看似毫无逻辑的计算步骤。在浏览过代码，手足无措数秒钟后，我迅速转换了思维：“化学反应的代码归根结底就是计算一个很长的公式。我没有必要看懂为什么这么做，我只需要知道哪些加减乘除运算可以被我优化就行了。”我的这种冷静能力与思维跳转能力非常惊人。</p>
<p>在<a href="https://zhouyifan.net/2019/12/23/Software-Engineering-Project/">这个大作业</a>中，我学到了查看代码性能瓶颈的方法。经过检测，代码最耗时的部分竟然是一个<code>exp()</code>自然指数运算。自然指数本身是一个很容易理解的函数，高中数学就讲过这个函数的性质。这个函数在数学上很好表达，在程序中计算起来却非常麻烦。因为这个函数的值往往是一个无限不循环数，而程序只能进行有限的计算。程序只能通过多次的加法、乘法来得到一个十分近似的结果。程序总计进行了3亿次自然指数运算，代码性能瓶颈出现在这确实也情有可原。</p>
<p>程序中的指数运算，需要调用标准库。标准库是高级程序员们反复锤炼，被无数人反复验证的代码。这个自然指数运算对我来说是根本不可能修改的。看到这个情况，我心都凉了半截。</p>
<p>虽然一上来就碰到如此困难的情况，但我再次调整了心态。指数运算耗费了三成的时间，还有七成的运行时间可以被优化。我把目光又放到了其他运算速度较慢的代码上。凭借多年的高级语言（比较抽象、接近自然语言的程序语言）编程经验，我意气风发，大刀阔斧地对代码进行优化。我主要对代码进行了预处理、循环合并这两类优化。预处理就是把一些程序中不会变动的常量提前算好，避免每次重新计算。就好像你提前买一箱抽纸，这样几个月都不用再去买纸一样。循环合并就是把条件一样，需要反复做的事情，再每个“反复”中一起做掉。比如让你去测全校同学的身高体重，量身高和量体重的仪器摆在一起，这里假设两台仪器不能同时工作。你不能让同学挨个测完身高，回教室休息一下，再一个一个回来量体重。每个人量身高体重的时间虽然没变，但来回教室、排队等待的时间浪费了。最好的方法是每个人先量身高，再量体重。如果你懂编程，你或许能理解，这里的来回教室时间就是CPU从内存中读数据的时间，排队时间可以看成循环变量、控制流耗费的时间，在一个循环里做尽可能多的事能减小时间开销。</p>
<p>以上两类优化是非常基础的优化。经过优化后，程序运行时间从26秒到了17秒。很可惜，速度还没有达到2倍，我的成绩连及格也没有。我反复查看了其他部分的代码，绞尽脑汁也没有想出哪里可以优化。于是，我只好把目光再次放到了开始的指数运算上。</p>
<p>经过观察，我发现指数运算是一批一批做的，也就是一次会对多个数据依次执行步骤相同的指数运算，而标准库只能每次对一个数据进行指数运算。这里面有没有可以利用的空间呢？我凭借着十多年与搜索引擎打交道的经验，总算找到了一个比较厉害的数学运算库。里面有一种对批量数据进行指数运算的函数。我满怀期望地用这种”高级“函数替换了原来的函数。结果非常令人惊喜：程序的执行时间从17秒降到了12~13秒，速度整体变成了原来的2倍，我总算及格了。</p>
<p>开心之余，我总感觉自己忘了些什么事。仔细回忆了一下，代码优化不仅要快，原来程序的输出结果还不能被更改。我还没有验证新代码的正确性呢！我赶快把新代码和旧代码的结果进行了比较，发现新代码的输出结果发生了变化！</p>
<p>我改动了那么多处，究竟是哪一步出错了啊？！为了找出代码中的BUG，我采用了古老而有效的控制变量法。我把旧代码粘贴了回去，一段一段地把代码更新成新代码。如果某一次更新后运行结果有问题，就说明这段代码有问题。我顺利地找出了不少的低级错误。可是，我最不想碰到的情况发生了——</p>
<p>我发现指数运算的那整段代码中存在问题。我尝试地把指数运算的那一行改回了旧版本的代码，输出结果就正常了。也就是说，这种优化的指数运算会导致结果不正确。</p>
<p>我又慌了，心想指数运算这道坎可能就是过不去了。但我内心中突然涌现出的自信告诉我，我的新代码没有错误。这个新的指数运算函数是intel公司写的，如果有问题，只可能是他们有问题。代码的运行结果虽然改变了，但是代码不一定有错——这两句话并不是矛盾的。在精度较高的数学计算中，如果小数点后面好多位出现了偏差，只能算是结果有误差，不能说结果错误。况且原来的代码也只是对数学计算结果的一种近似，你怎么能保证原来的代码就离正确的结果更近呢？给你两块手表，两者差了几秒，你能知道哪块手表是正确的时间吗？保证这样的心理，我对新代码输出结果的误差进行了计算。</p>
<p>经检验，新代码和源代码的相对误差在小数点后6位，也就是0.0001%这个数量级。老师可没有强调结果要完全相同，或者误差保持在什么范围内。从道理上来看，只要保证代码整体的正确性就行。在强烈的自我暗示下，我接受了代码输出结果不完全相同，但我却是正确的这一事实。</p>
<p>如前面所述，我已经尽可能地在其他地方优化代码，就目前而言，这份代码对我来说是最优的。第一天天色已晚，我选择偃旗息鼓，明日再战。</p>
<p> 第二天十点，烈日当空，天朗气清。我再次开始着手优化代码。经过缜密的分析，我觉得我缺少部分的知识，我需要从别的角度入手，用一些我不太熟悉的方法优化代码。</p>
<p>程序的运行可以分成串行和并行。串行的概念非常简单，我们人的大脑就是串行的。你不能说我左半边脑子在浏览选项题，右边的大脑跑去想填空题去了。并行就是多个串行，可以理解成多个人合作做一件事。写论文时，你写正文，我写摘要，我们同时开始写，这就是一种并行。显然，N个人干活，必然不能让事情的快N倍。因为人与人之间的交流存在着极大的效率浪费。如若不然，为什么每个企业要设置那么复杂的管理体系呢？并行程序也是如此，在硬件支持的情况下，程序并行可以提高速度，但也有一定的资源损耗。</p>
<p>我学过并行的知识，了解并行的概念，但没有并行编程的经验。于是，我只好以”C++ 并行编程”为搜索词去搜索有关信息。很快，我就搜到了一个满意的答案：有一种叫OpenMP的并行编程API，只需要在程序里加一些代码，就能把串行的程序变成并行的程序。但是，要是只加少量代码的话，只能并行执行循环结构，且只有对重复次数较多的循环起到优化作用。比如搬1000个箱子，让10个人来搬来搬肯定比1个搬快，而且大家只需要在搬完后交换一次信息，确认一下所有箱子都搬完了。想把更复杂的代码并行执行并达到优化效果，就要学更多的知识了。考虑到我所剩的时间不多，我打算只用OpenMP来并行优化循环。</p>
<p>我写了个算加法的循环，并用OpenMP并行优化。经过实验后，这个简单循环的运算确实变快了，优化成功了。看来，并行加速并不是很难啊。现在大作业代码的性能瓶颈还是那个指数运算。指数运算是作用在一个数组上的，目前的实现方式是循环对数组的每一个元素做指数运算。如果能把这个循环并行化，但代码的运算速度肯定能快上很多。于是，我把OpenMP的并行代码运用到了指数运算循环上。</p>
<p>可是，并行化后，代码不仅没有变快，反而变慢了。甚至随着并行线程数（可以理解为同时有多少个人在做同一个工作）增加，代码会运行得越来越慢。</p>
<p>做为一个自信的程序员，碰到问题时，我的第一反应肯定不是觉得我自己写错了，而是这个OpenMP调用得用问题。可能我使用这个工具的方法不太对。既然这样，也没时间去学新的东西了，没有轮子就自己造轮子，我只好自己来写一个多线程的并行程序了。于是，我删掉了OpenMP的代码，手写了创造线程、用线程做指数运算、同步线程的代码。我自己写的代码，肯定没问题了吧？</p>
<p>结果，用我自己的并行代码运行程序后，程序的运行速度也变慢了。做为一个正常的程序员，在代码全是自己写的情况下还发现了运行上的问题，第一反应就是出bug了。于是，我把那段并行的代码拎了出来，单独测试了好久。可是，无论怎么调试，都没有发现问题。第二天就在无聊而令人烦恼的调试中度过了。</p>
<p>晚上，躺在床上，苦恼地思索着代码里的问题。突然，我想到：是不是我的代码一直没有问题，而是并行这个方案有问题？做循环的次数只有10多次，如果用多线程的话，线程之间沟通的时间，就远远多于并行运算本身减少的时间。正所谓“三个和尚没水喝啊”。没办法，第二天就这么浪费了。我及时止损，准备使用新的方法优化程序。</p>
<p>经过昨晚的计划后，第三天，我早早地打开电脑查询一个新的技术。我顺着并行编程这一条线索，想到了另一个并行技术——SIMD（单指令多数据流）。这个技术就好比之前是一个人搬一箱货物，现在这个人可以拿手推车，一次搬几箱货物。在SIMD中，唯一增加的成本，就是货物得提前按组打包，这样才能够一组一组地搬运。这一项成本远远小于之前多线程之间沟通的成本。我去网上查了什么AVX指令集，学会了如何一次对4个数据进行计算。这样，不仅是那个指数运算，还有一些相邻的乘除法运算也可以顺便用并行</p>
<p>这次程序运算时间在8、9秒左右浮动。严格来说，程序并没有优化到三倍。但是，要是精心挑选一组比较看得过去的测试时间的话，应该能在报告里声称我把程序优化了3倍。这下，80分才算勉勉强强拿到。</p>
<p>代码真的就不能再优化了吗？既然老师敢说把速度优化4倍就能拿到满分，说明这份代码肯定还是有优化空间的。我把这份代码从头到尾读了一遍又一遍，在我的知识范围内，能用的优化小伎俩都用上了。但是，程序的运算时间几乎没有减少。我感觉自己已经弹尽粮绝了。</p>
<p>经过多次优化后，指数运算的用时占比已经不算很多了。一些对一个常量数组的遍历、取值、运算的用时占比逐渐高了起来。就好像一个邮递员要挨家挨户上门取件，再把货物送到另一个地方一样。这一部分的时间完全耗费在了跑到每个人家门口，敲门等人开门上，跑腿的时间反倒是可以忽略不计。这部分循环操作数组的代码是不能用之前的并行来优化的。这部分的代码没有什么过多的操作，自然也几乎没有改动的空间。我通读代码，看到这一团改不了的代码时，总会无可奈何地快速跳过。</p>
<p>我对代码优化已经绝望了。走神时，我想起了计算机体系结构课里的知识：现在CPU采用了流水线的设计，跳转指令（比如循环）会导致流水线的断流。为了让代码更快运行，某些时候能不用分支、循环就不要用。</p>
<p>我突然产生了一个奇怪的念头：循环吗……如果我把从常量数组取值的循环全部拆掉如何？这个可以理解成快递员上门取一层楼的货品时，可以用循环表示：取这个房间的货，往前走；去这个房间的货，往前走；……；如果这一层没有住户了，就上一层楼。但是，这是一个常量数组，即我可以提前知道这栋楼有几层，每层有哪几个房间。这样，快递员的指令就变成了：去101取件；去102取件……去606取件这样确切的命令。快递员不用动脑筋去观察什么时候把这层楼走完，可以上一层楼了。抛弃掉回产生跳转指令的循环后，按理说代码能变快很多。</p>
<p>但是，我们初学编程认识循环时，就知道循环是用来简化代码的。现在，我却要反常识地把循环拆掉，把要执行的代码展开来，一行一行写出来。这太反常识了。当然，循环拆掉代码展开后，代码会变得特别长，其中会包含很多重复的代码。与其我手动写，不如写个脚本自动把这些代码生成出来。于是，我写了一段生成代码的Python代码，把原来的循环拆掉了。</p>
<p>我持着怀疑的态度，一边苦笑地看着那些丑陋的代码，一边等待着程序运行结束。没想到，程序的运算时间竟然大幅度变少了！这次运行时间之间减少到了6秒多，程序的速度基本上是最开始的4倍了。我还没来得及烦恼怎么跨过优化3.5倍这道分数砍，程序一下就优化到满分了。</p>
<p>这太有趣了！违反常理地拆掉循环竟然能让代码加速。我关掉了再也不想多看一眼的代码，开开心心地把优化四倍的结果写进实验报告里。然后，我开始着手写这篇文章，记录一下这段过程紧张，结局却是Happy Ending的代码优化之旅。</p>
<p>噢，对了，源代码我是有的，但我一定不会开源。我能想象到，老师肯定会把同一份大作业用好几年。为了让这门课公平一点，我是不会上传代码的。当然了，稍微有一点编程水平的人，看完这篇文章后，都知道了该怎么优化程序了。这篇文章也算是给和我一样初学代码优化的人一些学习上的启发吧。</p>
<blockquote>
<p>这是2020年6月份的文章，我当时写到一半搁笔没写了。趁现在有时间，赶快填一个坑。</p>
<p>现在重温这段经历，当时提笔时的激动已经没了，只剩下了怀念。两年前，我肯定想不到这四天不到的代码优化经历，竟然是我本科期间学代码优化技术学得最多、学习效率最高的一段时间，也想不到几天后我即将开始新的工作，会把之前学到的这些代码优化技术全部用出来。</p>
<p>对了，最近我在写很多文章。今天不认真的文字写了3000~4000，之前比较认真的写得话一天3000字都不到。我本来以为这个速度很慢，上网一看，这才是正常速度。把写东西单纯当一个爱好也挺不错嘛。虽然既没有编程有趣，也没有编程挣钱就是了。</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/03/08/20220123-everyone-is-lonely/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/08/20220123-everyone-is-lonely/" class="post-title-link" itemprop="url">人，都是孤独的</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-08 18:48:11" itemprop="dateCreated datePublished" datetime="2022-03-08T18:48:11+08:00">2022-03-08</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/" itemprop="url" rel="index"><span itemprop="name">杂谈</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/%E9%9A%8F%E7%AC%94/" itemprop="url" rel="index"><span itemprop="name">随笔</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>最近，我欣赏了《白色相簿1》的游戏和动画，心中五味杂陈，一直有话想说却不知道该怎么表达。今天，我从高烧中熬过了一晚，身体和精神上都得到了净化。趁此机会，我打算写一写我最近的一些感想。文章的结构和内容也不去仔细琢磨了。想到哪写到哪。</p>
<p>结果我没能够第一次性写完。第二次写的时候想了下，希望这篇文章能总结一些我的个人观点，并一如既往地传递我的积极的人生态度。</p>
<h2 id="心理学之习惯论"><a href="#心理学之习惯论" class="headerlink" title="心理学之习惯论"></a>心理学之习惯论</h2><p>社会学中有“原生家庭”这一概念。原生家庭，指孩子从小和父母一起构成的家庭。书本里说，一个人后天的行为都是原生家庭的再现。后天和其他人的相处模式，其实来自与和原生家庭里某位家庭成员的相处模式。心理治疗师在了解咨询人时，也首先会询问咨询人童年的信息。稍有调查就能发现，科学家们十分认同“童年对一个人后天的性格有很大的影响”这一观点。</p>
<p>仅基于这一观点，我根据我自己的观察与分析，想提出一个更一般的观点：人的性格，完全取决于人的习惯。什么是性格？拿“开朗”与“内敛”这一对相斥的性格来举例。当你和一个人讲话时，如果说这个人很开朗，那么这个人或许会笑着主动接过你的话题；如果这个人比较内敛，那么这个人或许回答个两句就不做声了。仔细一看，性格不就是某种意义上的习惯嘛：人在某种情况下最自然的反应，就是习惯。说一个人有早起的习惯、跑步的习惯，其实就是说一个人最自然的起床事件是早上6点，或者是一个人到了下午会自然地出门跑步。推广地来看，性格就是习惯。</p>
<p>如果只是把常识中的定义推广，那并没有什么用。我提出的“性格取决于习惯”的观点，有什么意义呢？其实我想强调两点：和你后天会养成跑步的习惯一样，性格不仅仅形成于人的童年；性格和习惯一样，是人长期以来刻在大脑里的信息，虽然难以改变，但是坚持下来还是有改变的可能的。</p>
<p>我认为，人的大脑在遇见新的事件时，会产生一个面对这个事件的解决方法。如果这个方法总是有效，人在面对特定事件时就会有一套固定的反应。这就是人的习惯。人在面对自己的情感时做出的习惯反应，就是性格。由于人在一出生时就要面临饥饿、恐惧等会伴随人一生的情感，所以人的性格大都在童年时被决定下来。人在后天也可能获得某一性格，想象一个从小衣食无忧的富家子弟，突然要在荒郊野外一个人生活，那么这个人会在后天才培养起面对负面情感的性格。总之，新性格诞生于新事件中形成的固有反应。</p>
<p>当人已经形成了某一性格时，这通常意味着人已经从许多生活经历中巩固了这一性格：比如一个人健谈，可能是他从小只要好好和父母交流，就能满足自己的需要；可能是从小喜欢和朋友聊天，感到很快乐；可能是喜欢让他人听自己侃侃而谈，享受被尊重的感觉……在长期生活经验的影响下，这个人的大脑已经形成了“只要和别人多说话，就能给自己带来快乐”的简单反射。这些一条条生活经验，使一个人的性格根深蒂固，成为了人的一部分。不过，反过来说，人的性格还是有改变的可能，只要逐条否定自己过去的做法即可。一个人习惯说谎，可能是小时候每次说实话都会被父母骂，而撒谎总能成功逃避。要彻底改变爱说谎的性格，需要面对自己童年的伤痕，思考自己以前每次说谎的后果，让自己彻底明白说谎不总是能让自己和他人变好。</p>
<p>这里从程序的角度总结一下我的“习惯论”：人在面对事物时，会把自己的计划写在一张表格上。表格的第一列是碰到的事件，第二列是碰到事件的解决方法。事件是”碰到饥饿“、”碰到恐惧“这些低层次的情感或者是”是不是要起床“这些很容易表达出来的事件。人在碰到新事件时，会另起一行，记录下事件的名称和自己的解决方法。下次再碰到同样的事件时，人会尝试同样的解决方法，并试图加以修正。当这个解决方法已经用过多次后，人在碰到事件时就不会加以思考，而是顺其自然地采用固定的解决方法。这种处理机制的动机也很好理解：人的思考能力是有限的，如果什么东西都要过一遍脑子，人早就累死了。因此，人通过”性格“这种采取过去相同行为的优化方法来减轻自己的思考量。</p>
<h2 id="孤独感从何而来"><a href="#孤独感从何而来" class="headerlink" title="孤独感从何而来"></a>孤独感从何而来</h2><p>上述内容只能算是一个不严谨的观点。我这辈子应该没时间去研究心理学，不会将其系统化。我之所以写那么多，是想提供一种心理分析工具，来分析人的各类心理活动。</p>
<p>孤独，展开来说是人感觉“一个人很难受，如果有人在自己身边就好了”。做为一种常见的情感，大家都可以轻松地说出孤独的定义。但是，仔细一看，为什么人们都会觉得“如果有人在自己身边就好了”呢？仔细去挖掘这一想法的动机，会发现孤独并没有看起来那么简单。</p>
<p>既然孤独是一个所有人都会有的情感，那么分析它就要抓住所有人的共性：婴儿时期。刚出生的时候，所有人都是十分相似的。大家对这个世界一无所知，却天生被赋予了“要在世界上努力活下去”的本能。饿了，会哭；热了，会哭；要睡着了，会哭……在身体上有不适时，婴儿会害怕。这时，通常母亲会安抚婴儿的情绪。婴儿第一次认识到了其他生命的存在，知道了其他生命能够减轻自己的负面情绪。在成长的过程中，人们认识了其他亲人，认识了伙伴，发现确实和他人相处能够令自己更加安心，这一条解决方法被记录在了人的大脑里。反过来讲，遇到麻烦时，也会下意识地寻求他人的帮助。寻求不到，就变成了孤独。</p>
<p>按理说人长大了，学会自己找东西吃，自己生活了，不会再有幼年时的那些担忧了。为什么还会感到孤独呢？这是因为，孤独不仅是由饥饿等简单的情绪构成的。只要身体上，或者尤其是心理上有了不适，人感觉自己已经无法处理一切后，就会像刚出生的婴儿一样感到害怕。过去的习惯让人下意识去寻求帮助，但却发现周围的人已经帮助不到自己了——随着年龄的增加，人的烦恼也愈发复杂，终有一天，人的烦恼只有自己能理解。所以，人感到了孤独。</p>
<h2 id="贫瘠的表达能力"><a href="#贫瘠的表达能力" class="headerlink" title="贫瘠的表达能力"></a>贫瘠的表达能力</h2><p>一瞬间的不适，只能归于害怕。长时间的害怕，才足以称之为孤独。正因为人与人之间的交流效率实在是太低了，以至于人在大部分情况下无法互相传递情感，才导致了孤独的常驻。                   </p>
<p>相比动物，人发明了语言，发明了文字，似乎就拥有了无穷的表达能力。但实际上，人类之间传递信息的效率比想象中要低出许多。</p>
<p>相比大家都有这样的经历吧：向他人问路后，他人自顾自的讲了一通。碍于面子，我们只好感谢对方的帮助，再顺着他人指着的直线方向寻求下一个人的帮助。在参观点一份套餐，我们只需要报出套餐的名字，甚至只要简单说一句“我要那个”就够了。交谈的时候，我们很多时候并没有听清别人在讲什么，有的时候仅仅通过对方的申请才大概脑补出对方的回答是积极的还是消极的。就连有充足时间组织语言的文字聊天，不配上表情包什么的，总是会令对话十分尴尬。</p>
<p>哪怕有了语言和文字，人类之间的沟通效率还是太低了。不然，为什么一些简单的概念要花45分钟来讲清楚呢？为什么同样是上课，有人学得好，有人学得差呢？为什么有的时候几行代码的事情要花好几页的长篇大论来解释呢？</p>
<p>要是说构筑于理性之上的深邃的理论，只要花上时间，不管再久，都是能够讲清楚的。那么，更加复杂的，用感性编织出来的无形的情感，有时恐怕用再多的言语也无法说清楚吧。</p>
<p>我曾经为了追寻高质量的交流，严密地建立了一个人的交流模型。人与人之间交流的效率，或者通俗说是深度，至少由交流欲望、对交流背景的了解程度、思辨能力相乘而得（相乘意味着只要一个因素偏低，最终的结果就也会偏低）。比如说想进行深刻的学术交流，就要在双方午后都乐意探讨学术时，与在同一领域做研究的聪明而乐于表达的人进行交流。</p>
<p>把这个模型放到情感上，就会得出非常悲伤的结论了：人和任何人交流情感的效率，几乎都无法超过自己和自己交流情感的效率。人为了处理自己的不安，有极强地感受自己情感的意愿；自己一般是最了解自己的。只有思辨能力，算得上是一个可变的因素。有些人处理自己心理的能力不行，有的时候对情感的沟通效率没有深谙人心的心理咨询师强。但在绝大多数情况下，人只能感受到自己强烈的情感，而他人是无法体会到相同的感觉的。</p>
<p>长时间的害怕孕育了孤独，人自然地寻求着效率最高的，自我心理疏导的方式来排解这些负面感情。或许熟练之后，害怕的感觉不在了，那个被称为“孤独”的感觉却伴随着只能自我对话的习惯，永远凝固并附着在我们的心上。</p>
<h2 id="在一起"><a href="#在一起" class="headerlink" title="在一起"></a>在一起</h2><p>在我看来，人注定是孤独的，这是一个从理论分析上来看无解的问题。</p>
<p>这是一个所有人都自然而然会碰到的问题。幸运的是，几乎所有人都在不自觉地努力寻找解决孤独发方法。</p>
<p>虽然从前面的分析可以看出，孤独不是那么简单的一个概念。但在大众普遍的认知里，孤独的原因是“一个人”。要是能和另一个人长期待在一起，就不会有孤独感了。因此，在大众的认知里，两个人交往，意味着两个人会“在一起”了。</p>
<p> 但是，在一起真的能解决问题吗？</p>
<p>如果是为了身体上的接触，那通过基因获得的一瞬间的快感是五八支语长久以来的孤独的吧。如果是恋爱的话，也只是在激素和新鲜感的驱动下，从一个幻想中的完美对象里汲取自己欠缺的情感。哪怕是数十年亲人般的陪伴，也有可能只是过多的共同经历把双方重塑成了一个新的集体，分离已经成了如伤害自己一般不会去想的选项了。</p>
<p>再近的距离、再久的同行，也无法解决情感无法交流这一本质问题。</p>
<p>但或许，消除孤独并不需要无损地体会或传递情感。相处得久了，两个人之间可能会产生的能力：不需要完全理解对方，只要一个信号，就能顺利地索取与给予所需的情感。在这种情况下，两人之间构造了一种新的沟通方式，一个简单的行动，几句简单的叮咛，反倒能化作携带了无数信息的暖流，流入人的心里。说到底，人只能体会到自己的想法，自己因为害怕而寻求帮助，所以也能够用自己的方式去理解别人的行动。</p>
<p>不过恕我直言，在我的观察统计下，凭大部分人的思考水平，世界上能做到这种程度的家庭少之又少。只有少数的夫妻能够较好地消除孤独，并把美好而恰当的关怀传递给孩子。</p>
<h2 id="向世界宣泄自我"><a href="#向世界宣泄自我" class="headerlink" title="向世界宣泄自我"></a>向世界宣泄自我</h2><p>如果没有人能好好倾听自己的复杂的想法，为什么不干脆放弃向某个人说明，并用自己的方式把自己的想法完完全全地展示出来呢？</p>
<p>之前我看到网上有人讨论，为什么伟大的艺术家或思想家都有抑郁的倾向。有人说，不是他们创作后才抑郁，而是因为抑郁，参创作出了伟大的作品。我认为这是正确。越是进行深刻的思考，越是能得出常人难以理解的结论，越是缺少和常人的共同语言，孤独感越发沉重。于是，他们选择逃避，逃避进了自己最擅长的事情里，继续思考着。在正反馈的作用下，不被常人理解的痛苦加深到了难以忍受的程度，只好放弃与常人的交流，把自己长久的以来的感受宣泄给整个世界。</p>
<p>但要说那些创作者是抑郁吗？我看未必。固然有很多创作者最后都选择了自杀，但哪怕是这些人，他们生前都是热爱着生活的吧。正是因为抱有着热爱，所以不断创作着，不断发泄着情感，不断与黑暗的内心抗争着。他们明明知道，自己的作品被会被其他人欣赏，赞美的话语永远无法传进他们的耳内。即使如此，他们还是把作品留传下来，把高尚的思想传递给他人，给予每一个和自己相同困境的人以温暖。这样的人，绝对称不上是对这个世界失望吧？</p>
<p>中学时，叫我写一篇800字在作品会让我头疼死。但后来，我发现有东西想写时，文字就会从我的指尖流到屏幕上。先是理论，再是情感。只要有想表达的东西，这些抽象的想法总能转换成具体的文字。既然如此，那艺术家就更加幸福了。他们有画笔，有音符。无限的信息，被压缩到了简单的实体中。再也不用考虑有没有人愿意倾听，再也不用考虑有没有人能够理解，只需要把自己的情感，原原本本地创作出来就好了。</p>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>在人的出厂设置被确定后，人的孤独就是不可避免的事情了。但也不必过度悲观，选好表达方式，想好表达对象，每个人都可以战胜孤独。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/03/04/20220303-Singapore-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/04/20220303-Singapore-1/" class="post-title-link" itemprop="url">《新加坡2022》游戏攻略（一）：凌晨四点的香港</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-04 01:13:40" itemprop="dateCreated datePublished" datetime="2022-03-04T01:13:40+08:00">2022-03-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/%E7%94%9F%E6%B4%BB/" itemprop="url" rel="index"><span itemprop="name">生活</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>今天不吹牛、不搞笑、不乱用第一人称叙述，好好写一篇正经点的文章。</p>
<h1 id="《新加坡2022》游戏攻略（一）：凌晨四点的香港"><a href="#《新加坡2022》游戏攻略（一）：凌晨四点的香港" class="headerlink" title="《新加坡2022》游戏攻略（一）：凌晨四点的香港"></a>《新加坡2022》游戏攻略（一）：凌晨四点的香港</h1><p>你认为出国是一件有趣的事吗？我想，大部分人的答案都是肯定的。出国嘛，在新鲜的地方，碰到新鲜的事物，想必是充满乐趣的。可是，一出国我才发现，出国，尤其是出国居住一段时间而不是出国旅游，是一个充满挑战的过程。你需要克服沟通上的障碍，了解当地的生活方式，迅速把国内的生活经验迁移到新的国家里。又正逢全球疫情，严格的防疫政策更是令适应新生活的难度大增。对我来说，这几天的出国体验与其说是游玩，不如说是攻克一款角色扮演游戏。接下来，我将按时间顺序分享一下持S-Pass（新加坡的一种工作签证）在疫情尚未结束的2022年在新加坡安顿、生活的经历。希望这篇文章能给即将出国却没有出过国的人一些启发，同时也能让大部分人看得有趣。</p>
<h2 id="冒冒失失地启程"><a href="#冒冒失失地启程" class="headerlink" title="冒冒失失地启程"></a>冒冒失失地启程</h2><p>如何用一段文字来描述上海地铁的拥挤呢？</p>
<p>一般人肯定会去描写上班高峰期上海地铁的“盛况”：地铁像浸满了水的海绵一样，再用它来吸水的话，能把水滴吸进去，同时也会把一些水漏出来。</p>
<p>但我不会这么写。我会写道：九点半，上班高峰期已过，通往徐家汇的地铁上依然挤得水泄不通。</p>
<p>要问我为什么会知道这件事？没办法，出国前最后一天，我还是起了个早（相对而言），被迫体验了一次上班期的上海地铁。为了完成任务：<strong>获取核酸检测报告</strong>，我不得不在早上前往离宾馆较远的一个医院。</p>
<blockquote>
<p>任务名：获取核酸检测报告</p>
<p>任务背景：我是在头一天晚上先在香港中转，第二天下午4点抵达新加坡。入境香港和新加坡时，需提交入境时前48小时内的核酸检测报告。</p>
<p>任务目标：</p>
<ul>
<li>获得时间合适的核酸检测报告</li>
<li>核酸检测报告必须是英文</li>
</ul>
</blockquote>
<p>如上所述，我在出发第二天下午入境新加坡，最早要在出发前一天下午4点做核酸检测。考虑到医院的服务时间、入境时间与飞机抵达时间的时间差，我打算出发当日上午做核酸检测。由于核酸报告必须是英文，我保险起见挑了家只出双语检测报告的医院（我不想打广告，就不说医院名了，可以轻松地在网上搜到）。于是乎，我只好一大早挤地铁赶往那家离我的宾馆较远的医院了。</p>
<p>仔细一想，这么早起来挤地铁，完全是转机害的。要是能够直飞过去，就没这么多麻烦了。可是，从香港转机到新加坡，只要2000多元，其他的转机方案都7000起步，从上海直飞更是10000起步。不过也托了是住在上海的福，能有从香港转机的选择，国内其他地方似乎连这个实惠的选择都没有。为了省点钱，踩着时间点去做核酸检测这点苦头还是得吃的。</p>
<p>我9:50到的医院，酒店12点退房，医院到酒店的交通时间是半小时，时间并没有那么赶。但是，登记检测信息时，工作人员热心地告诉我，我没有在个人信息中上传护照信息，打印的报告里不会有我的护照号码，建议我登记了护照信息再做检测。同时，他还告诉我只有在上午11点前完成检测，报告才能在下午4点时得到，否则报告得第二天才出来。我的护照还在宾馆里。这样一看，时间非常紧迫。我算了下，现在回宾馆，最早也是10:50回到医院，太赶了。同时我还回忆起来，新加坡仅要求核酸检测报告上有证明身份的信息，除了护照号外，出生日期也算是一种身份证明。相比于报告上没有护照号，在规定的时间里得不到检测报告更成问题。于是，在0.03秒内，我完成了回忆与利弊分析，没有选择回去，而是直接做了核酸检测。</p>
<p>出发的一大早，我就碰到了一项需要决策的挑战。虽然略显冒失，但我还算是成功了。没想到，这次的挑战仅仅是个开始……</p>
<h2 id="上海机场"><a href="#上海机场" class="headerlink" title="上海机场"></a>上海机场</h2><p>做完核酸检测，我在时间充裕的情况下整理好了行李，在宾馆退了房。我身背沉甸甸的登山包，一只手推装了Switch、PS5、书、衣服的旅行箱，另一只手把杂物袋按在旅行箱上辅助推行，就这样踏上了旅程。</p>
<p>刚才的核酸检测任务还没有结束。报告是4点钟给出，但我4点钟的时候必须要赶到机场了。报告是电子版的，我不可能去医院要一份纸质报告，只能在机场打印。还好上海浦东机场是有打印服务的。我按照网上的信息，在T2航站楼2楼星巴克对面找到了打印处，花6元打印了一张纸的报告。至此，获取核酸检测报告任务正式完成。</p>
<p>我还预约了携程的取外币服务，在A号值机口附近成功以1:4.9的高价换到了一点新加坡元（正常的汇率是1:4.6），就当是花一点小钱体会一下为什么贪官的资产不好转移到国外吧。</p>
<p>正当我悠哉游哉地准备去托运行李时，猛然看见“充电宝禁止托运”这一信息。我的充电宝塞在大旅行箱里。为了避免未来的大麻烦，我强忍着麻烦，打开了旅行箱，从衣服堆中挖出了充电宝。果然，我碰到了意料之内的麻烦——行李被打乱后，旅行箱关不上了，这可太麻烦了。我只好把最厚的棉袄从旅行箱里拿出来，这才解除了麻烦。没想到，这件棉袄在后面还帮到了我，正所谓麻烦反被麻烦误啊。</p>
<p>国际航班的值机处的服务非常到位。工作人员细心地帮我检查文件，还叮嘱我要提前下好一个填新加坡入境信息的APP。国际航班的安检处人也很少，总算不用排老长的队了。我刚想多夸几句国际航班的好处，就被安检给刁难到了。安检时，不仅要脱外套，带金属的皮带还得脱——唉，脱就脱，反正是男生嘛。什么？螺丝刀、剪刀都不能带？唉，扔就扔吧，亏我当时特意从房间里翻出来的。怎么书包检查了好几次还要检查？书包里电线太多了？为什么叫我们拿出电子设备的时候不叫我们把电线也拿出来啊。真没想到做飞机安检的时间能超过安检排队的时间。</p>
<p>我向来是讨厌提前到火车站或机场的。但今天我特意在起飞前提早了3个半小时到机场，时间还将将够用。感觉机场的主线任务也不算好做啊。</p>
<h2 id="第一趟飞机"><a href="#第一趟飞机" class="headerlink" title="第一趟飞机"></a>第一趟飞机</h2><p>上了飞机，整趟旅途最舒服的时间到来了。</p>
<p>我第一次坐上了两列过道，一排8个座位的大飞机，第一次在飞机上看到可以自己选择节目的机载电视。飞机上的电影都挺新，我本来兴致勃勃地播放起了柯南最新剧场版，却偶然看到隔壁大哥正用耳机连着电视，又一看电视只支持USB的耳机口，我没有能接USB的耳机，因为不能享受到最完美的体验，一气之下关掉了电视。</p>
<p>还好我是电脑不离身的人。我反手就掏出电脑玩起了不吃资源的小型游戏。飞机上微凉，我正好披上棉袄，不冷不热刚刚好。就这样，我在娱乐、就餐与瞌睡中，度过了舒适的机上时光。</p>
<p>没想到啊，飞机上我不仅享受的是最后的晚餐，还是最后的睡眠。</p>
<h2 id="折磨人的过夜转机"><a href="#折磨人的过夜转机" class="headerlink" title="折磨人的过夜转机"></a>折磨人的过夜转机</h2><p>在香港转机，钱是少花了，可麻烦事一点也不少。我必须在香港机场停留，直到去新加坡的航班启航。我得从晚上9点多停留到第二天约中午12点，在机场候机室过夜是免不了的了。</p>
<blockquote>
<p>任务名：香港机场过夜</p>
<p>任务背景：在去新加坡的航班抵达之前，我必须一直待在香港机场候机室。</p>
<p>任务目标：</p>
<ul>
<li>熬过这段时间</li>
</ul>
</blockquote>
<p>在订票的时候，我就已经做好了在机场过夜的准备了。我看了下别人的攻略，得知可以躺在三连坐的座位上睡觉过夜。但我也做好了最坏的打算，反正早上6点抵达上海的绿皮火车我都熬过来了，随便找个地方靠一靠，一晚上睡不好也没什么问题。</p>
<p>办完第二天新加坡航班的登机牌后，我找好一个有插座的座位，搭起了一个临时个人领地。这个有插座的座位是二连坐，躺不下来，果然事情没有那么顺利。幸好我准备了后手——我在出发前，买了三包装的抽纸，这些抽纸放在我手提的杂物袋里。我在到上海机场之前的那个午休时，恰好发现放在行李箱上的杂物袋里的抽纸可以做为我坐躺时的侧面靠枕。实验表明这个靠枕睡起来还挺舒服的。我本来准备把这个天然靠枕装置做为香港机场过夜的杀手锏，却猛然发现我的行李箱早就拿去托运了。看来我的准备还是有一些漏洞啊。</p>
<p>没关系啊，我也不亏，有无限电量的电脑，就可以通宵玩游戏了。在机场转机，就省了几千块钱。等于说我通宵玩游戏一晚赚了几千块钱。这么一想我不仅不亏，反而赚大了。</p>
<p>小玩了一会儿，我想起还有个新任务要做：</p>
<blockquote>
<p>任务名：新加坡租房</p>
<p>任务背景：我要在新加坡有一个临时的居住地，离学校尽可能近，不能太贵。</p>
<p>任务目标：</p>
<ul>
<li>获取足够房源</li>
<li>成功解决居住问题</li>
</ul>
</blockquote>
<p>我之前就计划在过夜的时候来搜集新加坡房源信息。出发之前，我总会觉得时间还早，而且不能看房，搜集信息没什么意义。在有充足时间，且事情愈加紧急的情况下，搜集房源是最高效的。</p>
<p>之前我租房都受到了帮助，因此没花太大功夫。这次，对租房方式的不熟悉加上对国外的不熟悉，令搜索房源成了件对我而言极其困难的事。还好这晚时间有多，加上我过人的智力与游戏装备购买比较经验，我迅速掌握了新加坡租房网站（propertyguru）的用法，了解了学校附近租房的普遍价位。当然，实话实说，并不是我搜集信息的能力很强，而是学校附近的房子都太贵了，在我预算之内的房子根本就没几间。若是加上整租这一需求，满足条件的房子中仅剩下了一所公寓。这下也好了，没得选择，等于不用选择，等于选择完毕。很早之前我就了解过这所公寓，条件和价格都没什么大问题，到时候直接去住就行了。</p>
<p>想到这里，我心中的一大重担总算落地，心情瞬间愉悦起来。接近凌晨两点，机场笼罩着昏昏沉沉的气氛，只有我一个人大幅度地摆着手，在机场里兴奋地踱步，想象着之后的美好生活。</p>
<p>可惜，大脑和胃部同时向我发出了警告，我只好老老实实地回到座位上，以低功耗模式思考起来。我啃起了早就准备好的巧克力，令大脑可以持久地工作。</p>
<p>夜还很长，该干些什么呢？机场灯火通明，施工人员在最不会打扰到旅客、最恰当的时间里轰轰隆隆地进行着机场设施的修建，过夜的旅客们则瘫倒着勉强地休息着。这令人讽刺的对比太过于强烈了，我忽然灵感涌上心头，想动键盘写下一些东西。可惜灵感和情感到位了，大脑的运算能力不够了。我望着机场的天花板——香港机场的天花板，由三角形的金属板拼接而成。拱形的屋顶连接着候机室的两侧，由长廊的尽头延申而来，犹如巨龙的鳞片一般。机场内灯火通明，可黑夜中的天空却从屋顶金属板间的缝隙间透了进来，提醒着人们这是凌晨两点的候机室……我大概是想这样动笔，从景物写到在机场睡觉的人，讲一讲机场的不人性化，感慨下大伙儿转机的不易。但我的大脑当时太困了，空有思路，想了好久也没想好该怎样把天花板的模样描写清楚。</p>
<p>我意识到自己困了，开始准备睡觉，可机场恶劣的环境又令人难以入睡。我只好选了一个令我最容易入睡的方法：我找了个提供桌椅的办公区，以最接近上课时的坐姿趴在桌上，一边想象着老师讲课的场景，一边催自己入睡。果然，还是上课时的睡眠最香，我很快就睡着了。顺带一提，我是披着棉袄睡的，棉袄在制冷系统完美运行的香港机场又帮了我一次。</p>
<p>天亮了，为了时刻关注登机的信息，我回到了离其他人很近的原来的座位上。我靠在座椅上，第一次觉得薄而硬的座位是那么适合睡觉，惊醒后又立刻贪婪地入睡是多么令人满足。加上巧克力的能量续航，我在没有饿死也没有困死的情况下熬到了抵达新加坡的一刻。——前往新加坡的航班并没有提供饮食，对我而言坐飞机只是让睡觉的椅子变得软了一点而已。</p>
<h2 id="Hello-Singapore"><a href="#Hello-Singapore" class="headerlink" title="Hello Singapore"></a>Hello Singapore</h2><p>到了新加坡，我看到软绵绵的床和香喷喷的美食已经在向我招手了，肚子反倒不饿了，精神也抖擞了起来。</p>
<p>在入境处，我提供了以下材料：</p>
<ul>
<li>IPA（入境基本证书）</li>
<li>核酸检测报告</li>
<li>疫苗报告（是用微信小程序防疫健康码国际版生成的）</li>
<li>护照</li>
</ul>
<p>此外，在之前的两个机场除了提供这些材料外，也有若干信息要填。上海机场出境前要填什么海关信息，填去往香港的健康签名；在香港机场要填去新加坡的健康签名，还要填什么ICA表格。反正主要是打好疫苗，准备好核酸检测报告就行。剩下的资料工作人员都会帮忙指导。我算是比较幸运，在新加坡入境处花的时间少于上海和香港办理值机的时间。</p>
<p>按照之前的计划，我本来是要在机场买好手机卡和交通卡的。但我糊里糊涂跟着同行的旅伴走到了打车处，他告诉我没做完核酸检测前是坐不了公共交通的。我也没心思去思考他这段话是什么意思，只是想快点打上车，把行李放到宾馆里。</p>
<p>的士来了，司机是一位黄种老人，头发花白，却精神地帮我搬着行李，用老绅士来形容再贴切不过了。上了车，我说道:”I want to go to this place （我想去这个地方）.” Meanwhile I show the destination on the cell phone to the driver. 啊，不对，是我同时还给司机看了手机上标出来的目的地。</p>
<p>我这才发现，随着离国内越来越远，生活的方式变得越来越陌生。而我，得去主动适应这全新的环境……</p>
<h1 id="2022年3月博客广告"><a href="#2022年3月博客广告" class="headerlink" title="2022年3月博客广告"></a>2022年3月博客广告</h1><p>最近我可以在毫无压力的情况下放假了。我准备写一堆文章出来。敬请期待。不过现在肯定没有人会天天盯着我的博客，大概看的时候我已经把所有文章都写完了。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/02/17/20220216-friend-farewell/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/17/20220216-friend-farewell/" class="post-title-link" itemprop="url">朋友·离别</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-17 00:57:28" itemprop="dateCreated datePublished" datetime="2022-02-17T00:57:28+08:00">2022-02-17</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/" itemprop="url" rel="index"><span itemprop="name">杂谈</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/%E9%9A%8F%E7%AC%94/" itemprop="url" rel="index"><span itemprop="name">随笔</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>我和大家坐在一起吃饭。</p>
<p>把“大家”称作“同学们”的话，有点不太恰当，因为其中有些人严格意义上没有和我在同一间教室里一起学习过。倘若把年纪相近，在同一所学校同一段时间的校友都称作同学的话，把大家都叫做同学才算是勉强正确。</p>
<p>那为什么回到家乡后，第一件事是和大家出来吃饭呢？</p>
<p>因为在我看来，大家都是“朋友”吗？</p>
<hr>
<p>六七年前，同样一批人也是这样坐在一起吃饭。</p>
<p>但是，和大家在一起交谈时，我并不是很舒心。</p>
<p>我和有些人并不是很熟，不能像关系特别好的同学一样，自由自在地交流。</p>
<p>在那时，因为某些原因，我不得不强行找到一些能够交谈的人，以用一层薄纸封住我空虚的内心。</p>
<p>又因为某些原因，这层薄纸被捅破了。我发现，我的内心没有变得充实，反而日渐腐烂。</p>
<p>在修补内心的同时，我开始憎恶起这层阻碍我看到真相的纸来。</p>
<p>什么样的人才可以称得上朋友呢？</p>
<p>那恐怕只有和我思考水平相当，能够和我发自内心探讨深刻的问题的人吧。不需要花费多余的心思，只要所心所欲地说出自己真实的想法就好了。</p>
<p>以这个标准看去，我没有几个朋友。</p>
<p>也好，剩下的人对我来说，怎样都无所谓了。就是他们害得我没有及时发现自己内心的病症。</p>
<p>果然，如我所愿，我的内心一天天强大起来。只寻找这种意义上的朋友，成了我的信条。</p>
<hr>
<p>时至今日，我的想法似乎还是没有怎么改变。</p>
<p>随着时间的过去，我已经不用再费心思考朋友的定义，能够自然地和人接触了。</p>
<p>但如果有人问我什么是朋友，我依然会思索一会儿，再给出之前那一模一样的回答。</p>
<p>如果是在今天之前，还没有和再次大家坐在一起吃饭之前，我肯定是会这样回答。</p>
<p>现在，在这个饭桌上，之前就没什么共同语言的同学，如今也没有太多的话可以讲。</p>
<p>现在坐在我旁边，以前曾经玩在一块的同学，也因多年经历上的割裂，而不知道从哪个话题开始聊起。</p>
<p>大家都只好聊着吃饭之前，下午一起体验的游戏项目。</p>
<p>最近的共同经历，只有刚才的那几个小时了。再不然，就是中学时的那些事情了。</p>
<p>几年前，和同一批人相处时，那坐如针毡的感觉再次传了过来。</p>
<p>可是，我已经不是以前的我了。面对这令人恐惧，仿佛要把我拉回过去的感觉，我选择了抵抗：我要尽快离开这里，远离大家。我不想变回过去那个空虚的自己。</p>
<p>嗯，如果是真的和朋友一起的话，会有这么令人焦虑吗？</p>
<hr>
<p>但是，吃完饭后，大家把我拉到了下一个活动地点。</p>
<p>顺应着气氛，我和大家正常交流着。</p>
<p>就和今天下午一样，我们只是说着普通的话。</p>
<p>就像以前一样，没有深刻的思想，没有刻意的组织言语，我只是和大家说着话而已。</p>
<p>那不就和以前一样了吗？我仿佛是回到了因为心灵的空虚，而和他人交流的状态。</p>
<p>我已经不是以前的我了。我应该讨厌这样的情景才对。</p>
<p>可是，不知道从什么时候开始，我不太想离开这里了。现在的一切，不但不令我讨厌，反而让我觉得怀念。</p>
<p>正因为我已经改变了太多，能够再次体会到以前相同的感觉，就像是回到了以前一样。就好像这只不过是期末考试过后的一次聚会而已。</p>
<p>仔细想一想，过去的我的体验真的有那么糟糕吗？为什么我会沉醉于不再能回到的过去呢？</p>
<p>和不是朋友的人在一起的时光，会令人这么难以割舍吗？</p>
<hr>
<p>所有的活动结束后，我和一个同学恰巧在一起坐地铁回家。</p>
<p>以前，我们好像是经常会聊好玩的游戏。现在的我们，在玩的游戏上，似乎没有什么交集了。</p>
<p>我逐渐找回了现在的我的状态，主动地找了一些有意义的话题。我向他询问了近来的状态，并试图稍微聊一聊学科、职业这种稍微深刻一点的问题。</p>
<p>但是，和我预料得一样，谈话很快就以沉默告终。</p>
<p>有了四五年经历上的差别，我们早已像两束发散的光一样，射向了永远不会再次相交的远方。</p>
<p>或许从一开始，我们就没有相交过。我们性格之间的差异，本来就不足以支撑长时间的有效对话。</p>
<p>不过是恰巧，能够在同一时间、同一个教室里上课而已。</p>
<p>地铁即将来到换乘站，他要下车了。我像是要抓住什么似的，提前祝他新年快乐。</p>
<p>他笑着说，过几天在祝福也不算迟。</p>
<p>啊，我都忘了，过几天大家还会出来聚一次会。说新年快乐，还是有机会的啊。</p>
<p>但是，说再见的机会应该没有多少次了吧。</p>
<p>他……之前是我的朋友吧？</p>
<p>那么现在也一定还是我的朋友。</p>
<hr>
<p>下了地铁，我走在陌生的街道上。</p>
<p>说是陌生，只是因为地铁的出口建在了一个比较偏僻的巷子里。毕竟之前是没有地铁的。</p>
<p>什么嘛，只要走出小巷，还是我熟悉的街道啊。</p>
<p>说起来，之所以我对这里很熟悉，是因为我曾在附近住过一段时间。</p>
<p>我还在家乡的时候，换了好几次住处。今天我们跑了好几个地方，恰巧经过了每一片我熟悉的区域。</p>
<p>我一步一步走着。我努力地看着四周陌生的店铺与熟悉的道路，希望能把这一切的场景都在脑中刻下一丝印记。</p>
<p>为什么连一草一木也令我感到不舍呢？难道这些没有生命的场景，也是我的朋友吗？</p>
<p>什么是朋友呢？我又一次问了自己同样的问题。</p>
<p>能够和我探讨深刻问题的熟人——今天之前的回答是这样。</p>
<p>能够轻松地交谈的熟人——好像不是所有人都能找到合适的共同话题吧。</p>
<p>关系不错的人——这不是一般意义上的，对“朋友”两字的详细而无用的描述吗？</p>
<p>共同拥有一段愉快的时光的人——</p>
<p>这是我最终得出来的结论。</p>
<hr>
<p>离别之际，人为什么会对其他事物感到不舍呢？</p>
<p>经我的观察发现，人是自私的。</p>
<p>人自出生之际，就只能体会到自己的感受。人只会为了让自己获得更多的美好的感受，而贪婪地活着。</p>
<p>反过来说，人并不会主动在意其他事物，除非这些事物能够给自己带来好处。</p>
<p>或者，这些事物被当成了某个人的所有物。在意这个事物，就像在意自己那样符合道理。</p>
<p>路边的景色。</p>
<p>远去的故友。</p>
<p>这些事物显然不是能直接给我带来好处的东西。</p>
<p>可是，这些东西怎么也不像是我的所有物啊。</p>
<p>唯一能解释的就是，人不仅会把有形的事物当成自己的所有物，还会把经历当成自己的一部分。</p>
<p>对于家乡的风景来说，这里记载了我成长的一幕幕。它是我的经历中的一部分，是我自己的一部分。</p>
<p>对于人来说也是类似。和他人一起的，令人开心的经历，是我的一部分。</p>
<p>可是，朋友不是那么简单的一个词。朋友，可是要得到两方的认可才行啊。</p>
<p>那么，就这样解释好了。我们共同拥有一段愉快的时光。两段蜿蜒而不断延申的人生曲线上，那不起眼的几个交点，是我们人生的一部分，是我们互相视作朋友的证明。</p>
<p>离别，意味着再也见不到某事物。</p>
<p>意味着那些时光、那些风景不再会有了。</p>
<p>意味着人永远损失了一些东西。</p>
<p>面对不得不经历的损失，自私的人类会感到不舍啊！</p>
<hr>
<p>最近在玩“素晴日”，里面有几个话题令我很感兴趣。</p>
<p>一个是说，我们每个人都有一个自己的世界。</p>
<p>这个想法不假。人只有感知到了世界，思考并回应着世界，世界对人来说才有意义。</p>
<p>每个人都触碰了世界的一部分，对世界的交互方式有自己的理解。</p>
<p>那些东西，正是我们每个人自己的世界。</p>
<p>在我的想象中，每个人的世界都是外观一个不断的变动着的球。</p>
<p>说它是球也不算准确。球是三维的，只能和我们能看到的一切一样，记录三维的场景。</p>
<p>可是，自己的世界不仅有我们认知某一处的场景，还有我们在不同时刻见到的场景。</p>
<p>自己的世界，还包含着属于自己的经历。经历是有时间这一维度的，所以哪怕说我们的世界是球，那也得是一个不断变动，反映着不同时间的场景的球。</p>
<p>每个人都有自己的世界，即每个人都有一个属于自己的球。</p>
<p>总能找到一个时刻，两个球有了重叠的部分。</p>
<p>所以不用太悲观啊！即使同意了世界是由自己定义的这个观点，也能在自己的世界里找到别人的痕迹。</p>
<p>还有一个话题是说，人是唯一认识了死亡的生物。</p>
<p>事实上，死亡是生物永远也达不到的状态。生物死亡，即永远失去了“生物”这一称号。</p>
<p>所以，其他的生物都安然地活在世界上。</p>
<p>只有人会畏惧死亡。</p>
<p>为什么不会体会到的东西，会令人害怕呢？</p>
<p>我想，这是因为人体会过失去吧。</p>
<p>咽下去的东西，就再也体会不到它的味道了。</p>
<p>即使能重复做同样一件快乐的事情，同样的事情带来的乐趣终将令人厌倦。</p>
<p>只要是人，是有着高级智慧的人，就会意识到失去的存在。</p>
<p>死亡，意味着失去所有令人快乐的事物。</p>
<p>所以，人哪怕永远不会见到死亡，也害怕自己的世界被剥夺。</p>
<p>生活的价值，就是死亡失去的价值。</p>
<p>那也就是说，正是有了注定的死亡，才会有努力活下去的价值。</p>
<p>死亡的本质是失去。</p>
<p>那么，正是离别，赐予了我们朋友的价值。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2021/12/31/20211226-goodbye-my-sweet-dream/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/31/20211226-goodbye-my-sweet-dream/" class="post-title-link" itemprop="url">合眼告别美梦</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-31 01:15:33" itemprop="dateCreated datePublished" datetime="2021-12-31T01:15:33+08:00">2021-12-31</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/" itemprop="url" rel="index"><span itemprop="name">杂谈</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/%E9%9A%8F%E7%AC%94/" itemprop="url" rel="index"><span itemprop="name">随笔</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="合眼告别美梦"><a href="#合眼告别美梦" class="headerlink" title="合眼告别美梦"></a>合眼告别美梦</h1><p>七月，刚来上海参与为期半年的实习时，公寓旁地铁口前的野花还鲜艳地长着。哪怕是两场台风过后，树丛中仍能看见几抹彩色。可惜，最冷的新年一月来到了，再也不能看到树丛里的花朵了。</p>
<p>我所在的公司，离最近的大学只有数公里之隔。但校园内外的风景，却有天壤之别。校园里，只有低矮的教学楼，来去匆匆的行人，每天能看到的净是沉闷而重复的事物。而公司所在的写字楼有四十多层高。站在落地窗前，能看到黄浦江与天空交汇在一起，楼盘、马路、汽车，不过是背景板上的几处点缀。</p>
<p>刚来到办公室时，我还有点战战兢兢，不敢站在高处向下看。很快，我熟悉了周围的人，熟练了手头的工作，走遍了附近每一家美味的餐馆。现在，再站在窗前俯视大地时，我只会悠然地欣赏着外面的景色。</p>
<p>我的工作并不累人——当然，有些时候除外。十二月，我们组的项目即将上线。全组人都绷紧着弦，马不停蹄地赶着进度。月底，在全组人的掌声中，我们的项目如期面世。很快，项目受到了公司内外的一致认可。项目宣传稿发出去的第一时间，从不关心社交平台的我立刻转发了这篇宣传文。按下“发送”键后，手机界面跳转到了我的朋友圈，我忽然笑了出来：短短一分钟里，我的朋友圈被同一条宣传文刷屏了。而发出消息的，全是我们组里的人。大家虽然没有把激动的心情表达出来，却在宣传文发出后立刻不约而同地转发了这篇文章。</p>
<p>项目完成后，恰好要进行年终述职。身兼开发与管理二职的小组主管自豪地汇报着我们小组这几个月的工作内容。看着我参与过的一项项工作，我的思绪不禁回到了五个月前。</p>
<p>刚来公司的时候，我的编程环境怎么都配不好，连办理企业微信都花了整整一周。</p>
<p>来公司两周多后，我才初次为项目贡献了代码。虽然在现在看来那只是一份简单的修改，却让我感到自己总算是融入了整个小组。</p>
<p>后来，我实现了一个完整的功能，甚至还主导了一次大型的代码重构。</p>
<p>我习惯了与组内的前辈认真讨论工作的内容；我学会了其他部门共同协作；我结识了其他组的朋友，和他们一起聚餐。</p>
<p>总算啊，我们的项目上线了。大家的付出，都有了回报。</p>
<p>未来，这样的生活应该还会进行下去吧？</p>
<p>项目上线后，还会面对更多的挑战。我还会一如既往地为项目贡献自己的一份力。我越干越久，技能越来越丰富，说不定能独当一面，主导更多的工作。</p>
<p>凭借我的条件，找个门当户对的女朋友，安安稳稳地结个婚应该不是很难吧？到那个时候，我应该会搬出出租屋，租一个大一点的房子。</p>
<p>再攒个几年的钱，应该就能凑够房子的首付款了。我总算能住在自己的房子里了。是不是考虑再买辆车？或许还要考虑生孩子的事情了？</p>
<p>是啊，随着时间不断流逝，一切都会顺利地进行的。</p>
<p>到那时，我不再是骑自行车，而是开着汽车来到写字楼下。</p>
<p>我会穿着更成熟，更得体的衣服，与熟悉的同事们打着招呼。</p>
<p>敲一敲键盘，吃个午饭，睡个午觉，转眼就到了下午。</p>
<p>今天是每周组会的时间。在会议室里，我又会看到小组主管。</p>
<p>听着她汇报着我们小组的工作内容。</p>
<p>“……今年我们组把代码库开源了。明年，我们会探索更多方向……”</p>
<p>看吧，就像这样，她还会总结着我们组的工作内容。</p>
<p>“……我们会为我们公司做出更多贡献……”</p>
<p>嗯，为了我们公司。等等，“我们”的公司。那是谁的公司？明年一月，我就要离开这里了啊！</p>
<p>”……以上就是我的述职报告。“</p>
<p>小组主管完成了她今年的年终总结。我猛然回归神来——轮到我进行2021年的年终述职了。</p>
<p>”我虽然只在这里实习一小段时间，但我依然出色地完成了我的工作……“</p>
<p>和之前准备的一样，我熟练地开始了我的演讲。</p>
<p>”……在介绍我的具体工作之前，请允许我带大家一起回顾一下我的成长轨迹……“我把刚刚回忆出的内容，一字不差地描绘了出来。</p>
<p>当然，我没有讲出之后”梦境“里的那些事情。梦里的事情，是不是永远无法成真啊？</p>
<p>”……具体来讲，过去我做了这些事情……“</p>
<p>不对，过去的几个月对于我整个人生来说，算是美梦一般的存在了。刚毕业，没有家庭的压力，没有身体的负担，幸运地来到了这里。这是我之前想也不敢想的生活啊！</p>
<p>”……未来，可能我在这待不了多久了。但我真的很怀念在这里的时光，会继续当我们项目的社区贡献者……“</p>
<p>这真的不是一句客套话，这是我的真情实感啊。我虽然没和组里每个人都认真地聊过天，但我们曾经谈论过无数小细节，在同一片办公区里一起工作了几个月。没有组里的大家，哪有最后的成果，哪有这么开心的时光呢？我的想法，能传达给大家吗？</p>
<p>”……我在过去的工作里学到了很多。未来，我还会继续努力。谢谢大家。“</p>
<p>还好，我还可以谈着未来一个月的计划。我还可以轻轻合上眼，把这终将结束的美梦给做完。</p>
<p>然后，迎接告别。</p>
<p>初三的最后一天，全班浸没在泪水中，而我却木讷而不解地看着其他人。</p>
<p>大四的最后几晚，宿舍里的四个人默契地通宵玩着联机游戏。</p>
<p>下个月，我会一边告别，一边收拾行装。之后，我会转过身去，独自远离这令我看过无数美景的高楼。</p>
<p>公司，不过是又多了一份离职记录；上海，不过是又冷漠地送走了一位外乡人；亚洲大陆，不过是又见证了一次飞走的航班。</p>
<p>而我，又永远失去了一段生活，又多了一段新的生活。</p>
<h1 id="阅读理解环节"><a href="#阅读理解环节" class="headerlink" title="阅读理解环节"></a>阅读理解环节</h1><ol>
<li><p>如何理解标题“合眼告别美梦”？(14分)</p>
<p> 答：“美”，表示这段经历对作者来说十分享受（2分）。“梦”，表示作者终将告别这样的生活（2分）。用“合眼”而不是“闭眼”，更强调睁开眼又再次闭上眼睛的动作（1分），表达作者已经意识到这段经历已经步入尾声（1分），却依依不舍的心理（2分）。“告别”，是作者主动进行的，体现了作者抛弃旧生活，迈向新生活的决绝（2分）。标题虽然只有短短六字，却把作者对过去生活的赞美、对意识到一切终将过去的苦恼、对告别过去的不舍、对不再留恋过去的坚定这些复杂的感情都写了出来，可谓是妙笔生花（4分）。</p>
</li>
<li><p>文章中提到了结婚。结婚之后，竟然只是”租一个大一点的房子“，几年后才买房。没有房子，哪结得了婚？这里逻辑有没有问题？</p>
<p> 如果有人觉得这里逻辑有问题，那么说明这个人的心态已经不再年轻，已经完全融入这个无情的社会了。这个时候，你不应该去去关心我的文章写得有没有逻辑问题，而应该反思一下，自己什么时候开始心态发生了变化，什么时候把结婚和买房绑定到了一起。当你想通这些问题的时候，你一定已经收获了很多，而不会再去在意我文章写得有没有问题了。可以说，我这个地方故意要这样写，故意要去钓鱼，故意勾起读者对于自己的反省。</p>
</li>
<li><p>文章里提到的”开源“，”代码库“是什么意思？</p>
<p> 我们见到的程序，都是从源代码里生产出来的，就和食物是根据配方生产出来的一样。一般公司不会提供源代码。而”开源“，指公开一个项目的源代码。”代码库“，一般指 GitHub （放代码的网站）上开源出来的代码项目。</p>
<p> 补充资料：公元2021年，天才开源项目开发者周弈帆作为元老级开发者，参与了著名开源项目 MMDeploy 的开发。该项目成功减少了开发人工智能应用的成本，为后来全世界的人工智能化革命埋下了重要的伏笔。</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2021/12/25/20211207-Inscryption/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/25/20211207-Inscryption/" class="post-title-link" itemprop="url">《邪恶冥刻》简评：虎头蛇尾</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-25 21:33:46" itemprop="dateCreated datePublished" datetime="2021-12-25T21:33:46+08:00">2021-12-25</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/" itemprop="url" rel="index"><span itemprop="name">杂谈</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/%E4%BD%9C%E5%93%81%E8%B5%8F%E6%9E%90/" itemprop="url" rel="index"><span itemprop="name">作品赏析</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>做为天才卡牌游戏玩家兼设计师，我十分热爱卡牌游戏，经常向他人安利我喜欢的卡牌游戏。</p>
<p>一天，我的好朋友和我讲：“最近出了款叫《邪恶冥刻》的卡牌游戏，融合了”炉石”、”万智牌”、”游戏王”的玩法，还有《杀戮尖塔》中的Roguelike机制，强烈推荐你去玩。”</p>
<p>“这是款对称卡牌游戏吗？”做为资深卡牌游戏玩家，我立马提出了这个问题。所谓对称卡牌游戏，就是每个玩家的规则都是一样的，用同样的方式击败对手。而非对称卡牌游戏一般出现在PVE（人对战电脑）中，玩家一般使用卡牌作战，而boss只有血量和技能，不使用卡牌战斗。</p>
<p>“是的，但这款游戏做得很好。“朋友看出了我似乎有所顾虑，依然极力向我推荐着这款游戏。</p>
<p>我将信将疑地去体验了这款游戏，结果不出我所料——游戏的平衡性出了大问题。</p>
<p>最近我准备简单点评一些游戏。虽然我的点评可能会极度专业（我相信这个世界上没有多少人能提出像我这样专业的评论），且以批评、改进意见为主，但在介绍游戏的简况和优点时，我会用尽可能用通俗易懂的方式表达，让游戏经验较少的人也能读懂。《邪恶冥刻》给人的体验很糟糕。就像你去一家中端的饭店就餐，里面的装修、服务不输高档饭店。服务员热情地给你端上一块精美的一人餐蛋糕。虽说是一人餐，蛋糕上放满了琳琅满目的水果和巧克力。你舔了一口奶油，不禁感叹道：”太美味了！“你连忙又吃了几口，却发现这蛋糕越吃越没有味道。吃完之后，你发现肚子还是半饱。于是，你默默抱怨道：”有这个钱去装修饭店，不如在菜品上多下点功夫。“</p>
<p>在我看来，《邪恶冥刻》是一款十分可惜的游戏。作者很有实力，有丰富的游戏制作经验。他做出了一款画面表现力强的大杂烩游戏。刚玩一会儿，你会觉得游戏的各个机制都很吸引人。但是，游戏的平衡性极差，你很快就觉得游戏没有挑战性了。玩完了一个又一个看起来很有新意的关卡后，你用十来个小时就通关了整款游戏，看着作者花了不少时间做出来的演出（即玩家不能交互，而是像看电影一样看游戏剧情），感到索然无味。整款游戏可以用虎头蛇尾来评价，甚至是”虎头蛇身“。作者明明有丰富的游戏设计经验，却浪费了很多时间与资源去做冗余的游戏机制和演出，真的很令人惋惜。整款游戏的创意运用得好的话，这将是一部足以记入卡牌游戏历史的游戏。可能是因为卡牌游戏本身的平衡太难掌握了吧。</p>
<p>下面我将按照老规矩，先介绍游戏的亮点，再详细列出游戏的缺点。</p>
<p><strong>本文有一定剧透，虽然游戏不是由剧情主导的，被剧透也没啥关系</strong></p>
<h2 id="卡牌游戏大杂烩"><a href="#卡牌游戏大杂烩" class="headerlink" title="卡牌游戏大杂烩"></a>卡牌游戏大杂烩</h2><p>”我的回合！抽取一张松鼠牌！献祭一张松鼠，召唤一张狼崽。狼崽会在下一回合，变成3攻2血的狼，并在回合结束时朝前方自动攻击。失败的天平，会向你那边倾斜3点。“</p>
<p>”战斗胜利，我选择移动主角，朝左边的路前进，在篝火处升级我的狼崽。现在狼崽的生命，从1点变成了3点。”</p>
<p>如果玩家需要在游玩的同时描述游戏内容，我一定会这样进行解说。</p>
<p>有经验的卡牌游戏玩家在看完上面两段描述后，一定会露出会心的微笑：《邪恶冥刻》借鉴了许多游戏，包括”游戏王“的献祭系统、《炉石传说》的攻击与血量、《Artifact》的自动攻击、《杀戮尖塔》的Roguelike卡牌系统。</p>
<p>加入游戏机制是一件简单的事情。但难能可贵的是，《邪恶冥刻》把各个机制结合起来，创造了一套小而精的卡牌游戏规则：说它小，是因为游戏战场小，卡牌数量少，战斗结束得块；说它精，是因为游戏规则有趣且自洽。</p>
<p>小的游戏，做精自然容易。但这样的游戏往往会面临游戏内容不足的情况（比如《陷阵之志》（”into the breach”)，体量小而近乎完美）。作者似乎意识到了这一点，他用一种特别的方式来添加游戏内容：通过大幅度改变游戏的玩法来创造新的关卡。</p>
<h2 id="游戏系统大杂烩"><a href="#游戏系统大杂烩" class="headerlink" title="游戏系统大杂烩"></a>游戏系统大杂烩</h2><p>除了最初的Roguelike卡牌系统外，作者在保持核心玩法是卡牌对战的前提下，还引入数个不同的游戏系统。”像素画面“、”开卡包“、”向上下左右四个方向拓展地图的塞尔达式2D地图“。这些本不可能在同一部游戏中出现的元素，奇迹般地出现在了这同一部游戏里。作者的初衷算是达到了——随着游戏内容的增加，玩家的游戏时间确实得到了延长。</p>
<p>除了一些常见的”对战“、”冒险“游戏机制外，《邪恶冥刻》还把解谜要素（在我的定义里，”解密“指打开隐藏砖块、用钥匙打开门、发现boss弱点这种轻度的需要玩家动脑的游戏机制，”解谜“则指独特、高难度的谜题机制，需要玩家认真思考，这些机制一般是游戏的核心玩法）融入了游戏的所有章节里。在完成卡牌游戏的挑战之余，玩家一定要通过解谜来推动游戏的进度。这些解谜没有冗余之感，是玩家在激烈的卡牌对战过后的完美调剂品。</p>
<p>在一部游戏中塞入数个大的游戏系统，这是一般的游戏不敢做的事情。也只有有经验的独立游戏作者，敢于在自己的作品里炫技。很难得，这些游戏系统融合得不错，过度没有很突兀，值得游戏经验不是很丰富的设计师学习。</p>
<h2 id="成也卡牌，败也卡牌"><a href="#成也卡牌，败也卡牌" class="headerlink" title="成也卡牌，败也卡牌"></a>成也卡牌，败也卡牌</h2><p>卡牌游戏的特点是什么？</p>
<p>从玩家的角度出发，卡牌游戏最大的特点就是趣味性。简单的规则，狭小的战场，就能演绎出一部部精彩的对局。如前文所讲，得益于卡牌游戏本身的特性，《邪恶冥刻》设计了一种简单而趣味性极强的卡牌游戏机制。如何打出强力卡牌，怎么摆放卡牌，怎么样造成有效伤害……卡牌游戏的每一个机制都引入了思考。在一切都确认完毕后，玩家就能双手离开键盘，看着卡牌自动攻击，并触发卡牌效果的联动，最终获得游戏的胜利（或者翻车），体验着思考带来的成就感。（关于卡牌游戏的认真研究将会出现在以后的文章）</p>
<p>但从设计师的角度来看，卡牌游戏的设计难度是极大的。卡牌本身具有的随机性，加上卡牌间难以量化的联动效果，让卡牌游戏的平衡性成为了一个难题。《邪恶冥刻》主要就输在了平衡性这一点上——游戏在后期实在太简单了，战斗胜利丝毫不能给玩家带来乐趣。</p>
<p>越是机制精致、联动性强的卡牌游戏，越能给玩家带来耳目一新之感，也越难维持平衡性。这真是一件矛盾的事情。</p>
<p>这里将先分章节，再从宏观到具体，逐条列出<strong>我还能记住的</strong>游戏平衡性上的具体问题。这些内容是写给玩过游戏的玩家看的。</p>
<h3 id="总体"><a href="#总体" class="headerlink" title="总体"></a>总体</h3><h4 id="游戏机制"><a href="#游戏机制" class="headerlink" title="游戏机制"></a>游戏机制</h4><ol>
<li>血量差为5点的胜利条件，对于先手行动的玩家来说，过于容易触发。反过来，boss太强，玩家就会被秒杀。这是游戏的核心机制，也是一切平衡问题的根源，确实不好改进，不然游戏就完全变了。一些可能的改进意见会在后文提及。</li>
<li>缺乏对手牌臃肿的惩罚。一般情况下，卡牌游戏要求卡组精简，不然抽到关键牌的概率会缩小，卡组整体强度降低。做为一个每回合抽一张卡的游戏，理论上卡组臃肿带来的影响是很大的。但因为第1条胜利和失败都过于简单，为了让玩家在前几回合不被秒杀，作者在设计时让对手的卡组进攻性不至于太强，其结果玩家卡手几回合都没什么关系。这无论在单机卡牌游戏还是多人卡牌游戏中都是不允许的。</li>
<li>缺乏抽牌上的联动。这一条导致了第2条问题，玩家容易卡手，对手强度不敢太强，玩家卡手反而不被惩罚。</li>
<li>对手下回合策略已知。这条由《杀戮尖塔》发扬的机制是回合制游戏的一项历史性突破，我非常喜欢它，且赞同这条机制出现在卡牌游戏里。但无疑这条机制令游戏的难度又降了一层。</li>
<li>游戏中，有”站场“这个概念，即上回合生存的生物，下回合还能进行攻击。这是一条滚雪球的机制，有场面的玩家会一直扩大优势。而对手对于场面的控制极差。这导致玩家控制场面后很容易获得胜利。</li>
</ol>
<h4 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h4><ol>
<li>部分卡牌效果过强。如”检索牌库中的一张牌“。</li>
</ol>
<h3 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h3><h4 id="游戏机制-1"><a href="#游戏机制-1" class="headerlink" title="游戏机制"></a>游戏机制</h4><ol>
<li>”检索牌库中的一张牌“与无限资源之间的组合过于BT。比如令所有松鼠牌具有”检索“效果，这等于让玩家自由地控制手牌，所有战斗都失去了难度。作者保留这些强大的组合，可能是想让玩家觉得游戏很有趣。但是，还是那句话，卡牌游戏是以难度恰到好处的战斗为核心。一旦玩家无敌了，后面的战斗都是垃圾时间，有什么趣味可言呢？</li>
<li>强行打脸牌过弱（飞行牌），这导致防守牌（防空牌）也没有价值了。可能作者想设计出面对对面的打脸牌，玩家可以用防空牌来防御，也可以用打脸牌还击这样的充满抉择的游戏体验。但是，战场就那么点大，血就那么多，战斗一下就一边倒了，这种抉择根本体现不出来。如果战场更大，血量更多，维持血量差的博弈才会更加明显。</li>
<li>高费牌都难以打出，尤其是骨头牌。</li>
<li>种族机制联动不足，一旦联动了又过于强大。</li>
</ol>
<h4 id="细节-1"><a href="#细节-1" class="headerlink" title="细节"></a>细节</h4><ol>
<li>剧毒秒最终boss很有趣，但平衡性因此更加差劲了。</li>
<li>第三关boss难度实在太低了，这是很明显的设计水平有问题。第二阶段，玩家至少可以拿走对方一张牌，而且玩家场面上是有牌的，至少有一张牌可以直接攻击到对面。玩家轻松就可以在第二阶段一开始秒杀boss。</li>
<li>几大终极奖励的强度明显不一样。”每回合抽两张牌“、”抽牌变成检索“无疑是超强的能力，而”战斗开始获得8个骨头“的强度太低了。</li>
</ol>
<h3 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h3><h4 id="游戏机制-2"><a href="#游戏机制-2" class="headerlink" title="游戏机制"></a>游戏机制</h4><ol>
<li>献祭机制配合几乎无消耗的骨头机制过强。高费献祭牌太容易被召唤了。</li>
<li>高费骨头牌太难召唤了。</li>
<li>宝石机制（万智牌机制）的强度过低。</li>
<li>我现在根本记不住boss的卡组了。boss的卡组实在太弱了，且后面的boss不比前面厉害多少，我前期随便组的一套卡组就能打过所有的游戏内容。我虽然不能说出boss卡组设计上具体的问题，但很明显boss的卡组强度过低。</li>
</ol>
<h4 id="细节-2"><a href="#细节-2" class="headerlink" title="细节"></a>细节</h4><ol>
<li>游戏的一场战斗是可以无限次进行的，这导致玩家可以获得无限多的金钱，进而获得无限多的卡牌。”能无限获得的资源，必然要有一个能无限输出的途径“，这是我在小学的时候就领悟的游戏设计原则。在这样一个卡牌游戏里，能无限获取卡牌就是不合理的。玩家可以刷无限的牌，让游戏的攒卡机制报废。虽然这游戏的平衡实在太烂，战斗太简单，玩家根本不需要去刷钱就能获取胜利。</li>
<li>对应游戏机制的第1条，有张无消耗的牌，效果是能用一根骨头换一个骷髅。这张牌能瞬间产生大量的祭品，让高费献祭牌登场。这牌强度太高了。</li>
</ol>
<h3 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h3><h3 id="游戏机制-3"><a href="#游戏机制-3" class="headerlink" title="游戏机制"></a>游戏机制</h3><ol>
<li>游戏流程过长，后期各种卡牌效果联动、堆叠，整套卡组的强度已经爆表了，多强的boss都打不过玩家。</li>
<li>自定义卡牌很有趣，但这导致了第1条的效果堆叠问题。剧毒、顺劈、回手，这样一张无限资源的牌一旦摸到，游戏直接胜利。</li>
<li>电路通路的太难触发了，而且到了游戏后期，其他牌已经过强了，这个机制变成了鸡肋。</li>
</ol>
<h4 id="细节-3"><a href="#细节-3" class="headerlink" title="细节"></a>细节</h4><ol>
<li>有一个给卡牌加1攻，但卡牌死亡后永久被移除卡组的效果。这个效果的设计水平很低。这个效果的本意是加强一张卡，但要冒风险。但是，加1攻并不是多大的提升，反倒是移除一张卡能够提升整套卡组的质量。设计的本意很难达成。</li>
<li>有一张低费卡，能够对初次出现的敌人造成1点伤害。这张牌效果的强度太高了，比不少高费牌还强，直接让防守变得特别简单。</li>
</ol>
<h3 id="提升平衡性的建议"><a href="#提升平衡性的建议" class="headerlink" title="提升平衡性的建议"></a>提升平衡性的建议</h3><p>游戏小而精的机制，令游戏平衡性很难提升。但在对卡牌游戏设计有诸多思考的我看来，在核心机制不变的前提下，游戏的平衡性仍然有很多提升的余地：</p>
<h4 id="非对称性"><a href="#非对称性" class="headerlink" title="非对称性"></a>非对称性</h4><p>关于PVE卡牌的对称性上的思考，是我在比较《炉石传说》冒险模式和《杀戮尖塔》之后进行的。《炉石传说》本来是PVP游戏，凭借着玩家间的博弈，游戏的平衡性天然能够得到维持。所有卡牌都是为聪明的人类玩家设计的。但在炉石的PVE中，AI的水平很差，游戏的平衡性直接就崩溃了。由于游戏的胜负极大取决于对战双方的水平，哪怕给AI塞很多好卡来强行提升PVE对战的平衡性，玩家还是很难从PVE中获取足够的乐趣。反观《杀戮尖塔》，整套游戏就是基于非对称战斗的。只有玩家在使用卡牌，怪物只会按照简单的规则放技能。玩家不和对手博弈，只需要最大化自己卡组的能力即可。非对称的战斗，绝对更加适合PVE卡牌游戏。</p>
<p>坚持使用对称卡牌机制是造成新游戏规则无法拓展、难做平衡的最重要的原因（所以说在玩游戏之前，我就很在意这是不是一款对称性卡牌游戏）。在《邪恶冥刻》中，游戏机制可以向非对称的方向修改。玩家的操作可以一直用卡牌来表示，但是敌人的表达形式可以多种多样。事实上，游戏中就有一些非对称的战斗：战斗的胜利不再是使血量差大于5，而是杀死特定的敌人。朝这个方向做就对了呀！</p>
<h4 id="血量与场面"><a href="#血量与场面" class="headerlink" title="血量与场面"></a>血量与场面</h4><p>自始至终，游戏的胜利条件都是血量差大于等于5点。“血量差”这个概念令游戏的攻守之间的抉择非常有趣。但是，很明显，5点的血量差非常容易产生，boss战可能顷刻之间就结束了。</p>
<p>提升胜利所需血量差是一个眼见的提升平衡性的手段。但如果无脑提升血量，并不能改善游戏的平衡性。游戏的胜利条件表面上是血量占优，但实际上只要控制住场面，再大的血量差都能产生。也就是说，游戏真正的胜利条件，是取得场面的胜利。</p>
<p>在传统有卡牌战场的游戏里，血量这一机制的设定，其实是给游戏新增了一个胜利条件：我可以靠控制住场面，慢慢打死你；也可以通过快速造成伤害，在你的怪物还没召完前把你“抢血”抢死。这对应了传统卡牌游戏的控制卡组和快攻卡组。在《邪恶冥刻》中，设计师可能是为了让游戏多一种胜利条件，让快攻卡组可行，才令血量差这么小。</p>
<p>可惜，游戏的胜利条件设置得太糟糕了。作者也注意到5点伤害太容易打出，所以给boss加了很多“嘲讽”（令对方单位无法直接对我方角色造成伤害）怪物。这反过来使玩家的快攻卡组几乎不可行了。血量差5点的设定，讽刺般的起到了反效果，游戏只剩下了控制场面这一种赢法。</p>
<p>血量差5点是一个核心机制，几乎所有卡牌都围绕这一点做平衡，要改起来恐怕很难。但如果让我来对游戏修改的话，我会在游戏初期保持5点血量差的胜利条件，前期的弱卡以此为平衡标准进行设计。随着游戏进行，血量差的要求会逐渐放大，后续卡牌强度也逐渐增加，以新的血量差为平衡标准。</p>
<p>但我不能保证这种修改是有效的，实际的平衡性还需要在测试中确定。可以想到，由于胜利条件是血量差而不是血量，玩家可以通过不断进攻来代替防守，使得防守变成了一个很没有意义的事情。想把这个机制下的平衡做好还是太难了。游戏做得小而精，就不得不面对这种平衡性上的重大挑战，这种游戏还是太难设计了。</p>
<h4 id="技能卡"><a href="#技能卡" class="headerlink" title="技能卡"></a>技能卡</h4><p>游戏设置了许多新颖的机制，但令人意外的是技能卡（相对于怪物卡的概念，能立刻产生效果，用完立刻丢弃）这种基础机制竟然没有出现在游戏里。大概能够猜到，引入技能卡的话，游戏的平衡性将会更难维持。而且电脑的AI很不好写，想产生势均力敌的对称性战斗更难了。</p>
<p>如果让我做的话，无论会引入多少新的困难，我都会加入技能卡的设定。技能卡是对游戏机制做加法，而且是对游戏内容维度上的提升，可以极大扩充游戏的设计空间。也就是说，设计师有更多的空间去设置卡牌的能力，更容易设计出不一样的卡来。只要把游戏最基础的平衡性搭好，后续添加新卡、维护新卡的平衡性都会简单很多。</p>
<p>但估计还是出于维护那小而精的游戏机制，设计师没有向游戏里添加技能卡的设定。反正我是认为加入技能卡能大幅提升游戏的可玩性、平衡性。</p>
<h4 id="手牌资源"><a href="#手牌资源" class="headerlink" title="手牌资源"></a>手牌资源</h4><p>卡牌游戏一大核心、一大可玩之处，就是抽牌系统。玩家牌库中哪怕有再强的牌，手牌数量不够，或者因为卡组臃肿抽不到想要的牌，都会导致当前的战斗陷入窘境。抽牌系统是卡牌游戏必须要做好的一点。</p>
<p>理论上，卡牌间不仅要有场面上的交互机制，还应该有手牌上的交互机制：比如什么有某一种族就抽一张牌，抽一张同名牌等。玩家需要动脑去最大化抽牌效果。《邪恶冥刻》的手牌资源控制得极为糟糕：要么是无脑强而无聊的手牌资源机制，什么消耗一根骨头获得一张白板卡，什么亡语把卡牌回手，什么去牌库中检索一张牌；要么这张卡就和抽牌一点关系都没有。连”抽一张牌“这种再基础不过的效果都没有。设计师根本没有花心思在抽牌系统上，根本没有去想怎么样构建一套好玩的抽牌体系。</p>
<p>没有考虑手牌资源，导致玩家的牌库很容易臃肿；牌库臃肿，玩家就容易卡手；玩家卡手，如果boss强度过大，玩家一两个回合就撑不住了。设计师很明显发现了这个问题，所以根本不敢给boss太强的卡组，以照顾那些很容易卡手的卡组。这让游戏的平衡性崩溃得一塌糊涂，让卡牌游戏的一大玩法——构筑精简的卡组彻底消失。</p>
<p>让我来做的话，一定会考虑加入更多与手牌资源有关的机制，且和现有的种族、场面等机制结合起来。但同理，加入这些设定会让游戏发生翻天覆地的变化，很多东西都要重新设计。</p>
<h3 id="平衡性总结"><a href="#平衡性总结" class="headerlink" title="平衡性总结"></a>平衡性总结</h3><p>我玩完游戏已经一个多月了，但我随手一列，还是能从机制和细节上列出这么多平衡性问题。可以看出作者对于卡牌游戏平衡性的把握是如此糟糕。这要放到一个PVP卡牌游戏中，设计师早就被玩家喷死了。</p>
<p>成也卡牌，败也卡牌。基于有趣的卡牌游戏框架，设计师设计了一个简单的卡牌游戏系统，这个卡牌游戏玩个几盘感觉还不错。但是，设计师必须为这样一个简单的系统付出代价：这样的系统的平衡性非常难做，很难往里面添加有趣的新卡。糟糕的平衡性，会极大拖累游戏的可玩性。这样一个卡牌游戏系统，撑死了就只能支持10来个小时的游戏时间。等玩家稍微熟悉整套系统后，这个卡牌游戏就一点也不好玩了。</p>
<h2 id="丰富性与平衡性的讨论"><a href="#丰富性与平衡性的讨论" class="headerlink" title="丰富性与平衡性的讨论"></a>丰富性与平衡性的讨论</h2><p>这一节，我就不理性地进行分析，而是发表下我个人的看法了。</p>
<p>我喷了这么多平衡上的问题，也夸过游戏在丰富性的创新。综合而言，我对设计师的这种设计思想极度不满，对这部作品感到十分惋惜。</p>
<p>你说你做一个小体量的卡牌游戏，游戏机制差不多自洽。哪怕内容不多，几小时就玩完了，玩家体验尚可。我会认为这游戏非常有新意，做得很值得学习。</p>
<p>你说你做了一个小体量、平衡性极差，又臭又长的游戏，我会说这是个垃圾游戏，不用去玩。</p>
<p>问题是《邪恶冥刻》在初期以一个新颖的卡牌游戏系统吸引了玩家的眼球，后来又用这个差劲的系统浪费了玩家很多时间，同时炫技般地不断加入丰富多彩的游戏系统。不管是大一点的构筑/roguelike/有限资源这样的游戏资源系统，还是爬塔式/塞尔达式/自由式的地图系统，还是小一点的自制卡牌、读取玩家电脑信息、修改卡牌、临时添加战斗机制等一系列在现有游戏机制上的装饰，都做得十分丰富。设计师真的是设计水平很高。这要换一个游戏框架，比如换一个2D平台游戏，游戏一定会非常好玩。</p>
<p>但可惜，这就是一个卡牌游戏。卡牌游戏的趣味的核心，是恰到好处的难度。这种靠平衡性吃饭的游戏，就是考验设计师的硬实力，怎么样在设计好的游戏框架下，添加合理的游戏内容。加再多花里胡哨的装饰品，试图在游戏框架上扩充，而不去认真把框架内的东西填充好，是无法提升游戏的平衡性，无法做出一个足够好的游戏的。这就好比你写作文，内容乱七八糟，你说“我字写得很好看”；你写程序，逻辑混乱不堪，你说“我变量名取得好，注释写得清楚”；谈个恋爱，你长得不帅，没有钱，不会说话，你说“我程序写得很好”。这有用吗？最核心的评价指标达不到，其他的东西做得再好，有什么用呢？</p>
<p>我不会推荐，甚至会大力阻止别人去玩这款游戏。不是因为我不喜欢这款游戏。我从这个游戏里学到了很多东西：无论是好的设计思路，还是需要规避的缺点。我也十分认可设计师本身的水平。但是我太喜欢这款游戏了，以至于我会不断想象这款游戏如果是我做出来的会怎么样。如果是我做出了这样的游戏，我会非常难过：明明很有水平，也很有想法，却做得这么不好玩。这比做出了一款纯粹的烂游戏更加令人心疼。我这种矛盾的心理，令我对《邪恶冥刻》给出了极低的评价。这事关一个艺术家、一个设计师的尊严：费尽心思却做得有失水准的东西，宁可扔掉，也不该拿出来展示。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">A foresighted strategist with big-picture thinking. 大局观选手。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">62</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
