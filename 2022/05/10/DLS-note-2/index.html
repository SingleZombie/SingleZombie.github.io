<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="课堂笔记这堂课要学习的是逻辑回归——一种求解二分类任务的算法。同时，这堂课会补充实现逻辑回归必备的数学知识、编程知识。学完这堂课后，同学们应该能够用Python实现一个简单的小猫判定器。 本节课的目标在这节课里，我们要完成一个二分类任务。所谓二分类任务，就是给一个问题，然后给出一个“是”或“否”的回答。比如给出一张照片，问照片里是否有一只猫。 这节课中，我们用到的方法是逻辑回归。逻辑回归可以看成是">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达《深度学习专项》笔记+代码实战（二）：简单的神经网络——逻辑回归">
<meta property="og:url" content="https://zhouyifan.net/2022/05/10/DLS-note-2/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="课堂笔记这堂课要学习的是逻辑回归——一种求解二分类任务的算法。同时，这堂课会补充实现逻辑回归必备的数学知识、编程知识。学完这堂课后，同学们应该能够用Python实现一个简单的小猫判定器。 本节课的目标在这节课里，我们要完成一个二分类任务。所谓二分类任务，就是给一个问题，然后给出一个“是”或“否”的回答。比如给出一张照片，问照片里是否有一只猫。 这节课中，我们用到的方法是逻辑回归。逻辑回归可以看成是">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhouyifan.net/2022/05/10/DLS-note-2/0.png">
<meta property="og:image" content="https://zhouyifan.net/2022/05/10/DLS-note-2/1.png">
<meta property="og:image" content="https://zhouyifan.net/2022/05/10/DLS-note-2/2.png">
<meta property="og:image" content="https://zhouyifan.net/2022/05/10/DLS-note-2/3.png">
<meta property="og:image" content="https://zhouyifan.net/2022/05/10/DLS-note-2/4.png">
<meta property="article:published_time" content="2022-05-10T09:16:34.000Z">
<meta property="article:modified_time" content="2022-05-10T10:16:26.129Z">
<meta property="article:author" content="Zhou Yifan">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhouyifan.net/2022/05/10/DLS-note-2/0.png">

<link rel="canonical" href="https://zhouyifan.net/2022/05/10/DLS-note-2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>吴恩达《深度学习专项》笔记+代码实战（二）：简单的神经网络——逻辑回归 | 周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/05/10/DLS-note-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          吴恩达《深度学习专项》笔记+代码实战（二）：简单的神经网络——逻辑回归
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-10 17:16:34" itemprop="dateCreated datePublished" datetime="2022-05-10T17:16:34+08:00">2022-05-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="课堂笔记"><a href="#课堂笔记" class="headerlink" title="课堂笔记"></a>课堂笔记</h1><p>这堂课要学习的是逻辑回归——一种求解二分类任务的算法。同时，这堂课会补充实现逻辑回归必备的数学知识、编程知识。学完这堂课后，同学们应该能够用Python实现一个简单的小猫判定器。</p>
<h2 id="本节课的目标"><a href="#本节课的目标" class="headerlink" title="本节课的目标"></a>本节课的目标</h2><p>在这节课里，我们要完成一个二分类任务。所谓二分类任务，就是给一个问题，然后给出一个“是”或“否”的回答。比如给出一张照片，问照片里是否有一只猫。</p>
<p>这节课中，我们用到的方法是逻辑回归。逻辑回归可以看成是一个非常简单的神经网络。</p>
<h3 id="符号标记"><a href="#符号标记" class="headerlink" title="符号标记"></a>符号标记</h3><p>从这节课开始，我们会用到一套统一的符号标记：</p>
<p>$(x, y)$ 是一个训练样本。其中，$x$ 是一个长度为 $n_x$ 的一维向量，即 $x \in \mathcal{R}^{n_x}$。$y$ 是一个实数，取0或1，即$y \in {0, 1}$。取0表示问题的的答案为“否”，取1表示问题的答案为“是”。</p>
<blockquote>
<p>这套课默认读者对统计机器学习有基本的认识，似乎没有过多介绍训练集是什么。在有监督统计机器学习中，会给出<strong>训练数据</strong>。训练数据中的每一条<strong>训练样本</strong>包含一个“问题”和“问题的答案”。神经网络根据问题给出一个自己的解答，再和正确的答案对比，通过这样一个“学习”的过程来优化解答能力。</p>
<p>对计算机知识有所了解的人会知道，在计算机中，颜色主要是通过RGB（红绿蓝）三种颜色通道表示。每一种通道一般用长度8位的整数表示，即用一个0~255的数表示某颜色在红、绿、蓝上的深浅程度。这样，一个颜色就可以用一个长度为3的向量表示。一幅图像，其实就是许多颜色的集合，即许多长度为3的向量的集合。算上某颜色所在像素的位置$(x, y)$，图像就可以看成一个3维张量$I \in \mathcal{R}^{H \times W \times 3}$，其中$H$是图像高度，$W$是图像宽度，$3$是图像的通道数。在把图像输入进逻辑回归时，我们会把图像“拉直”成一个一维向量，就是前面提到的$x$，其中$x$的长度$n_x$满足$n_x = H \times W \times 3$。这里的“拉直”操作就是把张量里的数据按照顺序一个一个填入新的一维向量中。</p>
<p>其实向量就是一维的，但我还是很喜欢强调它是“一维”的。这是因为在计算机中所有数据都可以看成是数组（甚至C++的数组就叫<code>vector</code>)。二维数组不过是一维数组的数组，三位数组不过是二维数组的数组。在数学中，为了方便称呼，把一维数组叫“向量”，二维数组叫“矩阵”，三维及以上数组叫“张量”。其实在我看来它们之间只是一个维度的差别而已，叫“三维向量”、“一维张量”这种不是那么严谨的称呼也没什么问题。</p>
</blockquote>
<p>实际上，我们有很多个训练样本。第$i$个训练样本叫做$(x^{(i)}, y^{(i)})$。在后面使用其他标记时，也会使用上标$(i)$表示第$i$个训练样本得到的计算结果。样本总数记为$m$。</p>
<p>所有输入数据的集合构成一个矩阵（其中每个输入样本用<strong>列向量</strong>的形式表示，这是为了方便计算机的计算）：</p>
<script type="math/tex; mode=display">
X=\left[
  \begin{matrix}
  | & | & & | \\
  x^{(1)} & x^{(2)} & ... & x^{(m)} \\
  | & | & & |
  \end{matrix}
\right]
,X \in \mathcal{R}^{n_x \times m}</script><p>同理，所有真值也构成集合 $Y$:</p>
<script type="math/tex; mode=display">
Y=\left[
  \begin{matrix}
  y^{(1)} & y^{(2)} & ... & y^{(m)} 
  \end{matrix}
\right]
,Y \in \mathcal{R}^{m}</script><p>由于每个样本$y^{(i)}$是一个实数，所以集合$Y$是一个向量。</p>
<h2 id="逻辑回归的公式描述"><a href="#逻辑回归的公式描述" class="headerlink" title="逻辑回归的公式描述"></a>逻辑回归的公式描述</h2><p>逻辑回归是一个学习算法，用于对真值只有0或1的“逻辑”问题进行建模。给定输入$x$,逻辑回归输出一个$\hat{y}$。这个$\hat{y}$是对真值$y$的一个估计，准确来说，它描述的是$y=1$的概率，即$\hat{y}=P(y=1 \ | \ x)$</p>
<p>逻辑回归会使用一个带参数的函数计算$\hat{y}$。这里的参数包括$w \in \mathcal{R}^{n_x}, b \in \mathcal{R}$。</p>
<p>说起用于拟合的函数，最容易想到的是线性函数$w^Tx+b$（即做点乘再加$b$： $w^Tx+b = (\Sigma_{i=1}^{n_x}w_ix_i)+b $）。但线性函数的值域是$(- \infty,+\infty) $（即全体实数$\mathcal{R}$），概率的取值是$[0, 1]$，我们需要一个定义域为$\mathcal{R}$，值域为$[0, 1]$，把线性函数映射到$[0, 1]$上的一个函数。逻辑回归中用的是sigmoid函数$\sigma$,它的定义为</p>
<script type="math/tex; mode=display">\sigma(z)=\frac{1}{1 + e^{-z}}</script><p>这个函数可以有效地完成映射，它的函数图像长这个样子：</p>
<p><img src="/2022/05/10/DLS-note-2/0.png" alt></p>
<blockquote>
<p>这里不用计较为什么使用这个函数，只需要知道这个函数的趋势：$x$越小，$\sigma (x)$越靠近0；$x$越大，$\sigma (x)$越靠近1。</p>
</blockquote>
<p>也就是说，最终的逻辑回归公式长这个样子：$\hat{y} = \sigma(w^Tx+b)$。</p>
<h2 id="逻辑回归的损失函数（Cost-Function）"><a href="#逻辑回归的损失函数（Cost-Function）" class="headerlink" title="逻辑回归的损失函数（Cost Function）"></a>逻辑回归的损失函数（Cost Function）</h2><p>所有的机器学习问题本质上是一个优化问题，一般我们会定义一个<strong>损失函数（Cost Function）</strong>，再通过优化参数来最小化这个损失函数。</p>
<p>回顾一下我们的任务目标：我们定义了逻辑回归公式$\hat{y} = \sigma(w^Tx+b)$，我们希望$\hat{y}$尽可能和$y$相近。这里的“相近”，就是我们的优化目标。</p>
<p>逻辑回归中，定义了每个输出和真值的<strong>误差函数（Loss Function）</strong>，这个误差函数叫<strong>交叉熵</strong></p>
<script type="math/tex; mode=display">L(\hat{y}, y)=-(y \ log\hat{y} + (1-y) \ log(1-\hat{y}))</script><p>不使用另一种常见的误差函数均方误差的原因是，交叉熵较均方误差梯度更加平滑，更容易再之后的优化中找到全局最优解。</p>
<p>误差函数是定义在每个样本上的，而损失函数是定义在整个样本上的，表示所有样本误差的“总和”。这个“总和”其实就是平均值，即损失函数$J(w, b)$为:</p>
<script type="math/tex; mode=display">J(w, b)=\frac{1}{m}\Sigma_{i=1}^{m}-(y^{(i)} \ log\hat{y}^{(i)} + (1-y^{(i)}) \ log(1-\hat{y}^{(i)}))</script><h2 id="优化算法——梯度下降"><a href="#优化算法——梯度下降" class="headerlink" title="优化算法——梯度下降"></a>优化算法——梯度下降</h2><p>有了优化目标，接下来的问题就是如何用优化算法求出最优值。这里使用的是<strong>梯度下降（Gradient Descent）</strong> 法。梯度下降的思想很符合直觉：如果要让函数值更小，就应该让函数的输入沿着函数值下降最快的方向（梯度的方向）移动。</p>
<p>以课件中的一元函数为例：</p>
<p><img src="/2022/05/10/DLS-note-2/1.png" alt></p>
<p>一元函数的梯度值就是导数值，方向只有正和负两个方向。如果是参数最开始在A点，那么通过求导可以知道，往右走函数值才会变少；反之，对于B点，则应该往左移动。</p>
<p>每个点都应该向最小值“一小步一小步”地移动，直至抵达最低点。为什么要“一小步”移动呢？可以想象，如果一次移动的“步伐”过大，改变参数不仅不会让优化函数变小，甚至会让待优化函数变大。比如从A点开始，同样是往右移动，如果“步伐”过大，A点就会迈过最低点的红点，甚至跑到B点的上面。那么这样下去，待优化函数会越来越大，优化就失败了。</p>
<p>为了让优化能顺利进行，梯度下降法使用<strong>学习率（Learning Rate)</strong> 来控制参数优化的“步伐”，即用如下方法更新损失函数$J(w)$参数：</p>
<script type="math/tex; mode=display">
Repeat: \\
w \gets w - \alpha \frac{dJ}{dw}</script><p>这里的 $\alpha$ 就是学习率，它控制了每次梯度更新的幅度。</p>
<p>其实这里还有两个问题：参数$w$该如何初始化；该执行梯度下降多少次。在这个问题中初始化对结果影响不大，可以简单地令$w=0$。而优化的次数没有硬性的需求，先执行若干次，根据误差是否收敛再决定是否继续优化即可。</p>
<h2 id="前置知识补充"><a href="#前置知识补充" class="headerlink" title="前置知识补充"></a>前置知识补充</h2><p>到这里，逻辑回归的知识已经讲完了。让我们梳理一下：</p>
<p>在逻辑回归问题中，我们有输入样本集$X$和其对应的期望输出$Y$，我们希望找到拟合函数$\hat{Y}=W^TX+b$，使得$\hat{Y}$和$Y$尽可能接近，即让损失函数$J(W, b)=mean(-(Ylog\hat{Y}+(1-Y)log(1-\hat{Y})))$尽可能小。</p>
<p>我们可以用$0$来初始化所有待优化参数$W, b$，并执行梯度下降</p>
<script type="math/tex; mode=display">
\begin{align*}
W & \gets W - \alpha \frac{dJ}{dW} \\ 
b & \gets b - \alpha \frac{dJ}{db}
\end{align*}</script><p>若干次后得到一个较优的拟合函数。</p>
<p>为了让大家成功用代码实现逻辑回归，这门课贴心地给大家补充了数学知识和编程知识。</p>
<blockquote>
<p>在我的笔记中，补充前置知识的记录会潦草一些。</p>
</blockquote>
<h3 id="求导"><a href="#求导" class="headerlink" title="求导"></a>求导</h3><blockquote>
<p>这部分对中国学生来说十分简单，因为求导公式是高中教材的内容。</p>
</blockquote>
<p>导数即函数每时每刻的变化率，比如位移对时间的导数就是速度。比如，对于直线$y=kx$，函数的变化率时时刻刻都是$k$。对于二次函数$y=x^2$，$x$处的导数是$2x$。</p>
<h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p>其实，所有复杂的数学运算都可以拆成计算图表示法。</p>
<p><img src="/2022/05/10/DLS-note-2/2.png" alt></p>
<blockquote>
<p>计算图中的”图”其实是一个计算机概念，表示由节点和边组成的集合。不熟悉的话，当成日常用语里的图来理解也无妨。</p>
</blockquote>
<p>比如上图中，哪怕是简单的运算$2a+b$，也可以拆成两步：先算$2 \times a$，再算$(2a) + b$。</p>
<p>这里的“步”指原子运算，即最简单的运算。原子运算可以是加减乘除，也可以是求指数、求对数。复杂的运算，只是对简单运算的组合、嵌套。</p>
<p>明明简简单单可以用一行公式表示的事，要费很大的功夫画一张计算图呢？这是因为，对函数求导满足“链式法则”，借助计算图，可以更方便地用链式法则算出所有参数的导数。比如在上图中求$f$对$a$的导数，使用链式法则的话，可以通过先求$f$对$c$的导数，再求$c$对$a$的导数得到。</p>
<h3 id="利用计算图对逻辑回归求导"><a href="#利用计算图对逻辑回归求导" class="headerlink" title="利用计算图对逻辑回归求导"></a>利用计算图对逻辑回归求导</h3><p>逻辑回归有运算图：<br><img src="/2022/05/10/DLS-note-2/3.png" alt></p>
<p>现在利用链式法则从右向左求导：</p>
<script type="math/tex; mode=display">
\begin{align*}
z & =  w_1x_1 + w_2x_2 +b \\
a & =  \frac{1}{1+e^{-z}} \\
L & = -(yloga+(1-y)log(1-a)) \\

\frac{dL}{da} & =  -(\frac{y}{a}-\frac{1-y}{1-a}) \\
\frac{da}{dz} & =  \frac{e^{-z}}{(1+e^{-z})^2} = a(1-a)\\
\frac{dL}{dz} & = \frac{dL}{da} \frac{da}{dz} \\
&= -(\frac{y}{a}-\frac{1-y}{1-a}) \times a(1-a) \\
&= -(y(1-a)-(1-y)a) \\
&= -(y-ya-a+ya) \\
&= a-y \\
\frac{dL}{dw_i} &= \frac{dL}{dz}\frac{dz}{dw_i}=(a-y)x_i \\
\frac{dL}{db} &= \frac{dL}{dz}\frac{dz}{db}=(a-y)
\end{align*}</script><blockquote>
<p>这些运算里最难“注意到”的是$\frac{e^{-z}}{(1+e^{-z})^2} = a(1-a)$。</p>
<p>在学计算机科学的知识时，可以适当忽略一些数学证明，把算好的公式直接拿来用，比如这里的$\frac{dL}{dz}=a-y$。</p>
</blockquote>
<p>$\frac{dL}{dw_i}, \frac{dL}{db}$就是我们要的梯度了，用它们去更新原来的参数即可。值得一提的是，这里的梯度是对一个样本而言。对于全部$m$个样本来说，本轮的梯度应该是所有样本的梯度的平均值。实际实现时，对某个样本的梯度除以$m$即可。</p>
<h3 id="Python-向量化计算"><a href="#Python-向量化计算" class="headerlink" title="Python 向量化计算"></a>Python 向量化计算</h3><p>在刚刚的一轮迭代中，我们要用到两次循环：</p>
<ol>
<li>对$m$个样本循环处理</li>
<li>对$n_x$个权重$w_i$与对应的$x_i$相乘</li>
</ol>
<p>直接拿 Python 写 for 循环是很慢的，这里最好使用向量化计算。、</p>
<blockquote>
<p>课程中提到向量化的好处是可以用<strong>SIMD</strong>,即单指令多数据流，这个概念理解成计算机可以同时对16个或32个数做计算。如果输入的数据是向量的话，相比一个一个做for循环，这样计算的速度会更快。但实际上，除了无法使用SIMD以外，Python的低效也是拖慢速度的原因之一。哪怕是不用SIMD，单纯地用C++的for循环实现向量化计算，都能比用Python的循环快上很多。</p>
</blockquote>
<p>Python 的 numpy 库提供了向量化计算的接口。比如以下是向量化的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.zeros((<span class="number">10</span>)) <span class="comment"># 新建长度为10的向量</span></span><br><span class="line">b = np.ones((<span class="number">10</span>)) <span class="comment"># 新建长度为10的向量</span></span><br><span class="line">a = a + b <span class="comment"># 10个数同时做加法</span></span><br><span class="line">a = np.exp(a) <span class="comment"># 对10个数都做指数运算</span></span><br></pre></td></tr></table></figure>
<p>numpy 允许一种叫做“广播”的操作，这种操作能够完成不同维度数据间的运算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.ones(<span class="number">10</span>) <span class="comment"># a是长度为10，内容全是1的向量</span></span><br><span class="line">k = np.array([<span class="number">3</span>]) <span class="comment"># k是长度为1，内容是3的向量</span></span><br><span class="line">a = k * a <span class="comment"># 广播</span></span><br></pre></td></tr></table></figure>
<p>这里k的shape为<code>[1]</code>，a的shape为<code>[10]</code>。用k乘a，实际上就是令<code>a[i] = k[0] * a[i]</code>。也就是说，<code>k[0]</code>“广播”到了<code>a</code>的每一个元素上。</p>
<p>有一种快速理解广播的方法：可以认为k的形状从<code>[1]</code>变成了<code>[10]</code>，再让k和a逐个元素做乘法。</p>
<p>同理，如果用一个<code>a[x, y]</code>的矩阵加一个<code>b[x, 1]</code>的矩阵，实际上是做了下面的运算：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x):</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(y):</span><br><span class="line">    a[i, j] = a[i, j] + b[i, <span class="number">0</span>] </span><br></pre></td></tr></table></figure></p>
<p>用刚刚介绍的方法来理解，可以认为<code>b</code>从<code>[x, 1]</code>扩充成了<code>[x, y]</code>，再和<code>a</code>做逐个元素的加法运算。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这堂课的主要知识点有：</p>
<ul>
<li>什么是二分类问题。</li>
<li>如何对建立逻辑回归模型（拟合函数）。<ul>
<li>Sigmoid 函数 $\sigma(z)=\frac{1}{1 + e^{-z}}$</li>
</ul>
</li>
<li>误差函数与损失函数<ul>
<li>逻辑回归的误差函数：$L(\hat{y}, y)=-(y \ log\hat{y} + (1-y) \ log(1-\hat{y}))$</li>
</ul>
</li>
<li>用梯度下降算法优化损失函数</li>
<li>计算图的概念及如何利用计算图算梯度</li>
</ul>
<p>学完这堂课后，应该能完成的编程任务有：</p>
<ul>
<li>用numpy做向量化计算</li>
<li>实现逻辑回归<ul>
<li>对输入数据做reshape的预处理</li>
<li>用向量化计算算$\hat{y}$及参数的梯度</li>
<li>迭代优化损失函数</li>
</ul>
</li>
</ul>
<h1 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h1><p>这节课有两个编程作业:第一个作业要求使用numpy实现对张量的一些操作，第二个作业要求用逻辑回归实现了一个分类器。这些编程作业是在python的notebook上编写的。notebook按照“文字——代码——文字……”这样的结构组织，编程体验极差。作为编程最强王者，怎能受此“嗟来之‘码’”的屈辱？我决定从零开始，自己收集数据，并用numpy实现逻辑回归。</p>
<blockquote>
<p>其实我不分享作业代码的真正原因是：Coursera不允许公开展示作业代码。在之后的笔记中，我也会分享如何用自己的代码实现每堂课的编程目标。</p>
</blockquote>
<p>这篇笔记用到的代码已在GitHub上开源：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/LogisticRegression。">https://github.com/SingleZombie/DL-Demos/tree/master/LogisticRegression。</a></p>
<h2 id="程序设计"><a href="#程序设计" class="headerlink" title="程序设计"></a>程序设计</h2><p>不管写什么程序，都要先想好整体的架构，再开始动手写代码。</p>
<p>对于深度学习的项目，要设计架构十分简单。一般一个深度学习程序由以下几部分组成：</p>
<ul>
<li>数据预处理</li>
<li>定义网络结构</li>
<li>定义损失函数</li>
<li>定义优化策略</li>
<li>用训练pipeline串联起网络、损失函数、优化策略</li>
<li>测试模型精度</li>
</ul>
<p>当然，实现深度学习项目比一般的项目多一个步骤：除了写代码外，完成深度学习项目还需要收集数据。</p>
<p>接下来，我将按照<strong>数据收集</strong>、<strong>数据处理</strong>、<strong>网络结构</strong>、<strong>损失函数</strong>、<strong>训练</strong>、<strong>测试</strong>这几部分介绍这个项目。之后的笔记也会以这个形式介绍编程项目。</p>
<h2 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h2><p>说起最经典的二分类任务，大家都会想起小猫分类（或许跟吴恩达老师的课比较流行有关）。在这个项目中，我也顺应潮流，选择了一个猫狗数据集（<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/fusicfenta/cat-and-dog?resource=download）。">https://www.kaggle.com/datasets/fusicfenta/cat-and-dog?resource=download）。</a></p>
<p>在此数据集中，数据是按以下结构存储的：</p>
<p><img src="/2022/05/10/DLS-note-2/4.png" alt></p>
<p>在二分类任务中，数据的标签为0或1（表示是否是小猫）。而此数据集只是把猫、狗的图片分别放到了不同的文件夹里，这意味着我们待会儿要手动给这些数据打上0或1的标签。</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>由于训练集和测试集所在的目录相同，我们先写一个读数据集的函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span>(<span class="params"><span class="built_in">dir</span>, data_num</span>):</span></span><br><span class="line">    cat_images = glob(osp.join(<span class="built_in">dir</span>, <span class="string">&#x27;cats&#x27;</span>, <span class="string">&#x27;*.jpg&#x27;</span>))</span><br><span class="line">    dog_images = glob(osp.join(<span class="built_in">dir</span>, <span class="string">&#x27;dogs&#x27;</span>, <span class="string">&#x27;*.jpg&#x27;</span>))</span><br><span class="line">    cat_tensor = []</span><br><span class="line">    dog_tensor = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(cat_images):</span><br><span class="line">        <span class="keyword">if</span> idx &gt;= data_num:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        i = cv2.imread(image) / <span class="number">255</span></span><br><span class="line">        i = cv2.resize(i, input_shape)</span><br><span class="line">        cat_tensor.append(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(dog_images):</span><br><span class="line">        <span class="keyword">if</span> idx &gt;= data_num:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        i = cv2.imread(image) / <span class="number">255</span></span><br><span class="line">        i = cv2.resize(i, input_shape)</span><br><span class="line">        dog_tensor.append(i)</span><br><span class="line"></span><br><span class="line">    X = cat_tensor + dog_tensor</span><br><span class="line">    Y = [<span class="number">1</span>] * <span class="built_in">len</span>(cat_tensor) + [<span class="number">0</span>] * <span class="built_in">len</span>(dog_tensor)</span><br><span class="line">    X_Y = <span class="built_in">list</span>(<span class="built_in">zip</span>(X, Y))</span><br><span class="line">    shuffle(X_Y)</span><br><span class="line">    X, Y = <span class="built_in">zip</span>(*X_Y)</span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure><br>函数先是用<code>glob</code>读出文件夹下所有猫狗的图片路径，再按文件路径依次把文件读入。接着，函数为数据生成了0或1的标签。最后，函数把数据打乱，并返回数据。让我们来看看这段代码里有哪些要注意的地方。</p>
<p>在具体介绍代码之前，要说明一下我在这个数据集上做的两个特殊处理：</p>
<ol>
<li>这个函数有一个参数<code>data_num</code>，表示我们要读取多少张猫+多少张狗的数据。原数据集有上千张图片，直接读进内存肯定会把内存塞爆。为了实现上的方便，我加了一个控制数据数量的参数。在这个项目中，我只用了800张图片做训练集。</li>
<li>原图片是很大的，为了节约内存，我把所有图片都变成了input_shape=(224, 224)的大小。</li>
</ol>
<p>接下来，我们来看一下一般情况下数据处理中的一些知识。在读数据的时候，把数据<strong>归一化</strong>（令数据分布在(-1, 1)这个区间内）十分关键。如果不这样做的话，loss里的$loge^{z}$会趋近$log0$，梯度的收敛速度会极慢，训练会难以进行。这是这节课上没有讲的内容，但是它在实战中非常关键。</p>
<blockquote>
<p>这个时候输出loss的话，会得到一个Python无法表示的数字：<code>nan</code>。在训练中如果看到loss是<code>nan</code>，多半就是数据没有归一化的原因。这个是一个非常常见的bug，一定要记得做数据归一化！</p>
<p>第三节课里讲了激活函数的收敛速度问题。</p>
</blockquote>
<p>在这段代码里，归一化是靠</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">i = cv2.imread(image) / <span class="number">255</span></span><br></pre></td></tr></table></figure>
<p>实现的。</p>
<p>接着，我们用以下代码生成了训练输入和对应的标签：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = cat_tensor + dog_tensor</span><br><span class="line">Y = [<span class="number">1</span>] * <span class="built_in">len</span>(cat_tensor) + [<span class="number">0</span>] * <span class="built_in">len</span>(dog_tensor)</span><br></pre></td></tr></table></figure><br>现在，我们的数据是“[猫，猫，猫……狗，狗，狗]”这样整整齐齐地排列着。由于我们是一次性拿整个训练集去训练，训练数据是否打乱是无关紧要的。但为了兼容之后其他训练策略，这里我还是习惯性地把数据打乱了：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_Y = <span class="built_in">list</span>(<span class="built_in">zip</span>(X, Y))</span><br><span class="line">shuffle(X_Y)</span><br><span class="line">X, Y = <span class="built_in">zip</span>(*X_Y)</span><br></pre></td></tr></table></figure><br>使用这三行“魔法Python”可以打乱<code>list</code>对中的数据。</p>
<p>有了读一个文件夹的函数，用下面的代码就可以读训练集和测试集：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span>(<span class="params"><span class="built_in">dir</span>=<span class="string">&#x27;data/archive/dataset&#x27;</span>, input_shape=(<span class="params"><span class="number">224</span>, <span class="number">224</span></span>)</span>):</span></span><br><span class="line">    train_X, train_Y = load_dataset(osp.join(<span class="built_in">dir</span>, <span class="string">&#x27;training_set&#x27;</span>), <span class="number">400</span>)</span><br><span class="line">    test_X, test_Y = load_dataset(osp.join(<span class="built_in">dir</span>, <span class="string">&#x27;test_set&#x27;</span>), <span class="number">100</span>)</span><br><span class="line">    <span class="keyword">return</span> train_X, train_Y, test_X, test_Y</span><br></pre></td></tr></table></figure><br>这里训练集有400+400=800张图片，测试集有100+100=200张图片。如果大家发现内存还是占用太多的话，可以改小这两个数字。</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>在这个项目中，我们使用的是逻辑回归算法，可以看成是只有一个神经元的神经网络。如之前的课堂笔记所述，我们网络的公式是：</p>
<script type="math/tex; mode=display">
\hat{y} = \sigma(w^Tx+b)</script><p>这里我们要实现两个函数：</p>
<ol>
<li>resize_input：由于图片张量的形状是[h, w, c]，而网络的输入是一个列向量，我们要把图片张量resize一下。</li>
<li>sigmoid: 我们要用<code>numpy</code>函数组合出一个<code>sigmoid</code>函数。</li>
</ol>
<p>熟悉了<code>numpy</code>的API后，实现这两个函数还是很容易的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resize_input</span>(<span class="params">a: np.ndarray</span>):</span></span><br><span class="line">    h, w, c = a.shape</span><br><span class="line">    a.resize((h * w * c))</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure></p>
<p>这里我代码实现上写得有点“脏”，调用<code>resize_input</code>做数据预处理是放在<code>main</code>函数里的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y, test_X, test_Y = generate_data()</span><br><span class="line"></span><br><span class="line">train_X = [resize_input(x) <span class="keyword">for</span> x <span class="keyword">in</span> train_X]</span><br><span class="line">test_X = [resize_input(x) <span class="keyword">for</span> x <span class="keyword">in</span> test_X]</span><br><span class="line">train_X = np.array(train_X).T</span><br><span class="line">train_Y = np.array(train_Y)</span><br><span class="line">train_Y.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">test_X = np.array(test_X).T</span><br><span class="line">test_Y = np.array(test_Y)</span><br><span class="line">test_Y.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>经过这些预处理代码，X的shape会变成[$n_x$, $m$]，Y的shape会变成[$1$, $m$]，和课堂里讲的内容一致。</p>
<p>有了sigmoid函数和正确shape的输入，我们可以写出网络的推理函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">W, b, X</span>):</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(np.dot(W.T, X) + b)</span><br></pre></td></tr></table></figure>
<h2 id="损失函数与梯度下降"><a href="#损失函数与梯度下降" class="headerlink" title="损失函数与梯度下降"></a>损失函数与梯度下降</h2><p>如前面的笔记所述，损失函数可以用下面的方法计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(-(y * np.log(y_hat) + (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - y_hat)))</span><br></pre></td></tr></table></figure>
<p>我们定义损失函数，实际上为了求得每个参数的梯度。因此，在求梯度时，其实用不到损失函数本身，只需要知道每个参数对于损失函数的导数。在这个项目中，损失函数只用于输出，以监控当前的训练进度。</p>
<p>而在梯度下降中，我们不需要用到损失函数，只需要算出每个参数的梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">W, b, X, Y, lr</span>):</span></span><br><span class="line">    Z = np.dot(W.T, X) + b</span><br><span class="line">    A = sigmoid(Z)</span><br><span class="line">    d_Z = A - Y</span><br><span class="line">    d_W = np.mean(X * d_Z, <span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    d_b = np.<span class="built_in">sum</span>(d_Z)</span><br><span class="line">    <span class="keyword">return</span> W - lr * d_W, b - lr * d_b</span><br></pre></td></tr></table></figure>
<p>在这段代码中，我们根据前面算好的公式，算出了<code>W, b</code>的梯度并对<code>W, b</code>进行更新。这段代码有一点需要注意：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d_W = np.mean(X * d_Z, <span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br>这个<code>keepdims=True</code>是必不可少的。使用<code>np.sum, np.mean</code>这种会导致维度变少的计算时，如果加了<code>keepdims=True</code>,会让变少的那一个维度保持长度1.比如一个[4, 3]的矩阵，我们对第二维做求和，理论上得到的是一个[4]的向量。但如果用了<code>keepdims=True</code>，就会得到一个[4, 1]的矩阵。保持向量的维度，可以让某些广播运算正确进行。比如我要用[4, 3]的矩阵减去[4]的矩阵就会报错，而减去[4, 1]的矩阵就不会报错。</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">train_X, train_Y, step=<span class="number">1000</span>, learning_rate=<span class="number">0.00001</span></span>):</span></span><br><span class="line">    W, b = init_weights()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;learning rate: <span class="subst">&#123;learning_rate&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(step):</span><br><span class="line">        W, b = train_step(W, b, train_X, train_Y, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出当前训练进度</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            y_hat = predict(W, b, train_X)</span><br><span class="line">            ls = loss(y_hat, train_Y)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;step <span class="subst">&#123;i&#125;</span> loss: <span class="subst">&#123;ls&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> W, b</span><br></pre></td></tr></table></figure>
<p>有了刚刚的梯度下降函数<code>train_step</code>，训练实现起来就很方便了。我们只需要设置一个训练总次数<code>step</code>，再调用<code>train_step</code>更新参数即可。</p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>深度学习中，我们要用一个网络从来没有见过的数据集做测试，以验证网络能否泛化到一般的数据上。这里我们直接使用数据集中的<code>test_set</code>，用下面的代码计算分类任务的正确性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">W, b, test_X, test_Y</span>):</span></span><br><span class="line">    y_hat = predict(W, b, test_X)</span><br><span class="line">    predicts = np.where(y_hat &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    score = np.mean(np.where(predicts == test_Y, <span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;score&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>这里的<code>np.where</code>没有在课堂里讲过，这里补充介绍一下。<code>predicts=np.where(y_hat &gt; 0.5, 1, 0)</code>这一行，等价于下面的循环：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建一个和y_hat一样形状的ndarray</span></span><br><span class="line">predicts = np.zeros(y_hat.shape)</span><br><span class="line"><span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(y_hat):</span><br><span class="line">  <span class="keyword">if</span> v &gt; <span class="number">0.5</span>:</span><br><span class="line">    predicts[i] = <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    predicts[<span class="number">1</span>] = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>也就是说，我们对<code>y_hat</code>做了逐元素的判断<code>v &gt; 0.5?</code>，如果判断成立，则赋值<code>1</code>，否则赋值<code>0</code>。这就好像是一个老师在批改学生的作业，如果对了，就给1分，否则给0分。</p>
<p><code>y_hat 》 0.5</code>是有实际意义的：在二分类问题中，如果网络输出图片是小猫的概率大于0.5，我们就认为图片就是小猫的图片；否则，我们认为不是。</p>
<p>之后，我们用另一个<code>(np.where(predicts == test_Y, 1, 0)</code>来“批改作业”：如果预测值和真值一样，则打1分，否则打0分。</p>
<p>最后，我们用<code>score = np.mean(...)</code>算出每道题分数的平均值，来给整个网络的表现打一个总分。</p>
<p>这里要注意一下，整个项目中我们用了两个方式来评价网络：我们监控了<code>loss</code>,因为<code>loss</code>反应了网络在<strong>训练集</strong>上的表现；我们计算了网络在测试集上的准确度，因为准确度反应了网络在<strong>一般数据</strong>上的表现。之后的课堂里应该也会讲到如何使用这些指标来进一步优化网络，这里会算它们就行了。</p>
<h2 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h2><p>嘿嘿，想不到吧，除了之前计划的章节外，这里还多了一个趣味性比较强的调参章节。</p>
<p>搞深度学习，最好玩的地方就是调参数了。通过优化网络的超参数，我们能看到网络的性能在不断变好，准确率在不断变高。这个感觉就和考试分数越来越高，玩游戏刷的伤害越来越高给人带来的成就感一样。</p>
<p>在这个网络中，可以调的参数只有一个学习率。通过玩这个参数，我们能够更直观地认识学习率对梯度下降的影响。</p>
<p>这里我分享一下我的调参结果：</p>
<p>如果学习率&gt;=0.0003，网络更新的步伐过大，从而导致梯度不收敛，训练失败。</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">learning rate: 0.0003</span><br><span class="line">step 0 loss: 0.6918513655136874</span><br><span class="line">step 10 loss: 0.9047000002073068</span><br><span class="line">step 20 loss: 0.9751763789675365</span><br></pre></td></tr></table></figure>
<p>学习率==0.0002的话，网络差不多能以最快的速度收敛。</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">learning rate: 0.0002</span><br><span class="line">step 0 loss: 0.692168431534233</span><br><span class="line">step 10 loss: 0.684254876013497</span><br><span class="line">step 20 loss: 0.6780829877162996</span><br></pre></td></tr></table></figure>
<p>学习率==0.0001,甚至==0.00003也能训练，但是训练速度会变慢。</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">learning rate: 0.0001</span><br><span class="line">step 0 loss: 0.6926003513589579</span><br><span class="line">step 10 loss: 0.6883167092427446</span><br><span class="line">step 20 loss: 0.684621635180076</span><br></pre></td></tr></table></figure>
<p>这里判断网络的收敛速度时，要用到的指标是<strong>损失函数</strong>。我的代码里默认每10次训练输出一次损失函数的值。</p>
<blockquote>
<p>一般大家不会区别误差和损失函数，会把损失函数叫成 loss。</p>
</blockquote>
<p>为了节约时间，一开始我只训练了1000步，最后准确率只有0.57左右。哪怕我令输出全部为1，从期望上都能得到0.5的准确率。这个结果确实不尽如人意。</p>
<p>我自己亲手设计的模型，结果怎么能这么差呢？肯定是训练得不够。我一怒之下，加了个零，让程序跑10000步训练。看着loss不断降低，从0.69，到0.4，再到0.3，最后在0.24的小数点第3位之后变动，我的心情也越来越激动：能不能再低点，能不能再创新低？那感觉就像股市开盘看到自己买的股票高开，不断祈祷庄家快点买入一样。</p>
<p>在电脑前，盯着不断更新的控制台快一小时后，loss定格在了0.2385，我总算等到了10000步训练结束的那一刻。模型即将完成测试，准确率即将揭晓。<br>我定睛一看——准确率居然还只有0.575!</p>
<p>这肯定不是我代码的问题，一定是逻辑回归这个模型太烂了！希望在之后的课程中，我们能够用更复杂的模型跑出更好的结果。</p>
<p>欢迎大家也去下载这个demo(<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/LogisticRegression)，一起调一调参数~">https://github.com/SingleZombie/DL-Demos/tree/master/LogisticRegression)，一起调一调参数~</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/04/23/DLS-note-1/" rel="prev" title="吴恩达《深度学习专项》笔记+代码实战（一）：深度学习入门">
      <i class="fa fa-chevron-left"></i> 吴恩达《深度学习专项》笔记+代码实战（一）：深度学习入门
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%BE%E5%A0%82%E7%AC%94%E8%AE%B0"><span class="nav-number">1.</span> <span class="nav-text">课堂笔记</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%AC%E8%8A%82%E8%AF%BE%E7%9A%84%E7%9B%AE%E6%A0%87"><span class="nav-number">1.1.</span> <span class="nav-text">本节课的目标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E6%A0%87%E8%AE%B0"><span class="nav-number">1.1.1.</span> <span class="nav-text">符号标记</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%85%AC%E5%BC%8F%E6%8F%8F%E8%BF%B0"><span class="nav-number">1.2.</span> <span class="nav-text">逻辑回归的公式描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88Cost-Function%EF%BC%89"><span class="nav-number">1.3.</span> <span class="nav-text">逻辑回归的损失函数（Cost Function）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">1.4.</span> <span class="nav-text">优化算法——梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85"><span class="nav-number">1.5.</span> <span class="nav-text">前置知识补充</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%82%E5%AF%BC"><span class="nav-number">1.5.1.</span> <span class="nav-text">求导</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="nav-number">1.5.2.</span> <span class="nav-text">计算图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%A9%E7%94%A8%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%AF%B9%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%B1%82%E5%AF%BC"><span class="nav-number">1.5.3.</span> <span class="nav-text">利用计算图对逻辑回归求导</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Python-%E5%90%91%E9%87%8F%E5%8C%96%E8%AE%A1%E7%AE%97"><span class="nav-number">1.5.4.</span> <span class="nav-text">Python 向量化计算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.6.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98"><span class="nav-number">2.</span> <span class="nav-text">代码实战</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1"><span class="nav-number">2.1.</span> <span class="nav-text">程序设计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86"><span class="nav-number">2.2.</span> <span class="nav-text">数据收集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">2.3.</span> <span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-number">2.4.</span> <span class="nav-text">网络结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">2.5.</span> <span class="nav-text">损失函数与梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">2.6.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95"><span class="nav-number">2.7.</span> <span class="nav-text">测试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B0%83%E5%8F%82"><span class="nav-number">2.8.</span> <span class="nav-text">调参</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">A foresighted strategist with big-picture thinking. 大局观选手。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">61</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
